{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer.IO Batch Operations\n",
    "\n",
    "This notebook demonstrates how to use batch operations in Customer.IO for high-volume data processing.\n",
    "Batch operations allow you to send multiple API requests in a single call, improving performance and reducing API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Import the necessary functions and initialize the Customer.IO client."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Setup and imports\nimport os\nimport json\nfrom datetime import datetime\nfrom utils.api_client import CustomerIOClient\nfrom utils.people_manager import identify_user\nfrom utils.batch_manager import (\n    send_batch,\n    create_batch_operations,\n    validate_batch_size,\n    split_oversized_batch,\n    MAX_BATCH_SIZE_BYTES,\n    MAX_OPERATION_SIZE_BYTES\n)\nfrom utils.exceptions import CustomerIOError, ValidationError\n\n# Initialize client\nAPI_KEY = os.getenv('CUSTOMERIO_API_KEY', 'your-api-key-here')\nclient = CustomerIOClient(api_key=API_KEY, region='us')\n\nprint(\"Customer.IO batch operations loaded successfully\")\nprint(f\"Max batch size: {MAX_BATCH_SIZE_BYTES:,} bytes\")\nprint(f\"Max operation size: {MAX_OPERATION_SIZE_BYTES:,} bytes\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Batch Operations\n",
    "\n",
    "Batch operations in Customer.IO:\n",
    "- Send multiple operations in a single API request\n",
    "- Support identify, track, group, page, and screen operations\n",
    "- Maximum batch size: 500KB\n",
    "- Maximum individual operation size: 32KB\n",
    "- Improve performance for bulk data imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Batch Operations\n",
    "\n",
    "Create and send simple batch operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Batch identify multiple users\n",
    "users_data = [\n",
    "    {\n",
    "        \"user_id\": \"batch_user_001\",\n",
    "        \"traits\": {\n",
    "            \"email\": \"user1@example.com\",\n",
    "            \"name\": \"Alice Smith\",\n",
    "            \"plan\": \"free\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"user_id\": \"batch_user_002\",\n",
    "        \"traits\": {\n",
    "            \"email\": \"user2@example.com\",\n",
    "            \"name\": \"Bob Johnson\",\n",
    "            \"plan\": \"premium\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"user_id\": \"batch_user_003\",\n",
    "        \"traits\": {\n",
    "            \"email\": \"user3@example.com\",\n",
    "            \"name\": \"Carol Davis\",\n",
    "            \"plan\": \"enterprise\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create batch operations\n",
    "operations = create_batch_operations(\"identify\", users_data)\n",
    "print(f\"Created {len(operations)} identify operations\")\n",
    "\n",
    "# Send batch\n",
    "try:\n",
    "    result = send_batch(client, operations)\n",
    "    print(\"Batch sent successfully\")\n",
    "except CustomerIOError as e:\n",
    "    print(f\"Error sending batch: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Batch track multiple events\n",
    "events_data = [\n",
    "    {\n",
    "        \"user_id\": \"batch_user_001\",\n",
    "        \"event\": \"Account Created\",\n",
    "        \"properties\": {\"source\": \"organic\", \"referrer\": \"google\"}\n",
    "    },\n",
    "    {\n",
    "        \"user_id\": \"batch_user_001\",\n",
    "        \"event\": \"Tutorial Started\",\n",
    "        \"properties\": {\"tutorial_name\": \"Getting Started\"}\n",
    "    },\n",
    "    {\n",
    "        \"user_id\": \"batch_user_002\",\n",
    "        \"event\": \"Subscription Upgraded\",\n",
    "        \"properties\": {\"old_plan\": \"free\", \"new_plan\": \"premium\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create and send event batch\n",
    "event_operations = create_batch_operations(\"track\", events_data)\n",
    "\n",
    "try:\n",
    "    result = send_batch(client, event_operations)\n",
    "    print(f\"Sent {len(event_operations)} events in batch\")\n",
    "except CustomerIOError as e:\n",
    "    print(f\"Error sending events: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Operation Batches\n",
    "\n",
    "Combine different types of operations in a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mixed operations manually\n",
    "mixed_operations = [\n",
    "    # Identify a user\n",
    "    {\n",
    "        \"type\": \"identify\",\n",
    "        \"userId\": \"mixed_user_001\",\n",
    "        \"traits\": {\n",
    "            \"email\": \"mixed@example.com\",\n",
    "            \"name\": \"Mixed User\"\n",
    "        }\n",
    "    },\n",
    "    # Track an event for the same user\n",
    "    {\n",
    "        \"type\": \"track\",\n",
    "        \"userId\": \"mixed_user_001\",\n",
    "        \"event\": \"Onboarding Started\",\n",
    "        \"properties\": {\n",
    "            \"step\": 1,\n",
    "            \"total_steps\": 5\n",
    "        }\n",
    "    },\n",
    "    # Add user to a group\n",
    "    {\n",
    "        \"type\": \"group\",\n",
    "        \"userId\": \"mixed_user_001\",\n",
    "        \"groupId\": \"company_xyz\",\n",
    "        \"traits\": {\n",
    "            \"name\": \"XYZ Corporation\",\n",
    "            \"industry\": \"Technology\"\n",
    "        }\n",
    "    },\n",
    "    # Track a page view\n",
    "    {\n",
    "        \"type\": \"page\",\n",
    "        \"userId\": \"mixed_user_001\",\n",
    "        \"name\": \"Dashboard\",\n",
    "        \"properties\": {\n",
    "            \"url\": \"https://app.example.com/dashboard\",\n",
    "            \"title\": \"User Dashboard\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    result = send_batch(client, mixed_operations)\n",
    "    print(f\"Sent mixed batch with {len(mixed_operations)} operations\")\n",
    "except CustomerIOError as e:\n",
    "    print(f\"Error sending mixed batch: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size Validation\n",
    "\n",
    "Validate batch sizes before sending to avoid errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Check batch size before sending\n",
    "def check_batch_size(operations):\n",
    "    \"\"\"Check and report batch size.\"\"\"\n",
    "    batch_json = json.dumps({\"batch\": operations})\n",
    "    batch_size = len(batch_json.encode('utf-8'))\n",
    "    \n",
    "    print(f\"Batch contains {len(operations)} operations\")\n",
    "    print(f\"Total batch size: {batch_size:,} bytes\")\n",
    "    print(f\"Size limit: {MAX_BATCH_SIZE_BYTES:,} bytes\")\n",
    "    print(f\"Usage: {(batch_size / MAX_BATCH_SIZE_BYTES * 100):.1f}%\")\n",
    "    \n",
    "    return batch_size <= MAX_BATCH_SIZE_BYTES\n",
    "\n",
    "# Create a larger batch\n",
    "large_batch_data = [\n",
    "    {\n",
    "        \"user_id\": f\"user_{i:04d}\",\n",
    "        \"traits\": {\n",
    "            \"email\": f\"user{i}@example.com\",\n",
    "            \"name\": f\"User {i}\",\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"metadata\": {\"batch_import\": True, \"source\": \"csv\"}\n",
    "        }\n",
    "    }\n",
    "    for i in range(100)\n",
    "]\n",
    "\n",
    "large_operations = create_batch_operations(\"identify\", large_batch_data)\n",
    "\n",
    "if check_batch_size(large_operations):\n",
    "    print(\"\\nBatch is within size limits\")\n",
    "else:\n",
    "    print(\"\\nBatch exceeds size limits!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Validate batch using built-in function\n",
    "try:\n",
    "    validate_batch_size(large_operations)\n",
    "    print(\"Batch validation passed\")\n",
    "except ValidationError as e:\n",
    "    print(f\"Batch validation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Large Batches\n",
    "\n",
    "Split oversized batches into smaller chunks automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a very large dataset that will exceed batch limits\n",
    "very_large_data = []\n",
    "for i in range(1000):\n",
    "    very_large_data.append({\n",
    "        \"user_id\": f\"bulk_user_{i:05d}\",\n",
    "        \"traits\": {\n",
    "            \"email\": f\"bulk{i}@example.com\",\n",
    "            \"name\": f\"Bulk User {i}\",\n",
    "            \"description\": \"A\" * 1000,  # Large field to increase size\n",
    "            \"metadata\": {\n",
    "                \"import_batch\": \"2024-01-15\",\n",
    "                \"source\": \"legacy_system\",\n",
    "                \"migrated\": True\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Create operations\n",
    "oversized_operations = create_batch_operations(\"identify\", very_large_data)\n",
    "print(f\"Created {len(oversized_operations)} operations\")\n",
    "\n",
    "# Split into smaller batches\n",
    "batches = split_oversized_batch(oversized_operations)\n",
    "print(f\"\\nSplit into {len(batches)} batches:\")\n",
    "for i, batch in enumerate(batches):\n",
    "    batch_size = len(json.dumps({\"batch\": batch}).encode('utf-8'))\n",
    "    print(f\"  Batch {i+1}: {len(batch)} operations, {batch_size:,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Send split batches\ndef send_split_batches(client, batches):\n    \"\"\"Send multiple batches with progress tracking.\"\"\"\n    successful = 0\n    failed = 0\n    \n    for i, batch in enumerate(batches):\n        try:\n            send_batch(client, batch)\n            successful += 1\n            print(f\"Batch {i+1}/{len(batches)} sent successfully\")\n        except CustomerIOError as e:\n            failed += 1\n            print(f\"Batch {i+1}/{len(batches)} failed: {e}\")\n    \n    print(f\"\\nSummary: {successful} successful, {failed} failed\")\n    return successful, failed\n\n# Send the split batches\n# Uncomment to actually send (be careful with large volumes!)\n# send_split_batches(client, batches[:2])  # Send only first 2 for demo"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Example: User Import\n",
    "\n",
    "Complete workflow for importing users from an external system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserImporter:\n",
    "    \"\"\"Helper class for bulk user imports.\"\"\"\n",
    "    \n",
    "    def __init__(self, client, batch_size=100):\n",
    "        self.client = client\n",
    "        self.batch_size = batch_size\n",
    "        self.stats = {\n",
    "            \"total\": 0,\n",
    "            \"processed\": 0,\n",
    "            \"failed\": 0,\n",
    "            \"batches_sent\": 0\n",
    "        }\n",
    "    \n",
    "    def import_users(self, users_data):\n",
    "        \"\"\"Import users in optimized batches.\"\"\"\n",
    "        self.stats[\"total\"] = len(users_data)\n",
    "        \n",
    "        # Process users in chunks\n",
    "        for i in range(0, len(users_data), self.batch_size):\n",
    "            chunk = users_data[i:i + self.batch_size]\n",
    "            self._process_chunk(chunk)\n",
    "        \n",
    "        return self.stats\n",
    "    \n",
    "    def _process_chunk(self, chunk):\n",
    "        \"\"\"Process a chunk of users.\"\"\"\n",
    "        # Create identify operations\n",
    "        identify_ops = create_batch_operations(\"identify\", [\n",
    "            {\n",
    "                \"user_id\": user[\"id\"],\n",
    "                \"traits\": user[\"attributes\"]\n",
    "            }\n",
    "            for user in chunk\n",
    "        ])\n",
    "        \n",
    "        # Create initial event operations\n",
    "        event_ops = create_batch_operations(\"track\", [\n",
    "            {\n",
    "                \"user_id\": user[\"id\"],\n",
    "                \"event\": \"User Imported\",\n",
    "                \"properties\": {\n",
    "                    \"import_source\": \"bulk_import\",\n",
    "                    \"import_date\": datetime.now().isoformat()\n",
    "                }\n",
    "            }\n",
    "            for user in chunk\n",
    "        ])\n",
    "        \n",
    "        # Combine operations\n",
    "        all_operations = identify_ops + event_ops\n",
    "        \n",
    "        # Validate and potentially split\n",
    "        try:\n",
    "            validate_batch_size(all_operations)\n",
    "            batches = [all_operations]\n",
    "        except ValidationError:\n",
    "            batches = split_oversized_batch(all_operations)\n",
    "        \n",
    "        # Send batches\n",
    "        for batch in batches:\n",
    "            try:\n",
    "                send_batch(self.client, batch)\n",
    "                self.stats[\"processed\"] += len(batch) // 2  # Divide by 2 since we have identify + track\n",
    "                self.stats[\"batches_sent\"] += 1\n",
    "            except CustomerIOError as e:\n",
    "                self.stats[\"failed\"] += len(batch) // 2\n",
    "                print(f\"Batch failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of UserImporter\n",
    "sample_import_data = [\n",
    "    {\n",
    "        \"id\": f\"import_{i:04d}\",\n",
    "        \"attributes\": {\n",
    "            \"email\": f\"import{i}@example.com\",\n",
    "            \"name\": f\"Imported User {i}\",\n",
    "            \"signup_date\": \"2024-01-01\",\n",
    "            \"plan\": \"free\" if i % 3 == 0 else \"premium\",\n",
    "            \"source\": \"salesforce\"\n",
    "        }\n",
    "    }\n",
    "    for i in range(250)  # Import 250 users\n",
    "]\n",
    "\n",
    "importer = UserImporter(client, batch_size=50)\n",
    "stats = importer.import_users(sample_import_data)\n",
    "\n",
    "print(\"Import Summary:\")\n",
    "print(f\"  Total users: {stats['total']}\")\n",
    "print(f\"  Processed: {stats['processed']}\")\n",
    "print(f\"  Failed: {stats['failed']}\")\n",
    "print(f\"  Batches sent: {stats['batches_sent']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Example: Event Migration\n",
    "\n",
    "Migrate historical events from another system."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def migrate_historical_events(client, events_data, events_per_batch=200):\n    \"\"\"Migrate historical events with proper timestamps.\"\"\"\n    \n    # Sort events by timestamp to maintain order\n    sorted_events = sorted(events_data, key=lambda x: x[\"timestamp\"])\n    \n    total_batches = (len(sorted_events) + events_per_batch - 1) // events_per_batch\n    print(f\"Migrating {len(sorted_events)} events in {total_batches} batches\")\n    \n    for batch_num in range(total_batches):\n        start_idx = batch_num * events_per_batch\n        end_idx = min((batch_num + 1) * events_per_batch, len(sorted_events))\n        batch_events = sorted_events[start_idx:end_idx]\n        \n        # Create track operations with timestamps\n        operations = [\n            {\n                \"type\": \"track\",\n                \"userId\": event[\"user_id\"],\n                \"event\": event[\"event_name\"],\n                \"properties\": event[\"properties\"],\n                \"timestamp\": event[\"timestamp\"]\n            }\n            for event in batch_events\n        ]\n        \n        try:\n            send_batch(client, operations)\n            print(f\"Batch {batch_num + 1}/{total_batches} migrated\")\n        except CustomerIOError as e:\n            print(f\"Batch {batch_num + 1} failed: {e}\")\n\n# Example historical events\nhistorical_events = [\n    {\n        \"user_id\": \"hist_user_001\",\n        \"event_name\": \"Account Created\",\n        \"timestamp\": \"2023-01-15T10:00:00Z\",\n        \"properties\": {\"source\": \"organic\"}\n    },\n    {\n        \"user_id\": \"hist_user_001\",\n        \"event_name\": \"Feature Used\",\n        \"timestamp\": \"2023-01-16T14:30:00Z\",\n        \"properties\": {\"feature\": \"dashboard\"}\n    },\n    {\n        \"user_id\": \"hist_user_002\",\n        \"event_name\": \"Subscription Started\",\n        \"timestamp\": \"2023-02-01T09:00:00Z\",\n        \"properties\": {\"plan\": \"premium\", \"period\": \"monthly\"}\n    }\n]\n\n# Migrate the events\n# migrate_historical_events(client, historical_events, events_per_batch=10)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Operations with Context\n",
    "\n",
    "Add context information to all operations in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Add context to batch operations\n",
    "context_operations = [\n",
    "    {\n",
    "        \"type\": \"identify\",\n",
    "        \"userId\": \"context_user_001\",\n",
    "        \"traits\": {\"email\": \"context1@example.com\"}\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"track\",\n",
    "        \"userId\": \"context_user_001\",\n",
    "        \"event\": \"Button Clicked\",\n",
    "        \"properties\": {\"button_name\": \"signup\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Add context about the import\n",
    "batch_context = {\n",
    "    \"app\": {\n",
    "        \"name\": \"Data Importer\",\n",
    "        \"version\": \"2.0.0\"\n",
    "    },\n",
    "    \"library\": {\n",
    "        \"name\": \"customer-io-python\",\n",
    "        \"version\": \"1.0.0\"\n",
    "    },\n",
    "    \"import\": {\n",
    "        \"source\": \"legacy_database\",\n",
    "        \"import_id\": \"import_20240115_001\",\n",
    "        \"imported_at\": datetime.now().isoformat()\n",
    "    }\n",
    "}\n",
    "\n",
    "try:\n",
    "    result = send_batch(client, context_operations, context=batch_context)\n",
    "    print(\"Batch with context sent successfully\")\n",
    "except CustomerIOError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling and Retry Logic\n",
    "\n",
    "Implement robust error handling for batch operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class BatchProcessor:\n",
    "    \"\"\"Batch processor with retry logic.\"\"\"\n",
    "    \n",
    "    def __init__(self, client, max_retries=3, retry_delay=1):\n",
    "        self.client = client\n",
    "        self.max_retries = max_retries\n",
    "        self.retry_delay = retry_delay\n",
    "    \n",
    "    def send_with_retry(self, operations):\n",
    "        \"\"\"Send batch with automatic retry on failure.\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                result = send_batch(self.client, operations)\n",
    "                print(f\"Batch sent successfully on attempt {attempt + 1}\")\n",
    "                return result\n",
    "            except CustomerIOError as e:\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    wait_time = self.retry_delay * (2 ** attempt)  # Exponential backoff\n",
    "                    print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "                    print(f\"Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"All {self.max_retries} attempts failed\")\n",
    "                    raise\n",
    "    \n",
    "    def process_large_dataset(self, operations_list, batch_size=100):\n",
    "        \"\"\"Process large dataset with batching and retry.\"\"\"\n",
    "        results = {\n",
    "            \"successful_batches\": 0,\n",
    "            \"failed_batches\": 0,\n",
    "            \"total_operations\": len(operations_list)\n",
    "        }\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, len(operations_list), batch_size):\n",
    "            batch = operations_list[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                self.send_with_retry(batch)\n",
    "                results[\"successful_batches\"] += 1\n",
    "            except CustomerIOError as e:\n",
    "                results[\"failed_batches\"] += 1\n",
    "                print(f\"Failed to process batch after retries: {e}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Example usage\n",
    "processor = BatchProcessor(client, max_retries=3, retry_delay=0.5)\n",
    "\n",
    "# Create test operations\n",
    "test_operations = [\n",
    "    {\n",
    "        \"type\": \"identify\",\n",
    "        \"userId\": f\"retry_user_{i:03d}\",\n",
    "        \"traits\": {\"test\": True}\n",
    "    }\n",
    "    for i in range(10)\n",
    "]\n",
    "\n",
    "# Process with retry\n",
    "# processor.send_with_retry(test_operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### Batch Size Optimization\n",
    "- Keep batches under 500KB total size\n",
    "- Aim for 100-500 operations per batch for optimal performance\n",
    "- Monitor individual operation sizes (32KB limit)\n",
    "\n",
    "### Data Preparation\n",
    "- Validate data before creating batches\n",
    "- Sort events by timestamp when order matters\n",
    "- Remove duplicate operations before batching\n",
    "\n",
    "### Error Handling\n",
    "- Implement retry logic with exponential backoff\n",
    "- Log failed batches for manual review\n",
    "- Consider partial batch processing on failures\n",
    "\n",
    "### Performance\n",
    "- Use batch operations for imports > 10 records\n",
    "- Process large datasets in chunks\n",
    "- Monitor API rate limits (3000 requests/3 seconds)\n",
    "\n",
    "### Monitoring\n",
    "- Track batch success/failure rates\n",
    "- Monitor average batch sizes\n",
    "- Set up alerts for failed imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you understand batch operations, explore:\n",
    "\n",
    "- **01_people_management.ipynb** - Batch user identification\n",
    "- **02_event_tracking.ipynb** - Batch event tracking\n",
    "- **03_objects_and_relationships.ipynb** - Batch object operations\n",
    "\n",
    "For production use:\n",
    "- Implement comprehensive error handling\n",
    "- Set up monitoring for batch job performance\n",
    "- Create data validation pipelines before import"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}