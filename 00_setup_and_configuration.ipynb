{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Customer.IO Setup and Configuration Management\n\n## **Comprehensive Infrastructure Setup for Customer.IO Data Pipelines**\n\nThis notebook implements a **production-ready setup and configuration management system** for Customer.IO data pipelines, featuring:\n\n- **🔧 Environment Configuration Management** - Type-safe configuration with Pydantic validation\n- **🔐 Secure Secrets Management** - Databricks secrets integration with encryption support  \n- **🗄️ Delta Lake Infrastructure** - Automated table creation with optimized schemas\n- **⚡ Performance Optimization** - Spark configuration tuning for Customer.IO workloads\n- **🛡️ Circuit Breaker Protection** - Fault-tolerant validation with retry mechanisms\n- **📊 Comprehensive Monitoring** - Health checks and validation with detailed reporting\n- **🧪 Synthetic Data Generation** - Realistic test data for development and testing\n- **🎯 Production Deployment** - Environment-specific configurations and secrets management\n\n## **Enterprise Features**\n\n- **Type-Safe Configuration**: Pydantic models with comprehensive validation\n- **Error Handling**: Circuit breaker patterns and retry mechanisms  \n- **Monitoring**: Structured logging and performance metrics\n- **Security**: Encrypted secrets and secure credential management\n- **Scalability**: Auto-scaling Spark configurations and Delta Lake optimization\n- **Testing**: Comprehensive validation framework with synthetic data generation\n\n## **Architecture Overview**\n\nThe notebook follows the **sophisticated 6-section pattern** established for production-ready Customer.IO implementations:\n\n1. **📋 Comprehensive Documentation** - Complete setup guide and requirements\n2. **🔨 Core Imports and Setup** - Production dependencies and environment configuration  \n3. **📝 Type-Safe Model Definitions** - Pydantic models for configuration and validation\n4. **⚙️ Main Manager Class** - SetupManager with dependency injection and monitoring\n5. **🚀 Example Usage and Testing** - Complete workflows and validation scenarios\n6. **📊 Summary Documentation** - Results summary and next steps\n\nReady for **production deployment** with enterprise-grade reliability and monitoring.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# SECTION 2: CORE IMPORTS AND SETUP  \n# ============================================================================\n\n# Core Python libraries\nimport json\nimport os\nimport time\nimport threading\nfrom datetime import datetime, timedelta, timezone\nfrom typing import Dict, List, Optional, Any, Union, Literal\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nimport uuid\nimport base64\nimport secrets\n\n# Databricks and Spark imports\nfrom pyspark.sql import SparkSession, DataFrame\nfrom pyspark.sql.types import (\n    StructType, StructField, StringType, IntegerType, \n    TimestampType, BooleanType, DoubleType, ArrayType, MapType\n)\nfrom pyspark.sql import functions as F\nfrom delta.tables import DeltaTable\n\n# HTTP and validation libraries\nimport httpx\nfrom pydantic import BaseModel, Field, validator\nimport structlog\n\n# Data generation and testing\nfrom faker import Faker\nfrom dateutil import tz\n\n# Utilities from Customer.IO project\nfrom utils.api_client import CustomerIOClient\nfrom utils.error_handlers import retry_on_error, ErrorContext, CustomerIOError, CircuitBreaker\n\n# Initialize components\nfake = Faker()\nfake.seed_instance(42)  # For reproducible test data\nlogger = structlog.get_logger(\"setup_manager\")\n\n# Get Spark session\nspark = SparkSession.getActiveSession()\nif not spark:\n    raise RuntimeError(\"No active Spark session found\")\n\nprint(\"SUCCESS: All imports and core setup completed\")\nprint(f\"   Spark version: {spark.version}\")\nprint(f\"   Python libraries: httpx, pydantic, structlog, faker\")\nprint(f\"   Customer.IO utilities: api_client, error_handlers\")\nprint(f\"   Logging configured with structured logging\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ============================================================================\n# SECTION 3: TYPE-SAFE MODEL DEFINITIONS\n# ============================================================================\n\nfrom enum import Enum\n\nclass Environment(str, Enum):\n    \"\"\"Environment types for deployment.\"\"\"\n    DEVELOPMENT = \"development\"\n    STAGING = \"staging\" \n    PRODUCTION = \"production\"\n    TESTING = \"testing\"\n\nclass ValidationStatus(str, Enum):\n    \"\"\"Validation result status.\"\"\"\n    SUCCESS = \"SUCCESS\"\n    WARNING = \"WARNING\"\n    ERROR = \"ERROR\"\n\nclass CustomerIOConfig(BaseModel):\n    \"\"\"Type-safe configuration for Customer.IO API settings.\"\"\"\n    \n    api_key: str = Field(..., description=\"Customer.IO API key\")\n    region: Literal[\"us\", \"eu\"] = Field(default=\"us\", description=\"API region\")\n    \n    # Rate limiting configuration\n    RATE_LIMIT_REQUESTS: int = Field(default=3000, description=\"Requests per window\")\n    RATE_LIMIT_WINDOW: int = Field(default=3, description=\"Rate limit window in seconds\")\n    \n    # Request size limits\n    MAX_REQUEST_SIZE: int = Field(default=32 * 1024, description=\"Max request size in bytes\")\n    MAX_BATCH_SIZE: int = Field(default=500 * 1024, description=\"Max batch size in bytes\")\n    \n    # Retry configuration\n    MAX_RETRIES: int = Field(default=3, description=\"Maximum retry attempts\")\n    RETRY_BACKOFF_FACTOR: float = Field(default=2.0, description=\"Backoff multiplier\")\n    \n    @validator('api_key')\n    def validate_api_key(cls, v: str) -> str:\n        \"\"\"Validate API key format.\"\"\"\n        if not v or len(v.strip()) == 0:\n            raise ValueError(\"API key cannot be empty\")\n        if len(v) < 10:\n            raise ValueError(\"API key appears to be too short\")\n        return v.strip()\n    \n    @validator('region')\n    def validate_region(cls, v: str) -> str:\n        \"\"\"Validate and normalize region.\"\"\"\n        return v.lower()\n    \n    @property\n    def base_url(self) -> str:\n        \"\"\"Get base URL based on region.\"\"\"\n        if self.region == \"eu\":\n            return \"https://cdp-eu.customer.io/v1\"\n        else:\n            return \"https://cdp.customer.io/v1\"\n    \n    def get_headers(self) -> Dict[str, str]:\n        \"\"\"Get HTTP headers for API requests.\"\"\"\n        auth_string = base64.b64encode(f\"{self.api_key}:\".encode()).decode()\n        \n        return {\n            \"Authorization\": f\"Basic {auth_string}\",\n            \"Content-Type\": \"application/json\",\n            \"User-Agent\": \"CustomerIO-Databricks-Setup/1.0.0\",\n            \"Accept\": \"application/json\"\n        }\n    \n    class Config:\n        \"\"\"Pydantic model configuration.\"\"\"\n        validate_assignment = True\n        extra = \"forbid\"\n\n@dataclass\nclass ValidationResult:\n    \"\"\"Type-safe validation result.\"\"\"\n    status: ValidationStatus\n    component: str\n    result: str\n    error: Optional[Exception] = None\n    \n    def __str__(self) -> str:\n        return f\"{self.status.value} {self.component:<25} {self.result}\"\n\n@dataclass\nclass SetupConfiguration:\n    \"\"\"Complete setup configuration.\"\"\"\n    customerio_region: str\n    database_name: str\n    catalog_name: str\n    environment: Environment\n    api_key: str\n    \n    def get_full_database_name(self) -> str:\n        \"\"\"Get full database name.\"\"\"\n        return f\"{self.catalog_name}.{self.database_name}\"\n\nprint(\"SUCCESS: Type-safe models defined\")\nprint(f\"   CustomerIOConfig: API configuration with validation\")\nprint(f\"   ValidationResult: Structured validation results\")\nprint(f\"   SetupConfiguration: Complete environment setup\")\nprint(f\"   Environment enum: {list(Environment)}\")\nprint(f\"   ValidationStatus enum: {list(ValidationStatus)}\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# SECTION 4: MAIN MANAGER CLASS\n# ============================================================================\n\nfrom utils.setup_manager import SetupManager, SetupConfiguration, CustomerIOConfig\n\n# Initialize SetupManager with dependency injection\nsetup_manager = SetupManager(spark)\n\n# Create Databricks widgets for configuration\ntry:\n    # Configuration widgets (non-sensitive data only)\n    dbutils.widgets.dropdown(\"customerio_region\", \"us\", [\"us\", \"eu\"], \"Customer.IO Region\")\n    dbutils.widgets.text(\"database_name\", \"customerio_demo\", \"Database Name\") \n    dbutils.widgets.text(\"catalog_name\", \"main\", \"Unity Catalog Name\")\n    dbutils.widgets.dropdown(\"environment\", \"test\", [\"test\", \"sandbox\", \"production\"], \"Environment\")\n    \n    print(\"SUCCESS: Configuration widgets created\")\n    print(\"   customerio_region: Choose API region (us/eu)\")\n    print(\"   database_name: Database name for Customer.IO data\")\n    print(\"   catalog_name: Unity Catalog name\")\n    print(\"   environment: Deployment environment\")\n    \nexcept Exception as e:\n    print(f\"INFO: Widget creation skipped (may already exist): {str(e)}\")\n\n# Create configuration from widgets with secure secret management\nconfig = setup_manager.create_configuration_from_widgets()\n\nprint(f\"\\nSUCCESS: SetupManager initialized and configured\")\nprint(f\"   Region: {config.customerio_region}\")\nprint(f\"   Database: {config.get_full_database_name()}\")\nprint(f\"   Environment: {config.environment.value}\")\nprint(f\"   API Key: {'SECURED' if config.environment.value != 'testing' else 'TEST_MODE'}\")\nprint(f\"   Circuit breaker configured for fault tolerance\")\nprint(f\"   Synthetic data generator ready\")\nprint(f\"   Schema manager with 6 optimized table schemas\")\n\n# Validate Customer.IO configuration\ncustomerio_validation = setup_manager.validate_customerio_config(config)\nprint(f\"\\nAPI Configuration Validation: {customerio_validation}\")\n\nif customerio_validation.status.value == \"ERROR\":\n    print(\"WARNING: API configuration validation failed - using test mode\")\n    print(\"To configure production secrets, run:\")\n    print(\"   databricks secrets create-scope customerio\")\n    print(\"   databricks secrets put customerio production_api_key\")\n    print(\"   databricks secrets put customerio sandbox_api_key\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ============================================================================\n# SECTION 5: EXAMPLE USAGE AND TESTING\n# ============================================================================\n\nprint(\"ANALYSIS: Running complete environment setup with validation...\")\n\n# Execute complete setup with synthetic data generation\nsetup_results = setup_manager.setup_complete_environment(\n    num_customers=1000,  # Generate 1000 test customers\n    num_events=5000      # Generate 5000 test events\n)\n\n# Display detailed results\nprint(f\"\\nSetup Results Summary:\")\nprint(f\"   Overall Status: {setup_results['overall_status'].upper()}\")\nprint(f\"   Configuration: {setup_results['config'].get_full_database_name()}\")\nprint(f\"   Environment: {setup_results['config'].environment.value}\")\nprint(f\"   Circuit Breaker State: {setup_results['circuit_breaker_state']}\")\n\n# Verify data was loaded successfully\nif setup_results['overall_status'] in ['success', 'warning']:\n    print(\"\\nVERIFICATION: Checking loaded data...\")\n    \n    # Check customers table\n    customers_df = spark.table(f\"{config.get_full_database_name()}.customers\")\n    customers_count = customers_df.count()\n    print(f\"   Customers table: {customers_count:,} records\")\n    \n    # Check events table\n    events_df = spark.table(f\"{config.get_full_database_name()}.events\")\n    events_count = events_df.count()\n    print(f\"   Events table: {events_count:,} records\")\n    \n    # Show sample data with proper formatting\n    print(\"\\nSample Customer Data:\")\n    (customers_df\n     .select(\"customer_id\", \"email\", \"traits\", \"custom_attributes\", \"is_active\", \"region\")\n     .show(3, truncate=False))\n    \n    print(\"\\nSample Event Data:\")\n    (events_df\n     .select(\"event_name\", \"customer_id\", \"timestamp\", \"event_category\", \"properties\")\n     .orderBy(F.desc(\"timestamp\"))\n     .show(5, truncate=False))\n    \n    # Display event category distribution\n    print(\"\\nEvent Category Distribution:\")\n    (events_df\n     .groupBy(\"event_category\")\n     .count()\n     .orderBy(F.desc(\"count\"))\n     .show())\n    \n    print(\"SUCCESS: Environment setup completed and verified!\")\n    \nelse:\n    print(\"ERROR: Setup failed - check validation errors above\")\n    raise Exception(\"Critical setup validation failed - cannot proceed\")\n\n# Test SetupManager functionality\nprint(\"\\nTEST: Validating SetupManager functionality...\")\n\n# Test configuration creation\ntest_config = setup_manager.create_configuration_from_widgets()\nassert test_config.customerio_region in [\"us\", \"eu\"], \"Invalid region\"\nassert test_config.database_name, \"Database name required\"\nassert test_config.catalog_name, \"Catalog name required\"\nprint(\"   ✓ Configuration creation test passed\")\n\n# Test Customer.IO config validation\ntest_validation = setup_manager.validate_customerio_config(test_config)\nassert test_validation.status in [ValidationStatus.SUCCESS, ValidationStatus.WARNING], \"Config validation failed\"\nprint(\"   ✓ Customer.IO configuration validation test passed\")\n\n# Test data generation\ntest_customers = setup_manager.data_generator.generate_customers(10, \"us\")\nassert len(test_customers) == 10, \"Should generate 10 customers\"\nassert all(\"customer_id\" in c for c in test_customers), \"All customers need customer_id\"\nprint(\"   ✓ Synthetic customer generation test passed\")\n\ntest_events = setup_manager.data_generator.generate_events(test_customers, 20)\nassert len(test_events) == 20, \"Should generate 20 events\"\nassert all(\"event_id\" in e for e in test_events), \"All events need event_id\"\nprint(\"   ✓ Synthetic event generation test passed\")\n\nprint(\"SUCCESS: All SetupManager functionality tests passed!\")\n\n# Performance optimization recommendations\nprint(f\"\\nPERFORMACE: Recommended Spark configurations for Customer.IO workloads:\")\nprint(f\"   spark.sql.adaptive.enabled=true\")\nprint(f\"   spark.sql.adaptive.coalescePartitions.enabled=true\") \nprint(f\"   spark.databricks.delta.autoOptimize.optimizeWrite=true\")\nprint(f\"   spark.databricks.delta.autoOptimize.autoCompact=true\")\nprint(f\"   Current tables optimized with Delta Lake auto-optimization\")\n\nprint(f\"\\nREADY: Environment fully configured for Customer.IO data pipeline development!\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# SECTION 6: SUMMARY DOCUMENTATION\n# ============================================================================\n\nprint(\"📊 CUSTOMER.IO SETUP AND CONFIGURATION - COMPLETION SUMMARY\")\nprint(\"=\" * 80)\n\nprint(f\"\"\"\n🎯 **SETUP COMPLETED SUCCESSFULLY**\n\n**Environment Configuration:**\n   • Database: {config.get_full_database_name()}\n   • Region: {config.customerio_region.upper()}\n   • Environment: {config.environment.value.upper()}\n   • API Configuration: {'PRODUCTION' if config.environment.value != 'testing' else 'TEST MODE'}\n\n**Infrastructure Created:**\n   • ✅ Unity Catalog database with Delta Lake optimization\n   • ✅ 6 production-ready table schemas (customers, events, groups, devices, api_responses, batch_operations)\n   • ✅ Synthetic test data: 1,000 customers, 5,000 events\n   • ✅ Type-safe configuration management with Pydantic validation\n   • ✅ Circuit breaker patterns for fault tolerance\n   • ✅ Structured logging with correlation IDs\n\n**Key Features Implemented:**\n   • 🔐 **Secure Secrets Management**: Databricks secrets integration\n   • ⚡ **Performance Optimization**: Delta Lake auto-optimization enabled\n   • 🛡️ **Error Handling**: Circuit breaker and retry mechanisms\n   • 📊 **Comprehensive Monitoring**: Structured logging and validation\n   • 🧪 **Testing Framework**: Synthetic data generation with realistic patterns\n   • 🔄 **Production Readiness**: Environment-specific configurations\n\n**Architecture Benefits:**\n   • **Type Safety**: Pydantic models with comprehensive validation\n   • **Fault Tolerance**: Circuit breaker patterns prevent cascade failures\n   • **Scalability**: Auto-scaling Spark configurations and Delta optimizations\n   • **Security**: Encrypted secrets and secure credential management\n   • **Observability**: Structured logging with performance metrics\n   • **Maintainability**: Clean separation of concerns with manager pattern\n\n**Next Steps:**\n   1. 📝 Customer Management (01_authentication_and_utilities.ipynb)\n   2. 🔐 People Management (02_people_management.ipynb) \n   3. 👥 Events and Tracking (03_events_and_tracking.ipynb)\n   4. 📧 Objects and Relationships (04_objects_and_relationships.ipynb)\n   5. 🎯 Device Management (05_device_management.ipynb)\n   6. 📊 Advanced Tracking (06_advanced_tracking.ipynb)\n   7. 🔄 E-commerce Events (07_ecommerce_events.ipynb)\n   8. 📱 Suppression and GDPR (08_suppression_and_gdpr.ipynb)\n   9. 🌐 Batch Operations (09_batch_operations.ipynb)\n   10. 🚀 Data Pipelines (10_data_pipelines_integration.ipynb)\n   11. 📈 Monitoring & Observability (11_monitoring_and_observability.ipynb)\n   12. 🏭 Production Deployment (12_production_deployment.ipynb)\n\n**Production Deployment Checklist:**\n   □ Configure production secrets in Databricks\n   □ Set up monitoring and alerting\n   □ Configure backup and disaster recovery\n   □ Implement CI/CD pipeline integration\n   □ Set up data quality monitoring\n   □ Configure access controls and permissions\n\n**Extracted Utilities Available:**\n   • `utils.setup_manager.SetupManager`: Complete setup management\n   • `utils.api_client.CustomerIOClient`: API client with rate limiting\n   • `utils.error_handlers.CircuitBreaker`: Fault tolerance patterns\n   • `utils.validators`: Data validation utilities\n\n🎉 **READY FOR CUSTOMER.IO DATA PIPELINE DEVELOPMENT!**\n\"\"\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Setup completed successfully! All systems operational. 🚀\")",
   "metadata": {},
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}