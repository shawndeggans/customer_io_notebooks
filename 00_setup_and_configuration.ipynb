{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer.IO Data Pipelines API - Setup and Configuration\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook establishes the foundation for working with Customer.IO's Data Pipelines API in Databricks.\n",
    "It covers environment setup, authentication configuration, Delta Lake table creation, and synthetic data generation for demonstrations.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Databricks Runtime 11.3 LTS or higher\n",
    "- Customer.IO API key (for test/sandbox environment)\n",
    "- Databricks secrets configured for API credentials\n",
    "- Cluster with Delta Lake enabled\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **Customer.IO Data Pipelines API**: REST API for sending customer data and events\n",
    "- **Regional Endpoints**: Separate US and EU endpoints for data residency\n",
    "- **Rate Limits**: 3000 requests per 3 seconds\n",
    "- **Request Limits**: 32KB per request, 500KB per batch\n",
    "- **Delta Lake Integration**: Structured data storage for analytics and processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup and Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Package Management - Production Ready\n# These packages should be installed on the cluster in production\n# For development/testing, install with specific versions\n\nREQUIRED_PACKAGES = {\n    \"httpx\": \">=0.25.0\",\n    \"pydantic\": \">=2.0.0\", \n    \"structlog\": \">=24.0.0\",\n    \"faker\": \">=20.0.0\",\n    \"python-dateutil\": \">=2.8.0\"\n}\n\ndef install_packages():\n    \"\"\"Install required packages with version constraints.\"\"\"\n    packages = [f\"{pkg}{version}\" for pkg, version in REQUIRED_PACKAGES.items()]\n    package_string = \" \".join(packages)\n    \n    print(\"Installing packages with version constraints:\")\n    for pkg, version in REQUIRED_PACKAGES.items():\n        print(f\"  {pkg} {version}\")\n    \n    # Install packages\n    %pip install {package_string}\n\n# Install packages\ninstall_packages()\n\n# Restart Python kernel to use newly installed packages\ndbutils.library.restartPython()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport os\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional, Any\nimport uuid\n\n# Databricks and Spark imports\nfrom pyspark.sql import SparkSession, DataFrame\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, BooleanType, DoubleType, ArrayType, MapType\nfrom pyspark.sql import functions as F\nfrom delta.tables import DeltaTable\n\n# HTTP and validation libraries\nimport httpx\nfrom pydantic import BaseModel, Field, validator\nimport structlog\n\n# Data generation\nfrom faker import Faker\nfrom dateutil import tz\n\n# Initialize Faker for generating realistic test data\nfake = Faker()\nfake.seed_instance(42)  # For reproducible data\n\n# Initialize structured logging\nlogger = structlog.get_logger()\n\nprint(\"SUCCESS: All libraries imported successfully\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks Configuration and Widgets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# SECURITY: Use Databricks secrets instead of widgets for API keys\n# Never expose API keys in widgets or plain text!\n\n# Configuration widgets (non-sensitive data only)\ndbutils.widgets.dropdown(\"customerio_region\", \"us\", [\"us\", \"eu\"], \"Customer.IO Region\")\ndbutils.widgets.text(\"database_name\", \"customerio_demo\", \"Database Name\") \ndbutils.widgets.text(\"catalog_name\", \"main\", \"Unity Catalog Name\")\ndbutils.widgets.dropdown(\"environment\", \"test\", [\"test\", \"sandbox\", \"production\"], \"Environment\")\n\n# Get configuration values\nCUSTOMERIO_REGION = dbutils.widgets.get(\"customerio_region\")\nDATABASE_NAME = dbutils.widgets.get(\"database_name\")\nCATALOG_NAME = dbutils.widgets.get(\"catalog_name\")\nENVIRONMENT = dbutils.widgets.get(\"environment\")\n\n# SECURE: Get API key from Databricks secrets\ntry:\n    if ENVIRONMENT == \"production\":\n        CUSTOMERIO_API_KEY = dbutils.secrets.get(scope=\"customerio\", key=\"production_api_key\")\n    elif ENVIRONMENT == \"sandbox\":\n        CUSTOMERIO_API_KEY = dbutils.secrets.get(scope=\"customerio\", key=\"sandbox_api_key\")\n    else:\n        # Test environment - use mock key\n        CUSTOMERIO_API_KEY = \"test_key_demo_12345\"\n        print(\"WARNING: Using test mode with mock API key\")\nexcept Exception as e:\n    print(f\"ERROR: Failed to retrieve API key from secrets: {str(e)}\")\n    print(\"INFO: To configure secrets, run:\")\n    print(\"   databricks secrets create-scope customerio\")\n    print(\"   databricks secrets put customerio production_api_key\")\n    print(\"   databricks secrets put customerio sandbox_api_key\")\n    \n    # Fallback to test mode\n    CUSTOMERIO_API_KEY = \"test_key_demo_12345\"\n    ENVIRONMENT = \"test\"\n    print(\"INFO: Falling back to test mode\")\n\nprint(f\"Configuration:\")\nprint(f\"  Region: {CUSTOMERIO_REGION}\")\nprint(f\"  Database: {CATALOG_NAME}.{DATABASE_NAME}\")\nprint(f\"  Environment: {ENVIRONMENT}\")\nprint(f\"  API Key: {'SUCCESS: Retrieved from secrets' if ENVIRONMENT != 'test' else 'WARNING: Using test key'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer.IO API Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Customer.IO API Configuration with Type Safety and Validation\nfrom pydantic import BaseModel, Field, validator\nfrom typing import Dict, Literal\n\nclass CustomerIOConfig(BaseModel):\n    \"\"\"Type-safe configuration class for Customer.IO API settings.\"\"\"\n    \n    api_key: str = Field(..., description=\"Customer.IO API key\")\n    region: Literal[\"us\", \"eu\"] = Field(default=\"us\", description=\"API region\")\n    \n    # Rate limiting configuration (class variables)\n    RATE_LIMIT_REQUESTS: int = 3000\n    RATE_LIMIT_WINDOW: int = 3  # seconds\n    \n    # Request size limits\n    MAX_REQUEST_SIZE: int = 32 * 1024  # 32KB\n    MAX_BATCH_SIZE: int = 500 * 1024   # 500KB\n    \n    # Retry configuration\n    MAX_RETRIES: int = 3\n    RETRY_BACKOFF_FACTOR: float = 2.0\n    \n    @validator('api_key')\n    def validate_api_key(cls, v: str) -> str:\n        \"\"\"Validate API key format.\"\"\"\n        if not v or len(v.strip()) == 0:\n            raise ValueError(\"API key cannot be empty\")\n        if len(v) < 10:  # Reasonable minimum length\n            raise ValueError(\"API key appears to be too short\")\n        return v.strip()\n    \n    @validator('region')\n    def validate_region(cls, v: str) -> str:\n        \"\"\"Validate and normalize region.\"\"\"\n        return v.lower()\n    \n    @property\n    def base_url(self) -> str:\n        \"\"\"Get base URL based on region.\"\"\"\n        if self.region == \"eu\":\n            return \"https://cdp-eu.customer.io/v1\"\n        else:\n            return \"https://cdp.customer.io/v1\"\n    \n    def get_headers(self) -> Dict[str, str]:\n        \"\"\"Get HTTP headers for API requests.\"\"\"\n        import base64\n        \n        # Customer.IO uses Basic Auth with API key as username, empty password\n        auth_string = base64.b64encode(f\"{self.api_key}:\".encode()).decode()\n        \n        return {\n            \"Authorization\": f\"Basic {auth_string}\",\n            \"Content-Type\": \"application/json\",\n            \"User-Agent\": \"CustomerIO-Databricks-Notebooks/1.0.0\",\n            \"Accept\": \"application/json\"\n        }\n    \n    class Config:\n        \"\"\"Pydantic model configuration.\"\"\"\n        validate_assignment = True\n        extra = \"forbid\"\n\n# Test API key validation\ndef test_api_key_validation():\n    \"\"\"Test API key validation logic.\"\"\"\n    try:\n        # Test valid configuration\n        valid_config = CustomerIOConfig(api_key=CUSTOMERIO_API_KEY, region=CUSTOMERIO_REGION)\n        print(\"SUCCESS: Configuration validation passed\")\n        return valid_config\n    except Exception as e:\n        print(f\"ERROR: Configuration validation failed: {str(e)}\")\n        # Create fallback test configuration\n        test_config = CustomerIOConfig(api_key=\"test_key_demo_12345\", region=\"us\")\n        print(\"INFO: Using fallback test configuration\")\n        return test_config\n\n# Initialize and validate configuration\nconfig = test_api_key_validation()\n\nprint(f\"SUCCESS: Customer.IO API configured\")\nprint(f\"   Base URL: {config.base_url}\")\nprint(f\"   Rate Limit: {config.RATE_LIMIT_REQUESTS} requests per {config.RATE_LIMIT_WINDOW} seconds\")\nprint(f\"   Max Request Size: {config.MAX_REQUEST_SIZE / 1024:.0f}KB\")\nprint(f\"   Max Batch Size: {config.MAX_BATCH_SIZE / 1024:.0f}KB\")\nprint(f\"   Headers configured: {len(config.get_headers())} headers\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database and Table Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Create database if it doesn't exist\nspark.sql(f\"CREATE DATABASE IF NOT EXISTS {CATALOG_NAME}.{DATABASE_NAME}\")\nspark.sql(f\"USE {CATALOG_NAME}.{DATABASE_NAME}\")\n\nprint(f\"SUCCESS: Using database: {CATALOG_NAME}.{DATABASE_NAME}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Lake Schema Definitions\n",
    "\n",
    "Define schemas for Delta Lake tables that align with Customer.IO API data structures."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Schema for customers table (aligns with /identify endpoint)\ncustomers_schema = StructType([\n    StructField(\"customer_id\", StringType(), False),\n    StructField(\"user_id\", StringType(), True),\n    StructField(\"anonymous_id\", StringType(), True),\n    StructField(\"email\", StringType(), True),\n    StructField(\"created_at\", TimestampType(), True),\n    StructField(\"updated_at\", TimestampType(), True),\n    StructField(\"traits\", MapType(StringType(), StringType()), True),\n    StructField(\"custom_attributes\", MapType(StringType(), StringType()), True),\n    StructField(\"is_active\", BooleanType(), True),\n    StructField(\"last_seen\", TimestampType(), True),\n    StructField(\"source\", StringType(), True),\n    StructField(\"region\", StringType(), True)\n])\n\n# Schema for events table (aligns with /track endpoint)\nevents_schema = StructType([\n    StructField(\"event_id\", StringType(), False),\n    StructField(\"customer_id\", StringType(), True),\n    StructField(\"user_id\", StringType(), True),\n    StructField(\"anonymous_id\", StringType(), True),\n    StructField(\"event_name\", StringType(), False),\n    StructField(\"timestamp\", TimestampType(), False),\n    StructField(\"properties\", MapType(StringType(), StringType()), True),\n    StructField(\"context\", MapType(StringType(), StringType()), True),\n    StructField(\"is_semantic_event\", BooleanType(), True),\n    StructField(\"event_category\", StringType(), True),\n    StructField(\"source\", StringType(), True),\n    StructField(\"processed_at\", TimestampType(), True)\n])\n\n# Schema for groups table (aligns with /group endpoint)\ngroups_schema = StructType([\n    StructField(\"group_id\", StringType(), False),\n    StructField(\"group_type\", StringType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"created_at\", TimestampType(), True),\n    StructField(\"updated_at\", TimestampType(), True),\n    StructField(\"traits\", MapType(StringType(), StringType()), True),\n    StructField(\"parent_group_id\", StringType(), True),\n    StructField(\"is_active\", BooleanType(), True)\n])\n\n# Schema for devices table (device management)\ndevices_schema = StructType([\n    StructField(\"device_id\", StringType(), False),\n    StructField(\"customer_id\", StringType(), False),\n    StructField(\"device_token\", StringType(), False),\n    StructField(\"device_type\", StringType(), False),  # ios, android, web\n    StructField(\"platform\", StringType(), True),\n    StructField(\"app_version\", StringType(), True),\n    StructField(\"os_version\", StringType(), True),\n    StructField(\"created_at\", TimestampType(), True),\n    StructField(\"last_used\", TimestampType(), True),\n    StructField(\"is_active\", BooleanType(), True)\n])\n\n# Schema for API responses (logging and monitoring)\napi_responses_schema = StructType([\n    StructField(\"request_id\", StringType(), False),\n    StructField(\"endpoint\", StringType(), False),\n    StructField(\"method\", StringType(), False),\n    StructField(\"status_code\", IntegerType(), False),\n    StructField(\"response_time_ms\", IntegerType(), True),\n    StructField(\"request_size_bytes\", IntegerType(), True),\n    StructField(\"response_size_bytes\", IntegerType(), True),\n    StructField(\"timestamp\", TimestampType(), False),\n    StructField(\"error_message\", StringType(), True),\n    StructField(\"retry_count\", IntegerType(), True),\n    StructField(\"customer_id\", StringType(), True)\n])\n\n# Schema for batch operations tracking\nbatch_operations_schema = StructType([\n    StructField(\"batch_id\", StringType(), False),\n    StructField(\"operation_type\", StringType(), False),\n    StructField(\"total_records\", IntegerType(), False),\n    StructField(\"successful_records\", IntegerType(), True),\n    StructField(\"failed_records\", IntegerType(), True),\n    StructField(\"started_at\", TimestampType(), False),\n    StructField(\"completed_at\", TimestampType(), True),\n    StructField(\"status\", StringType(), False),  # pending, processing, completed, failed\n    StructField(\"error_summary\", ArrayType(StringType()), True)\n])\n\nprint(\"SUCCESS: Delta Lake schemas defined\")\nprint(f\"   Customers schema: {len(customers_schema.fields)} fields\")\nprint(f\"   Events schema: {len(events_schema.fields)} fields\")\nprint(f\"   Groups schema: {len(groups_schema.fields)} fields\")\nprint(f\"   Devices schema: {len(devices_schema.fields)} fields\")\nprint(f\"   API responses schema: {len(api_responses_schema.fields)} fields\")\nprint(f\"   Batch operations schema: {len(batch_operations_schema.fields)} fields\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Delta Lake Tables"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Type-safe function to create Delta table if it doesn't exist\nfrom typing import List, Optional\n\ndef create_delta_table_if_not_exists(\n    table_name: str, \n    schema: StructType, \n    partition_cols: Optional[List[str]] = None\n) -> bool:\n    \"\"\"\n    Create a Delta table if it doesn't already exist.\n    \n    Args:\n        table_name: Name of the table to create\n        schema: Spark StructType schema for the table\n        partition_cols: Optional list of columns to partition by\n        \n    Returns:\n        bool: True if table was created, False if it already existed\n        \n    Raises:\n        Exception: If table creation fails\n    \"\"\"\n    try:\n        # Check if table exists\n        spark.table(f\"{CATALOG_NAME}.{DATABASE_NAME}.{table_name}\")\n        print(f\"SUCCESS: Table {table_name} already exists\")\n        return False\n    except Exception:\n        try:\n            # Create empty DataFrame with schema\n            empty_df = spark.createDataFrame([], schema)\n            \n            # Write as Delta table\n            write_op = empty_df.write.format(\"delta\").mode(\"overwrite\")\n            \n            if partition_cols:\n                write_op = write_op.partitionBy(*partition_cols)\n            \n            write_op.saveAsTable(f\"{CATALOG_NAME}.{DATABASE_NAME}.{table_name}\")\n            print(f\"SUCCESS: Created Delta table: {table_name}\")\n            return True\n        except Exception as e:\n            print(f\"ERROR: Failed to create table {table_name}: {str(e)}\")\n            raise\n\n# Test-driven approach: Test table creation function\ndef test_table_creation():\n    \"\"\"Test table creation functionality.\"\"\"\n    print(\"TEST: Testing table creation functionality...\")\n    \n    # Test creating tables\n    tables_to_create = [\n        (\"customers\", customers_schema, [\"region\"]),\n        (\"events\", events_schema, [\"event_category\"]),\n        (\"groups\", groups_schema, None),\n        (\"devices\", devices_schema, [\"device_type\"]),\n        (\"api_responses\", api_responses_schema, None),\n        (\"batch_operations\", batch_operations_schema, None)\n    ]\n    \n    created_count = 0\n    for table_name, schema, partitions in tables_to_create:\n        try:\n            was_created = create_delta_table_if_not_exists(table_name, schema, partitions)\n            if was_created:\n                created_count += 1\n        except Exception as e:\n            print(f\"ERROR: Test failed for table {table_name}: {str(e)}\")\n            return False\n    \n    print(f\"SUCCESS: Table creation test passed - {created_count} new tables created\")\n    return True\n\n# Run table creation with testing\nif test_table_creation():\n    print(\"\\nSUCCESS: All Delta Lake tables created successfully\")\nelse:\n    print(\"\\nERROR: Table creation test failed\")\n    raise Exception(\"Failed to create required Delta Lake tables\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data for Demonstrations\n",
    "\n",
    "Create realistic synthetic data that matches Customer.IO API requirements for testing and demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Type-safe synthetic data generation with comprehensive testing\nfrom typing import Dict, List, Any, Optional\nfrom datetime import datetime\nimport uuid\n\ndef generate_synthetic_customers(num_customers: int = 1000) -> List[Dict[str, Any]]:\n    \"\"\"\n    Generate synthetic customer data for testing.\n    \n    Args:\n        num_customers: Number of customers to generate\n        \n    Returns:\n        List of customer dictionaries with required fields\n        \n    Raises:\n        ValueError: If num_customers is not positive\n        Exception: If data generation fails\n    \"\"\"\n    if num_customers <= 0:\n        raise ValueError(\"num_customers must be positive\")\n    \n    customers: List[Dict[str, Any]] = []\n    \n    try:\n        for i in range(num_customers):\n            customer_id = str(uuid.uuid4())\n            created_at = fake.date_time_between(start_date='-2y', end_date='now', tzinfo=tz.UTC)\n            \n            customer = {\n                \"customer_id\": customer_id,\n                \"user_id\": f\"user_{i+1:06d}\",\n                \"anonymous_id\": str(uuid.uuid4()) if fake.boolean(chance_of_getting_true=30) else None,\n                \"email\": fake.email(),\n                \"created_at\": created_at,\n                \"updated_at\": fake.date_time_between(start_date=created_at, end_date='now', tzinfo=tz.UTC),\n                \"traits\": {\n                    \"first_name\": fake.first_name(),\n                    \"last_name\": fake.last_name(),\n                    \"age\": str(fake.random_int(min=18, max=80)),\n                    \"city\": fake.city(),\n                    \"country\": fake.country(),\n                    \"plan\": fake.random_element([\"free\", \"basic\", \"premium\", \"enterprise\"]),\n                    \"signup_source\": fake.random_element([\"website\", \"mobile_app\", \"referral\", \"social\"])\n                },\n                \"custom_attributes\": {\n                    \"lifetime_value\": str(round(fake.random.uniform(0, 5000), 2)),\n                    \"last_purchase_amount\": str(round(fake.random.uniform(10, 500), 2)) if fake.boolean(chance_of_getting_true=60) else None,\n                    \"subscription_status\": fake.random_element([\"active\", \"canceled\", \"trial\", \"expired\"])\n                },\n                \"is_active\": fake.boolean(chance_of_getting_true=85),\n                \"last_seen\": fake.date_time_between(start_date='-30d', end_date='now', tzinfo=tz.UTC),\n                \"source\": \"synthetic_data\",\n                \"region\": CUSTOMERIO_REGION\n            }\n            customers.append(customer)\n    except Exception as e:\n        raise Exception(f\"Failed to generate customer data: {str(e)}\")\n    \n    return customers\n\ndef generate_synthetic_events(customers: List[Dict[str, Any]], num_events: int = 5000) -> List[Dict[str, Any]]:\n    \"\"\"\n    Generate synthetic event data for testing.\n    \n    Args:\n        customers: List of customer dictionaries\n        num_events: Number of events to generate\n        \n    Returns:\n        List of event dictionaries with required fields\n        \n    Raises:\n        ValueError: If customers list is empty or num_events is not positive\n        Exception: If event generation fails\n    \"\"\"\n    if not customers:\n        raise ValueError(\"customers list cannot be empty\")\n    if num_events <= 0:\n        raise ValueError(\"num_events must be positive\")\n    \n    events: List[Dict[str, Any]] = []\n    \n    # Define event types and their categories\n    event_types = {\n        \"ecommerce\": [\n            \"Product Viewed\", \"Product Added\", \"Cart Viewed\", \"Checkout Started\", \n            \"Order Completed\", \"Product Removed\", \"Coupon Applied\"\n        ],\n        \"engagement\": [\n            \"Page Viewed\", \"Button Clicked\", \"Form Submitted\", \"Video Played\", \n            \"Document Downloaded\", \"Search Performed\"\n        ],\n        \"lifecycle\": [\n            \"User Registered\", \"Profile Updated\", \"Settings Changed\", \"Account Upgraded\", \n            \"Subscription Canceled\", \"Password Reset\"\n        ],\n        \"mobile\": [\n            \"Application Opened\", \"Application Backgrounded\", \"Push Notification Clicked\",\n            \"Screen Viewed\", \"Feature Used\"\n        ]\n    }\n    \n    try:\n        for i in range(num_events):\n            customer = fake.random_element(customers)\n            category = fake.random_element(list(event_types.keys()))\n            event_name = fake.random_element(event_types[category])\n            \n            # Generate event properties based on category\n            properties: Dict[str, str] = {}\n            if category == \"ecommerce\":\n                properties.update({\n                    \"product_id\": f\"prod_{fake.random_int(min=1, max=1000)}\",\n                    \"product_name\": fake.catch_phrase(),\n                    \"price\": str(round(fake.random.uniform(9.99, 299.99), 2)),\n                    \"currency\": \"USD\",\n                    \"category\": fake.random_element([\"electronics\", \"clothing\", \"books\", \"home\", \"sports\"])\n                })\n            elif category == \"engagement\":\n                properties.update({\n                    \"page_url\": fake.url(),\n                    \"referrer\": fake.url() if fake.boolean(chance_of_getting_true=30) else \"\",\n                    \"session_id\": str(uuid.uuid4())\n                })\n            \n            event = {\n                \"event_id\": str(uuid.uuid4()),\n                \"customer_id\": customer[\"customer_id\"],\n                \"user_id\": customer[\"user_id\"],\n                \"anonymous_id\": customer.get(\"anonymous_id\"),\n                \"event_name\": event_name,\n                \"timestamp\": fake.date_time_between(start_date='-90d', end_date='now', tzinfo=tz.UTC),\n                \"properties\": properties,\n                \"context\": {\n                    \"ip\": fake.ipv4(),\n                    \"user_agent\": fake.user_agent(),\n                    \"locale\": fake.locale(),\n                    \"timezone\": str(fake.timezone())\n                },\n                \"is_semantic_event\": event_name in [item for sublist in event_types.values() for item in sublist[:3]],\n                \"event_category\": category,\n                \"source\": \"synthetic_data\",\n                \"processed_at\": datetime.now(tz.UTC)\n            }\n            events.append(event)\n    except Exception as e:\n        raise Exception(f\"Failed to generate event data: {str(e)}\")\n    \n    return events\n\n# Test-driven approach: Test data generation functions\ndef test_customer_generation():\n    \"\"\"Test customer data generation.\"\"\"\n    print(\"TEST: Testing customer data generation...\")\n    \n    # Test with valid parameters\n    test_customers = generate_synthetic_customers(5)\n    \n    # Validate structure\n    assert len(test_customers) == 5, \"Should generate exactly 5 customers\"\n    \n    for customer in test_customers:\n        # Test required fields\n        assert \"customer_id\" in customer, \"customer_id is required\"\n        assert \"user_id\" in customer, \"user_id is required\"\n        assert \"email\" in customer, \"email is required\"\n        assert \"traits\" in customer, \"traits is required\"\n        assert \"region\" in customer, \"region is required\"\n        \n        # Test data types\n        assert isinstance(customer[\"customer_id\"], str), \"customer_id should be string\"\n        assert isinstance(customer[\"traits\"], dict), \"traits should be dict\"\n        assert isinstance(customer[\"is_active\"], bool), \"is_active should be bool\"\n    \n    # Test error cases\n    try:\n        generate_synthetic_customers(0)\n        assert False, \"Should raise ValueError for zero customers\"\n    except ValueError:\n        pass  # Expected\n    \n    print(\"SUCCESS: Customer generation test passed\")\n    return test_customers\n\ndef test_event_generation():\n    \"\"\"Test event data generation.\"\"\"\n    print(\"TEST: Testing event data generation...\")\n    \n    # Generate test customers first\n    test_customers = generate_synthetic_customers(3)\n    \n    # Test event generation\n    test_events = generate_synthetic_events(test_customers, 10)\n    \n    # Validate structure\n    assert len(test_events) == 10, \"Should generate exactly 10 events\"\n    \n    for event in test_events:\n        # Test required fields\n        assert \"event_id\" in event, \"event_id is required\"\n        assert \"event_name\" in event, \"event_name is required\"\n        assert \"customer_id\" in event, \"customer_id is required\"\n        assert \"timestamp\" in event, \"timestamp is required\"\n        assert \"event_category\" in event, \"event_category is required\"\n        \n        # Test data types\n        assert isinstance(event[\"event_id\"], str), \"event_id should be string\"\n        assert isinstance(event[\"properties\"], dict), \"properties should be dict\"\n        assert isinstance(event[\"context\"], dict), \"context should be dict\"\n        assert isinstance(event[\"is_semantic_event\"], bool), \"is_semantic_event should be bool\"\n    \n    # Test error cases\n    try:\n        generate_synthetic_events([], 5)\n        assert False, \"Should raise ValueError for empty customers\"\n    except ValueError:\n        pass  # Expected\n    \n    print(\"SUCCESS: Event generation test passed\")\n    return test_events\n\n# Run tests and generate data\nprint(\"Running data generation tests...\")\ntest_customers = test_customer_generation()\ntest_events = test_event_generation()\n\nprint(\"\\nGenerating full synthetic dataset...\")\ntry:\n    synthetic_customers = generate_synthetic_customers(1000)\n    synthetic_events = generate_synthetic_events(synthetic_customers, 5000)\n    print(f\"SUCCESS: Generated {len(synthetic_customers)} customers and {len(synthetic_events)} events\")\nexcept Exception as e:\n    print(f\"ERROR: Data generation failed: {str(e)}\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Synthetic Data into Delta Tables"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Convert synthetic data to Spark DataFrames and load into Delta tables\n\n# Load customers data\ncustomers_df = spark.createDataFrame(synthetic_customers, customers_schema)\ncustomers_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{CATALOG_NAME}.{DATABASE_NAME}.customers\")\n\n# Load events data\nevents_df = spark.createDataFrame(synthetic_events, events_schema)\nevents_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{CATALOG_NAME}.{DATABASE_NAME}.events\")\n\nprint(\"SUCCESS: Synthetic data loaded into Delta tables\")\n\n# Show sample data\nprint(\"\\nSample customer data:\")\nspark.table(f\"{CATALOG_NAME}.{DATABASE_NAME}.customers\").select(\"customer_id\", \"email\", \"traits\", \"is_active\").show(3, truncate=False)\n\nprint(\"\\nSample event data:\")\nspark.table(f\"{CATALOG_NAME}.{DATABASE_NAME}.events\").select(\"event_name\", \"customer_id\", \"timestamp\", \"event_category\").show(5, truncate=False)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Comprehensive validation with error handling and circuit breaker patterns\nfrom typing import List, Tuple, Dict, Any\nimport time\nfrom dataclasses import dataclass\n\n@dataclass\nclass ValidationResult:\n    \"\"\"Type-safe validation result.\"\"\"\n    status: str  # \"SUCCESS\", \"ERROR\", \"WARNING\"\n    component: str\n    result: str\n    error: Optional[Exception] = None\n\nclass CircuitBreaker:\n    \"\"\"Circuit breaker for validation operations.\"\"\"\n    \n    def __init__(self, failure_threshold: int = 3, timeout: int = 60):\n        self.failure_threshold = failure_threshold\n        self.timeout = timeout\n        self.failure_count = 0\n        self.last_failure_time = 0\n        self.state = \"closed\"  # closed, open, half-open\n    \n    def call(self, func, *args, **kwargs):\n        \"\"\"Execute function with circuit breaker protection.\"\"\"\n        if self.state == \"open\":\n            if time.time() - self.last_failure_time > self.timeout:\n                self.state = \"half-open\"\n            else:\n                raise Exception(\"Circuit breaker is open\")\n        \n        try:\n            result = func(*args, **kwargs)\n            if self.state == \"half-open\":\n                self.state = \"closed\"\n                self.failure_count = 0\n            return result\n        except Exception as e:\n            self.failure_count += 1\n            self.last_failure_time = time.time()\n            \n            if self.failure_count >= self.failure_threshold:\n                self.state = \"open\"\n            \n            raise e\n\n# Initialize circuit breaker for validation\nvalidation_breaker = CircuitBreaker(failure_threshold=2, timeout=30)\n\ndef validate_database_access() -> ValidationResult:\n    \"\"\"Validate database access with error handling.\"\"\"\n    try:\n        spark.sql(f\"USE {CATALOG_NAME}.{DATABASE_NAME}\")\n        return ValidationResult(\"SUCCESS\", \"Database access\", \"OK\")\n    except Exception as e:\n        return ValidationResult(\"ERROR\", \"Database access\", f\"Failed: {str(e)}\", e)\n\ndef validate_table_exists(table_name: str) -> ValidationResult:\n    \"\"\"Validate that a table exists.\"\"\"\n    try:\n        spark.table(f\"{CATALOG_NAME}.{DATABASE_NAME}.{table_name}\")\n        return ValidationResult(\"SUCCESS\", f\"Table {table_name}\", \"Exists\")\n    except Exception as e:\n        return ValidationResult(\"ERROR\", f\"Table {table_name}\", \"Missing\", e)\n\ndef validate_data_quality(table_name: str) -> ValidationResult:\n    \"\"\"Validate data quality in a table.\"\"\"\n    try:\n        df = spark.table(f\"{CATALOG_NAME}.{DATABASE_NAME}.{table_name}\")\n        count = df.count()\n        \n        if count > 0:\n            return ValidationResult(\"SUCCESS\", f\"{table_name} data\", f\"{count} records\")\n        else:\n            return ValidationResult(\"WARNING\", f\"{table_name} data\", \"No records\")\n    except Exception as e:\n        return ValidationResult(\"ERROR\", f\"{table_name} data\", f\"Failed: {str(e)}\", e)\n\ndef validate_api_configuration() -> ValidationResult:\n    \"\"\"Validate API configuration.\"\"\"\n    try:\n        if not config.api_key:\n            return ValidationResult(\"ERROR\", \"API configuration\", \"Missing API key\")\n        \n        if not config.base_url:\n            return ValidationResult(\"ERROR\", \"API configuration\", \"Missing base URL\")\n        \n        # Test header generation\n        headers = config.get_headers()\n        if not headers.get(\"Authorization\"):\n            return ValidationResult(\"ERROR\", \"API configuration\", \"Invalid authorization header\")\n        \n        if ENVIRONMENT == \"test\":\n            return ValidationResult(\"WARNING\", \"API configuration\", \"Test mode - mock key\")\n        else:\n            return ValidationResult(\"SUCCESS\", \"API configuration\", \"Valid\")\n            \n    except Exception as e:\n        return ValidationResult(\"ERROR\", \"API configuration\", f\"Failed: {str(e)}\", e)\n\ndef validate_setup() -> Dict[str, Any]:\n    \"\"\"\n    Comprehensive setup validation with error handling and circuit breaker.\n    \n    Returns:\n        Dict containing validation results and summary\n    \"\"\"\n    print(\"ANALYSIS: Running comprehensive setup validation...\")\n    \n    validations: List[ValidationResult] = []\n    \n    # Database validation\n    try:\n        result = validation_breaker.call(validate_database_access)\n        validations.append(result)\n    except Exception as e:\n        validations.append(ValidationResult(\"ERROR\", \"Database access\", f\"Circuit breaker: {str(e)}\", e))\n    \n    # Table validation\n    required_tables = [\"customers\", \"events\", \"groups\", \"devices\", \"api_responses\", \"batch_operations\"]\n    for table in required_tables:\n        try:\n            result = validation_breaker.call(validate_table_exists, table)\n            validations.append(result)\n        except Exception as e:\n            validations.append(ValidationResult(\"ERROR\", f\"Table {table}\", f\"Circuit breaker: {str(e)}\", e))\n    \n    # Data quality validation\n    data_tables = [\"customers\", \"events\"]\n    for table in data_tables:\n        try:\n            result = validation_breaker.call(validate_data_quality, table)\n            validations.append(result)\n        except Exception as e:\n            validations.append(ValidationResult(\"ERROR\", f\"{table} data\", f\"Circuit breaker: {str(e)}\", e))\n    \n    # API configuration validation\n    try:\n        result = validation_breaker.call(validate_api_configuration)\n        validations.append(result)\n    except Exception as e:\n        validations.append(ValidationResult(\"ERROR\", \"API configuration\", f\"Circuit breaker: {str(e)}\", e))\n    \n    # Print detailed results\n    print(\"Setup Validation Results:\")\n    print(\"=\" * 60)\n    \n    for validation in validations:\n        print(f\"{validation.status} {validation.component:<25} {validation.result}\")\n        if validation.error and validation.status == \"ERROR\":\n            print(f\"    Error: {type(validation.error).__name__}: {str(validation.error)}\")\n    \n    # Calculate summary\n    passed = sum(1 for v in validations if v.status == \"SUCCESS\")\n    warnings = sum(1 for v in validations if v.status == \"WARNING\")\n    failed = sum(1 for v in validations if v.status == \"ERROR\")\n    total = len(validations)\n    \n    print(f\"\\nValidation Summary:\")\n    print(f\"  SUCCESS: Passed: {passed}\")\n    print(f\"  WARNING: Warnings: {warnings}\")\n    print(f\"  ERROR: Failed: {failed}\")\n    print(f\"  DATA: Total: {total}\")\n    \n    # Determine overall status\n    if failed == 0:\n        if warnings == 0:\n            print(\"COMPLETED: All validation checks passed! Ready to proceed.\")\n            overall_status = \"success\"\n        else:\n            print(\"WARNING: Validation passed with warnings. Review before proceeding.\")\n            overall_status = \"warning\"\n    else:\n        print(\"ERROR: Some validation checks failed. Please fix issues before proceeding.\")\n        overall_status = \"failed\"\n    \n    # Return detailed results for programmatic use\n    return {\n        \"overall_status\": overall_status,\n        \"passed\": passed,\n        \"warnings\": warnings,\n        \"failed\": failed,\n        \"total\": total,\n        \"validations\": validations,\n        \"circuit_breaker_state\": validation_breaker.state\n    }\n\ndef test_validation_function():\n    \"\"\"Test the validation function itself.\"\"\"\n    print(\"TEST: Testing validation function...\")\n    \n    try:\n        # Test validation function\n        results = validate_setup()\n        \n        # Validate structure\n        assert \"overall_status\" in results, \"overall_status is required\"\n        assert \"validations\" in results, \"validations is required\"\n        assert isinstance(results[\"validations\"], list), \"validations should be list\"\n        \n        # Test that we have some validations\n        assert len(results[\"validations\"]) > 0, \"Should have validation results\"\n        \n        # Test ValidationResult structure\n        for validation in results[\"validations\"]:\n            assert hasattr(validation, \"status\"), \"ValidationResult should have status\"\n            assert hasattr(validation, \"component\"), \"ValidationResult should have component\"\n            assert hasattr(validation, \"result\"), \"ValidationResult should have result\"\n        \n        print(\"SUCCESS: Validation function test passed\")\n        return True\n    except Exception as e:\n        print(f\"ERROR: Validation function test failed: {str(e)}\")\n        return False\n\n# Test and run validation\nif test_validation_function():\n    validation_results = validate_setup()\n    \n    # Raise exception if critical validations failed\n    if validation_results[\"overall_status\"] == \"failed\":\n        critical_failures = [v for v in validation_results[\"validations\"] \n                           if v.status == \"ERROR\" and \"Database\" in v.component]\n        \n        if critical_failures:\n            raise Exception(\"Critical validation failures detected - cannot proceed\")\nelse:\n    raise Exception(\"Validation function test failed\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Configuration Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Display recommended cluster configuration\nprint(\"Recommended Databricks Cluster Configuration:\")\nprint(\"=\" * 50)\n\nrecommended_config = {\n    \"Databricks Runtime\": \"11.3.x-scala2.12 or higher\",\n    \"Node Type (Driver)\": \"Standard_DS3_v2 (14 GB Memory, 4 Cores)\",\n    \"Node Type (Workers)\": \"Standard_DS3_v2 (14 GB Memory, 4 Cores)\",\n    \"Workers\": \"2-4 (autoscaling enabled)\",\n    \"Auto Termination\": \"120 minutes\",\n    \"Spark Config\": {\n        \"spark.sql.adaptive.enabled\": \"true\",\n        \"spark.sql.adaptive.coalescePartitions.enabled\": \"true\",\n        \"spark.sql.adaptive.coalescePartitions.minPartitionNum\": \"1\",\n        \"spark.sql.adaptive.coalescePartitions.initialPartitionNum\": \"200\",\n        \"spark.sql.adaptive.skewJoin.enabled\": \"true\",\n        \"spark.databricks.delta.preview.enabled\": \"true\",\n        \"spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite\": \"true\",\n        \"spark.databricks.delta.properties.defaults.autoOptimize.autoCompact\": \"true\"\n    },\n    \"Environment Variables\": {\n        \"CUSTOMERIO_REGION\": CUSTOMERIO_REGION,\n        \"DATABRICKS_ENV\": ENVIRONMENT\n    }\n}\n\nfor key, value in recommended_config.items():\n    if isinstance(value, dict):\n        print(f\"{key}:\")\n        for sub_key, sub_value in value.items():\n            print(f\"  {sub_key}: {sub_value}\")\n    else:\n        print(f\"{key}: {value}\")\n\nprint(\"\\nNOTE: These configurations are optimized for Customer.IO API workloads\")\nprint(\"   with Delta Lake and structured streaming capabilities.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Next Steps\n\nThis notebook has successfully set up the foundation for working with Customer.IO's Data Pipelines API in Databricks:\n\nSUCCESS: **Environment configured** with required packages and libraries\nSUCCESS: **API configuration** set up with regional endpoints and rate limiting\nSUCCESS: **Delta Lake tables** created with proper schemas\nSUCCESS: **Synthetic data** generated and loaded for testing\nSUCCESS: **Setup validation** completed successfully\n\n### Ready for Next Notebooks:\n\n1. **01_authentication_and_utilities.ipynb** - Build the Customer.IO API client and utility functions\n2. **02_people_management.ipynb** - Implement user identification and lifecycle management\n3. **03_events_and_tracking.ipynb** - Event tracking and custom event implementation\n\n### Key Resources Created:\n\n- **Database**: `{CATALOG_NAME}.{DATABASE_NAME}`\n- **Tables**: customers, events, groups, devices, api_responses, batch_operations\n- **Configuration**: Customer.IO API settings and Databricks optimization\n- **Sample Data**: 1,000 customers and 5,000 events for testing\n\n### Important Notes:\n\n- All sensitive data (API keys) should be stored in Databricks secrets\n- Rate limiting is configured for 3000 requests per 3 seconds\n- Regional endpoints are properly configured for data residency\n- Delta Lake tables are optimized for analytical workloads",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}