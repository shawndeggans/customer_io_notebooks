{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer.IO Data Pipelines API - Advanced Pipeline Integration\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook demonstrates advanced data pipeline integration and orchestration with Customer.IO's Data Pipelines API.\n",
    "It covers pipeline design patterns, data flow orchestration, transformation pipelines, scheduling, dependency management, and integration with external data sources.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Complete setup from `00_setup_and_configuration.ipynb`\n",
    "- Complete authentication setup from `01_authentication_and_utilities.ipynb`\n",
    "- Understanding of batch operations from `09_batch_operations.ipynb`\n",
    "- Customer.IO API key configured in Databricks secrets\n",
    "- Understanding of data pipeline concepts and ETL processes\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **Pipeline Orchestration**: Coordinating complex data workflows\n",
    "- **Data Transformation**: ETL processes and data quality management\n",
    "- **Dependency Management**: Pipeline dependencies and execution order\n",
    "- **Scheduling**: Time-based and event-driven pipeline execution\n",
    "- **Data Quality**: Validation, cleansing, and enrichment\n",
    "- **Integration Patterns**: Connecting multiple data sources and systems\n",
    "\n",
    "## Pipeline Operations Covered\n",
    "\n",
    "1. **Pipeline Design**: Multi-stage pipelines with dependencies\n",
    "2. **Data Ingestion**: Batch and streaming data ingestion patterns\n",
    "3. **Transformation**: Data cleansing, enrichment, and standardization\n",
    "4. **Orchestration**: Workflow management and execution coordination\n",
    "5. **Monitoring**: Pipeline health, performance, and data quality metrics\n",
    "6. **Integration**: External systems, APIs, and data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "import concurrent.futures\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from typing import Dict, List, Optional, Any, Union, Tuple, Callable, Iterator\n",
    "import json\n",
    "import uuid\n",
    "from enum import Enum\n",
    "from collections import defaultdict, deque\n",
    "import time\n",
    "import threading\n",
    "import queue\n",
    "import statistics\n",
    "from dataclasses import dataclass, field\n",
    "import math\n",
    "import hashlib\n",
    "import re\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "print(\"SUCCESS: Standard libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add utils directory to Python path\n",
    "sys.path.append('/Workspace/Repos/customer_io_notebooks/utils')\n",
    "print(\"SUCCESS: Utils directory added to Python path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Customer.IO API utilities\n",
    "from utils.api_client import CustomerIOClient\n",
    "from utils.event_manager import EventManager\n",
    "from utils.people_manager import PeopleManager\n",
    "from utils.validators import (\n",
    "    EventRequest,\n",
    "    PersonRequest,\n",
    "    validate_request_size,\n",
    "    create_context\n",
    ")\n",
    "\n",
    "print(\"SUCCESS: Customer.IO API utilities imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import transformation utilities\n",
    "from utils.transformers import (\n",
    "    BatchTransformer,\n",
    "    ContextTransformer\n",
    ")\n",
    "\n",
    "print(\"SUCCESS: Transformation utilities imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import error handling utilities\n",
    "from utils.error_handlers import (\n",
    "    CustomerIOError,\n",
    "    RateLimitError,\n",
    "    ValidationError,\n",
    "    NetworkError,\n",
    "    retry_on_error,\n",
    "    ErrorContext\n",
    ")\n",
    "\n",
    "print(\"SUCCESS: Error handling utilities imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Databricks and Spark utilities\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "print(\"SUCCESS: Databricks and Spark utilities imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import validation and logging\n",
    "import structlog\n",
    "from pydantic import ValidationError as PydanticValidationError, BaseModel, Field, validator\n",
    "\n",
    "# Initialize logger\n",
    "logger = structlog.get_logger(\"pipeline_integration\")\n",
    "\n",
    "print(\"SUCCESS: Validation and logging initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Client Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from setup notebook (secure approach)\n",
    "try:\n",
    "    CUSTOMERIO_REGION = dbutils.widgets.get(\"customerio_region\") or \"us\"\n",
    "    DATABASE_NAME = dbutils.widgets.get(\"database_name\") or \"customerio_demo\"\n",
    "    CATALOG_NAME = dbutils.widgets.get(\"catalog_name\") or \"main\"\n",
    "    ENVIRONMENT = dbutils.widgets.get(\"environment\") or \"test\"\n",
    "    \n",
    "    print(f\"Configuration loaded from setup notebook:\")\n",
    "    print(f\"  Region: {CUSTOMERIO_REGION}\")\n",
    "    print(f\"  Database: {CATALOG_NAME}.{DATABASE_NAME}\")\n",
    "    print(f\"  Environment: {ENVIRONMENT}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"WARNING: Could not load configuration from setup notebook: {str(e)}\")\n",
    "    print(\"INFO: Using fallback configuration\")\n",
    "    CUSTOMERIO_REGION = \"us\"\n",
    "    DATABASE_NAME = \"customerio_demo\"\n",
    "    CATALOG_NAME = \"main\"\n",
    "    ENVIRONMENT = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Customer.IO API key from secure storage\n",
    "CUSTOMERIO_API_KEY = dbutils.secrets.get(\"customerio\", \"api_key\")\n",
    "print(\"SUCCESS: Customer.IO API key retrieved from secure storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Spark to use the specified database\n",
    "spark.sql(f\"USE {CATALOG_NAME}.{DATABASE_NAME}\")\n",
    "print(\"SUCCESS: Database configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Customer.IO client and managers\n",
    "try:\n",
    "    client = CustomerIOClient(\n",
    "        api_key=CUSTOMERIO_API_KEY,\n",
    "        region=CUSTOMERIO_REGION,\n",
    "        timeout=30,\n",
    "        max_retries=3,\n",
    "        retry_backoff_factor=2.0,\n",
    "        enable_logging=True,\n",
    "        spark_session=spark\n",
    "    )\n",
    "    \n",
    "    # Initialize managers\n",
    "    event_manager = EventManager(client)\n",
    "    people_manager = PeopleManager(client)\n",
    "    \n",
    "    print(\"SUCCESS: Customer.IO client and managers initialized for pipeline integration\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to initialize Customer.IO client: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-Driven Development: Pipeline Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function: Validate pipeline stage configuration\n",
    "def test_pipeline_stage_validation():\n",
    "    \"\"\"Test that pipeline stages have proper configuration.\"\"\"\n",
    "    \n",
    "    # Test valid pipeline stage\n",
    "    valid_stage = {\n",
    "        \"stage_id\": \"data_ingestion\",\n",
    "        \"stage_name\": \"Data Ingestion\",\n",
    "        \"stage_type\": \"extract\",\n",
    "        \"dependencies\": [],\n",
    "        \"timeout_minutes\": 30,\n",
    "        \"retry_attempts\": 3,\n",
    "        \"parallel_execution\": True,\n",
    "        \"data_quality_checks\": True,\n",
    "        \"configuration\": {\n",
    "            \"source\": \"database\",\n",
    "            \"batch_size\": 1000\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Validate required fields\n",
    "    required_fields = [\"stage_id\", \"stage_name\", \"stage_type\", \"timeout_minutes\"]\n",
    "    for field in required_fields:\n",
    "        if field not in valid_stage:\n",
    "            print(f\"ERROR: Missing required pipeline stage field: {field}\")\n",
    "            return False\n",
    "    \n",
    "    # Validate stage types\n",
    "    valid_stage_types = [\"extract\", \"transform\", \"load\", \"validate\", \"monitor\"]\n",
    "    if valid_stage[\"stage_type\"] not in valid_stage_types:\n",
    "        print(f\"ERROR: Invalid stage type: {valid_stage['stage_type']}\")\n",
    "        return False\n",
    "    \n",
    "    # Validate timeout\n",
    "    if valid_stage[\"timeout_minutes\"] <= 0 or valid_stage[\"timeout_minutes\"] > 1440:  # Max 24 hours\n",
    "        print(\"ERROR: Timeout must be between 1 and 1440 minutes\")\n",
    "        return False\n",
    "    \n",
    "    print(\"SUCCESS: Pipeline stage validation test passed\")\n",
    "    return True\n",
    "\n",
    "# Run the test\n",
    "test_pipeline_stage_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function: Validate data quality metrics\n",
    "def test_data_quality_validation():\n",
    "    \"\"\"Test that data quality metrics have proper structure.\"\"\"\n",
    "    \n",
    "    # Test valid data quality metrics\n",
    "    quality_metrics = {\n",
    "        \"total_records\": 10000,\n",
    "        \"valid_records\": 9850,\n",
    "        \"invalid_records\": 150,\n",
    "        \"duplicate_records\": 20,\n",
    "        \"null_values\": 75,\n",
    "        \"data_completeness_percent\": 98.5,\n",
    "        \"data_accuracy_percent\": 99.2,\n",
    "        \"schema_compliance_percent\": 100.0,\n",
    "        \"quality_score\": 0.972,\n",
    "        \"validation_rules_passed\": 45,\n",
    "        \"validation_rules_failed\": 3\n",
    "    }\n",
    "    \n",
    "    # Validate required fields\n",
    "    required_fields = [\"total_records\", \"valid_records\", \"invalid_records\", \"quality_score\"]\n",
    "    for field in required_fields:\n",
    "        if field not in quality_metrics:\n",
    "            print(f\"ERROR: Missing required quality metrics field: {field}\")\n",
    "            return False\n",
    "    \n",
    "    # Validate record counts\n",
    "    total = quality_metrics[\"total_records\"]\n",
    "    valid = quality_metrics[\"valid_records\"]\n",
    "    invalid = quality_metrics[\"invalid_records\"]\n",
    "    \n",
    "    if valid + invalid != total:\n",
    "        print(f\"ERROR: Record counts don't match: {valid} + {invalid} != {total}\")\n",
    "        return False\n",
    "    \n",
    "    # Validate quality score range\n",
    "    quality_score = quality_metrics[\"quality_score\"]\n",
    "    if not (0.0 <= quality_score <= 1.0):\n",
    "        print(f\"ERROR: Quality score must be between 0.0 and 1.0: {quality_score}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"SUCCESS: Data quality validation test passed\")\n",
    "    return True\n",
    "\n",
    "# Run the test\n",
    "test_data_quality_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function: Validate pipeline dependency graph\n",
    "def test_pipeline_dependency_validation():\n",
    "    \"\"\"Test that pipeline dependencies form a valid DAG (no cycles).\"\"\"\n",
    "    \n",
    "    # Test pipeline with dependencies\n",
    "    pipeline_stages = {\n",
    "        \"extract_users\": {\"dependencies\": []},\n",
    "        \"extract_events\": {\"dependencies\": []},\n",
    "        \"transform_users\": {\"dependencies\": [\"extract_users\"]},\n",
    "        \"transform_events\": {\"dependencies\": [\"extract_events\"]},\n",
    "        \"enrich_data\": {\"dependencies\": [\"transform_users\", \"transform_events\"]},\n",
    "        \"load_customerio\": {\"dependencies\": [\"enrich_data\"]}\n",
    "    }\n",
    "    \n",
    "    # Check for cycles using DFS\n",
    "    def has_cycle(stages):\n",
    "        visited = set()\n",
    "        rec_stack = set()\n",
    "        \n",
    "        def dfs(stage):\n",
    "            if stage in rec_stack:\n",
    "                return True  # Cycle detected\n",
    "            if stage in visited:\n",
    "                return False\n",
    "            \n",
    "            visited.add(stage)\n",
    "            rec_stack.add(stage)\n",
    "            \n",
    "            for dependency in stages.get(stage, {}).get(\"dependencies\", []):\n",
    "                if dfs(dependency):\n",
    "                    return True\n",
    "            \n",
    "            rec_stack.remove(stage)\n",
    "            return False\n",
    "        \n",
    "        for stage in stages:\n",
    "            if dfs(stage):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    if has_cycle(pipeline_stages):\n",
    "        print(\"ERROR: Pipeline contains circular dependencies\")\n",
    "        return False\n",
    "    \n",
    "    # Validate all dependencies exist\n",
    "    for stage, config in pipeline_stages.items():\n",
    "        for dep in config.get(\"dependencies\", []):\n",
    "            if dep not in pipeline_stages:\n",
    "                print(f\"ERROR: Dependency '{dep}' for stage '{stage}' does not exist\")\n",
    "                return False\n",
    "    \n",
    "    print(\"SUCCESS: Pipeline dependency validation test passed\")\n",
    "    return True\n",
    "\n",
    "# Run the test\n",
    "test_pipeline_dependency_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Data Types and Enumerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline-specific enumerations\n",
    "class PipelineStageType(str, Enum):\n",
    "    \"\"\"Enumeration for pipeline stage types.\"\"\"\n",
    "    EXTRACT = \"extract\"\n",
    "    TRANSFORM = \"transform\"\n",
    "    LOAD = \"load\"\n",
    "    VALIDATE = \"validate\"\n",
    "    MONITOR = \"monitor\"\n",
    "    NOTIFY = \"notify\"\n",
    "\n",
    "class PipelineStatus(str, Enum):\n",
    "    \"\"\"Enumeration for pipeline execution status.\"\"\"\n",
    "    PENDING = \"pending\"\n",
    "    RUNNING = \"running\"\n",
    "    COMPLETED = \"completed\"\n",
    "    FAILED = \"failed\"\n",
    "    CANCELLED = \"cancelled\"\n",
    "    SKIPPED = \"skipped\"\n",
    "\n",
    "class ExecutionStrategy(str, Enum):\n",
    "    \"\"\"Enumeration for execution strategies.\"\"\"\n",
    "    SEQUENTIAL = \"sequential\"\n",
    "    PARALLEL = \"parallel\"\n",
    "    CONDITIONAL = \"conditional\"\n",
    "    HYBRID = \"hybrid\"\n",
    "\n",
    "class DataSourceType(str, Enum):\n",
    "    \"\"\"Enumeration for data source types.\"\"\"\n",
    "    DATABASE = \"database\"\n",
    "    FILE_SYSTEM = \"file_system\"\n",
    "    API = \"api\"\n",
    "    STREAM = \"stream\"\n",
    "    WAREHOUSE = \"warehouse\"\n",
    "    LAKE = \"lake\"\n",
    "\n",
    "class DataQualityRule(str, Enum):\n",
    "    \"\"\"Enumeration for data quality rule types.\"\"\"\n",
    "    NOT_NULL = \"not_null\"\n",
    "    UNIQUE = \"unique\"\n",
    "    FORMAT_VALIDATION = \"format_validation\"\n",
    "    RANGE_CHECK = \"range_check\"\n",
    "    REFERENCE_CHECK = \"reference_check\"\n",
    "    CUSTOM_RULE = \"custom_rule\"\n",
    "\n",
    "class TriggerType(str, Enum):\n",
    "    \"\"\"Enumeration for pipeline trigger types.\"\"\"\n",
    "    SCHEDULE = \"schedule\"\n",
    "    EVENT = \"event\"\n",
    "    MANUAL = \"manual\"\n",
    "    DATA_ARRIVAL = \"data_arrival\"\n",
    "    DEPENDENCY = \"dependency\"\n",
    "\n",
    "print(\"SUCCESS: Pipeline enumerations defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type-Safe Pipeline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data quality metrics model\n",
    "class DataQualityMetrics(BaseModel):\n",
    "    \"\"\"Type-safe data quality metrics model.\"\"\"\n",
    "    total_records: int = Field(..., ge=0, description=\"Total number of records\")\n",
    "    valid_records: int = Field(..., ge=0, description=\"Number of valid records\")\n",
    "    invalid_records: int = Field(..., ge=0, description=\"Number of invalid records\")\n",
    "    duplicate_records: int = Field(default=0, ge=0, description=\"Number of duplicate records\")\n",
    "    null_values: int = Field(default=0, ge=0, description=\"Number of null values\")\n",
    "    data_completeness_percent: float = Field(default=0.0, ge=0.0, le=100.0, description=\"Data completeness percentage\")\n",
    "    data_accuracy_percent: float = Field(default=0.0, ge=0.0, le=100.0, description=\"Data accuracy percentage\")\n",
    "    schema_compliance_percent: float = Field(default=0.0, ge=0.0, le=100.0, description=\"Schema compliance percentage\")\n",
    "    quality_score: float = Field(default=0.0, ge=0.0, le=1.0, description=\"Overall quality score\")\n",
    "    validation_rules_passed: int = Field(default=0, ge=0, description=\"Number of validation rules passed\")\n",
    "    validation_rules_failed: int = Field(default=0, ge=0, description=\"Number of validation rules failed\")\n",
    "    quality_issues: List[str] = Field(default_factory=list, description=\"List of quality issues\")\n",
    "    \n",
    "    @validator('valid_records', 'invalid_records')\n",
    "    def validate_record_counts(cls, v: int, values: Dict) -> int:\n",
    "        \"\"\"Validate record counts are consistent.\"\"\"\n",
    "        if 'total_records' in values:\n",
    "            total = values['total_records']\n",
    "            if v > total:\n",
    "                raise ValueError(f\"Record count {v} cannot exceed total {total}\")\n",
    "        return v\n",
    "    \n",
    "    def calculate_derived_metrics(self) -> None:\n",
    "        \"\"\"Calculate derived quality metrics.\"\"\"\n",
    "        if self.total_records > 0:\n",
    "            self.data_completeness_percent = (self.valid_records / self.total_records) * 100\n",
    "            \n",
    "            # Calculate accuracy (considering duplicates and nulls)\n",
    "            clean_records = self.valid_records - self.duplicate_records\n",
    "            self.data_accuracy_percent = (clean_records / self.total_records) * 100\n",
    "            \n",
    "            # Calculate overall quality score\n",
    "            completeness_weight = 0.4\n",
    "            accuracy_weight = 0.4\n",
    "            compliance_weight = 0.2\n",
    "            \n",
    "            self.quality_score = (\n",
    "                (self.data_completeness_percent / 100 * completeness_weight) +\n",
    "                (self.data_accuracy_percent / 100 * accuracy_weight) +\n",
    "                (self.schema_compliance_percent / 100 * compliance_weight)\n",
    "            )\n",
    "    \n",
    "    def is_acceptable_quality(self, threshold: float = 0.8) -> bool:\n",
    "        \"\"\"Check if data quality meets threshold.\"\"\"\n",
    "        return self.quality_score >= threshold\n",
    "    \n",
    "    class Config:\n",
    "        \"\"\"Pydantic model configuration.\"\"\"\n",
    "        validate_assignment = True\n",
    "\n",
    "print(\"SUCCESS: DataQualityMetrics model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline stage model\n",
    "class PipelineStage(BaseModel):\n",
    "    \"\"\"Type-safe pipeline stage model.\"\"\"\n",
    "    stage_id: str = Field(..., description=\"Unique stage identifier\")\n",
    "    stage_name: str = Field(..., description=\"Human-readable stage name\")\n",
    "    stage_type: PipelineStageType = Field(..., description=\"Type of pipeline stage\")\n",
    "    dependencies: List[str] = Field(default_factory=list, description=\"Stage dependencies\")\n",
    "    timeout_minutes: int = Field(default=30, gt=0, le=1440, description=\"Stage timeout in minutes\")\n",
    "    retry_attempts: int = Field(default=3, ge=0, le=10, description=\"Maximum retry attempts\")\n",
    "    parallel_execution: bool = Field(default=True, description=\"Allow parallel execution\")\n",
    "    data_quality_checks: bool = Field(default=True, description=\"Enable data quality checks\")\n",
    "    configuration: Dict[str, Any] = Field(default_factory=dict, description=\"Stage configuration\")\n",
    "    expected_runtime_minutes: Optional[float] = Field(None, gt=0, description=\"Expected runtime\")\n",
    "    resource_requirements: Dict[str, Any] = Field(default_factory=dict, description=\"Resource requirements\")\n",
    "    \n",
    "    # Execution tracking\n",
    "    status: PipelineStatus = Field(default=PipelineStatus.PENDING)\n",
    "    started_at: Optional[datetime] = Field(None, description=\"Stage start time\")\n",
    "    completed_at: Optional[datetime] = Field(None, description=\"Stage completion time\")\n",
    "    execution_time_minutes: Optional[float] = Field(None, ge=0, description=\"Actual execution time\")\n",
    "    retry_count: int = Field(default=0, ge=0, description=\"Current retry count\")\n",
    "    error_message: Optional[str] = Field(None, description=\"Error message if failed\")\n",
    "    quality_metrics: Optional[DataQualityMetrics] = Field(None, description=\"Quality metrics\")\n",
    "    \n",
    "    @validator('stage_id')\n",
    "    def validate_stage_id(cls, v: str) -> str:\n",
    "        \"\"\"Validate stage ID format.\"\"\"\n",
    "        if not v or len(v.strip()) == 0:\n",
    "            raise ValueError(\"Stage ID cannot be empty\")\n",
    "        if not re.match(r'^[a-zA-Z0-9_]+$', v):\n",
    "            raise ValueError(\"Stage ID must contain only alphanumeric characters and underscores\")\n",
    "        return v.strip()\n",
    "    \n",
    "    def can_execute(self, completed_stages: Set[str]) -> bool:\n",
    "        \"\"\"Check if stage can execute based on dependencies.\"\"\"\n",
    "        return all(dep in completed_stages for dep in self.dependencies)\n",
    "    \n",
    "    def get_execution_duration(self) -> Optional[timedelta]:\n",
    "        \"\"\"Get stage execution duration.\"\"\"\n",
    "        if self.started_at and self.completed_at:\n",
    "            return self.completed_at - self.started_at\n",
    "        return None\n",
    "    \n",
    "    def is_overdue(self) -> bool:\n",
    "        \"\"\"Check if stage is overdue based on timeout.\"\"\"\n",
    "        if not self.started_at or self.status in [PipelineStatus.COMPLETED, PipelineStatus.FAILED]:\n",
    "            return False\n",
    "        \n",
    "        elapsed = datetime.now(timezone.utc) - self.started_at\n",
    "        return elapsed.total_seconds() > (self.timeout_minutes * 60)\n",
    "    \n",
    "    class Config:\n",
    "        \"\"\"Pydantic model configuration.\"\"\"\n",
    "        use_enum_values = True\n",
    "        validate_assignment = True\n",
    "\n",
    "print(\"SUCCESS: PipelineStage model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline execution model\n",
    "class PipelineExecution(BaseModel):\n",
    "    \"\"\"Type-safe pipeline execution model.\"\"\"\n",
    "    execution_id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n",
    "    pipeline_id: str = Field(..., description=\"Pipeline identifier\")\n",
    "    pipeline_name: str = Field(..., description=\"Pipeline name\")\n",
    "    pipeline_version: str = Field(default=\"1.0\", description=\"Pipeline version\")\n",
    "    stages: List[PipelineStage] = Field(..., description=\"Pipeline stages\")\n",
    "    execution_strategy: ExecutionStrategy = Field(default=ExecutionStrategy.HYBRID)\n",
    "    trigger_type: TriggerType = Field(..., description=\"What triggered this execution\")\n",
    "    \n",
    "    # Execution state\n",
    "    status: PipelineStatus = Field(default=PipelineStatus.PENDING)\n",
    "    started_at: Optional[datetime] = Field(None, description=\"Execution start time\")\n",
    "    completed_at: Optional[datetime] = Field(None, description=\"Execution completion time\")\n",
    "    total_execution_time_minutes: Optional[float] = Field(None, ge=0, description=\"Total execution time\")\n",
    "    \n",
    "    # Progress tracking\n",
    "    completed_stages: Set[str] = Field(default_factory=set, description=\"Completed stage IDs\")\n",
    "    failed_stages: Set[str] = Field(default_factory=set, description=\"Failed stage IDs\")\n",
    "    skipped_stages: Set[str] = Field(default_factory=set, description=\"Skipped stage IDs\")\n",
    "    \n",
    "    # Metrics and monitoring\n",
    "    total_records_processed: int = Field(default=0, ge=0, description=\"Total records processed\")\n",
    "    total_errors: int = Field(default=0, ge=0, description=\"Total errors encountered\")\n",
    "    overall_quality_score: Optional[float] = Field(None, ge=0.0, le=1.0, description=\"Overall quality score\")\n",
    "    metadata: Dict[str, Any] = Field(default_factory=dict, description=\"Execution metadata\")\n",
    "    \n",
    "    @validator('stages')\n",
    "    def validate_stages(cls, v: List[PipelineStage]) -> List[PipelineStage]:\n",
    "        \"\"\"Validate pipeline stages.\"\"\"\n",
    "        if not v:\n",
    "            raise ValueError(\"Pipeline must have at least one stage\")\n",
    "        \n",
    "        # Check for duplicate stage IDs\n",
    "        stage_ids = [stage.stage_id for stage in v]\n",
    "        if len(stage_ids) != len(set(stage_ids)):\n",
    "            raise ValueError(\"Pipeline stages must have unique IDs\")\n",
    "        \n",
    "        return v\n",
    "    \n",
    "    def get_progress_percent(self) -> float:\n",
    "        \"\"\"Get execution progress as percentage.\"\"\"\n",
    "        if not self.stages:\n",
    "            return 100.0\n",
    "        return (len(self.completed_stages) / len(self.stages)) * 100\n",
    "    \n",
    "    def get_executable_stages(self) -> List[PipelineStage]:\n",
    "        \"\"\"Get stages that can be executed now.\"\"\"\n",
    "        executable = []\n",
    "        for stage in self.stages:\n",
    "            if (stage.status == PipelineStatus.PENDING and \n",
    "                stage.can_execute(self.completed_stages) and\n",
    "                stage.stage_id not in self.failed_stages):\n",
    "                executable.append(stage)\n",
    "        return executable\n",
    "    \n",
    "    def is_complete(self) -> bool:\n",
    "        \"\"\"Check if pipeline execution is complete.\"\"\"\n",
    "        return self.status in [PipelineStatus.COMPLETED, PipelineStatus.FAILED, PipelineStatus.CANCELLED]\n",
    "    \n",
    "    def calculate_overall_metrics(self) -> None:\n",
    "        \"\"\"Calculate overall pipeline metrics.\"\"\"\n",
    "        # Calculate total records and errors\n",
    "        self.total_records_processed = sum(\n",
    "            stage.quality_metrics.total_records \n",
    "            for stage in self.stages \n",
    "            if stage.quality_metrics\n",
    "        )\n",
    "        \n",
    "        # Calculate overall quality score\n",
    "        quality_scores = [\n",
    "            stage.quality_metrics.quality_score \n",
    "            for stage in self.stages \n",
    "            if stage.quality_metrics and stage.quality_metrics.quality_score > 0\n",
    "        ]\n",
    "        \n",
    "        if quality_scores:\n",
    "            self.overall_quality_score = statistics.mean(quality_scores)\n",
    "        \n",
    "        # Calculate total execution time\n",
    "        if self.started_at and self.completed_at:\n",
    "            self.total_execution_time_minutes = (\n",
    "                self.completed_at - self.started_at\n",
    "            ).total_seconds() / 60\n",
    "    \n",
    "    class Config:\n",
    "        \"\"\"Pydantic model configuration.\"\"\"\n",
    "        use_enum_values = True\n",
    "        validate_assignment = True\n",
    "\n",
    "print(\"SUCCESS: PipelineExecution model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline Orchestration Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstract base class for pipeline stages\n",
    "class PipelineStageExecutor(ABC):\n",
    "    \"\"\"Abstract base class for pipeline stage executors.\"\"\"\n",
    "    \n",
    "    def __init__(self, stage: PipelineStage):\n",
    "        self.stage = stage\n",
    "        self.logger = structlog.get_logger(f\"stage_{stage.stage_id}\")\n",
    "    \n",
    "    @abstractmethod\n",
    "    def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Execute the pipeline stage.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def validate_inputs(self, context: Dict[str, Any]) -> bool:\n",
    "        \"\"\"Validate stage inputs.\"\"\"\n",
    "        return True\n",
    "    \n",
    "    def cleanup(self, context: Dict[str, Any]) -> None:\n",
    "        \"\"\"Cleanup stage resources.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Concrete stage executors\n",
    "class DataExtractionStage(PipelineStageExecutor):\n",
    "    \"\"\"Data extraction stage executor.\"\"\"\n",
    "    \n",
    "    def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Extract data from configured sources.\"\"\"\n",
    "        \n",
    "        source_type = self.stage.configuration.get(\"source_type\", \"database\")\n",
    "        batch_size = self.stage.configuration.get(\"batch_size\", 1000)\n",
    "        \n",
    "        self.logger.info(\n",
    "            \"Starting data extraction\",\n",
    "            source_type=source_type,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        # Simulate data extraction\n",
    "        if ENVIRONMENT == \"test\":\n",
    "            # Generate synthetic data\n",
    "            extracted_data = [\n",
    "                {\n",
    "                    \"user_id\": f\"extracted_user_{i}\",\n",
    "                    \"email\": f\"user{i}@example.com\",\n",
    "                    \"signup_date\": (datetime.now(timezone.utc) - timedelta(days=i)).isoformat(),\n",
    "                    \"source\": \"data_extraction\"\n",
    "                }\n",
    "                for i in range(batch_size)\n",
    "            ]\n",
    "            \n",
    "            # Simulate quality metrics\n",
    "            quality_metrics = DataQualityMetrics(\n",
    "                total_records=len(extracted_data),\n",
    "                valid_records=int(len(extracted_data) * 0.95),\n",
    "                invalid_records=int(len(extracted_data) * 0.05),\n",
    "                schema_compliance_percent=98.5\n",
    "            )\n",
    "            quality_metrics.calculate_derived_metrics()\n",
    "            \n",
    "            self.stage.quality_metrics = quality_metrics\n",
    "            \n",
    "            return {\n",
    "                \"extracted_data\": extracted_data,\n",
    "                \"record_count\": len(extracted_data),\n",
    "                \"source_type\": source_type\n",
    "            }\n",
    "        \n",
    "        return {\"extracted_data\": [], \"record_count\": 0}\n",
    "\n",
    "class DataTransformationStage(PipelineStageExecutor):\n",
    "    \"\"\"Data transformation stage executor.\"\"\"\n",
    "    \n",
    "    def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Transform and enrich data.\"\"\"\n",
    "        \n",
    "        input_data = context.get(\"extracted_data\", [])\n",
    "        transformation_rules = self.stage.configuration.get(\"rules\", [])\n",
    "        \n",
    "        self.logger.info(\n",
    "            \"Starting data transformation\",\n",
    "            input_records=len(input_data),\n",
    "            transformation_rules=len(transformation_rules)\n",
    "        )\n",
    "        \n",
    "        # Apply transformations\n",
    "        transformed_data = []\n",
    "        \n",
    "        for record in input_data:\n",
    "            # Enrich with additional data\n",
    "            transformed_record = record.copy()\n",
    "            transformed_record.update({\n",
    "                \"transformed_at\": datetime.now(timezone.utc).isoformat(),\n",
    "                \"user_segment\": \"standard\" if \"user\" in record.get(\"user_id\", \"\") else \"premium\",\n",
    "                \"data_quality_score\": 0.95,\n",
    "                \"enriched\": True\n",
    "            })\n",
    "            \n",
    "            # Validate email format\n",
    "            email = record.get(\"email\", \"\")\n",
    "            if email and \"@\" in email:\n",
    "                transformed_record[\"email_valid\"] = True\n",
    "            else:\n",
    "                transformed_record[\"email_valid\"] = False\n",
    "            \n",
    "            transformed_data.append(transformed_record)\n",
    "        \n",
    "        # Calculate quality metrics\n",
    "        valid_emails = sum(1 for r in transformed_data if r.get(\"email_valid\", False))\n",
    "        \n",
    "        quality_metrics = DataQualityMetrics(\n",
    "            total_records=len(transformed_data),\n",
    "            valid_records=valid_emails,\n",
    "            invalid_records=len(transformed_data) - valid_emails,\n",
    "            schema_compliance_percent=100.0\n",
    "        )\n",
    "        quality_metrics.calculate_derived_metrics()\n",
    "        \n",
    "        self.stage.quality_metrics = quality_metrics\n",
    "        \n",
    "        return {\n",
    "            \"transformed_data\": transformed_data,\n",
    "            \"record_count\": len(transformed_data),\n",
    "            \"valid_records\": valid_emails\n",
    "        }\n",
    "\n",
    "class CustomerIOLoadStage(PipelineStageExecutor):\n",
    "    \"\"\"Customer.IO data loading stage executor.\"\"\"\n",
    "    \n",
    "    def __init__(self, stage: PipelineStage, client: CustomerIOClient):\n",
    "        super().__init__(stage)\n",
    "        self.client = client\n",
    "    \n",
    "    def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Load data into Customer.IO.\"\"\"\n",
    "        \n",
    "        input_data = context.get(\"transformed_data\", [])\n",
    "        batch_size = self.stage.configuration.get(\"batch_size\", 100)\n",
    "        \n",
    "        self.logger.info(\n",
    "            \"Starting Customer.IO data load\",\n",
    "            input_records=len(input_data),\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        # Process in batches\n",
    "        successful_records = 0\n",
    "        failed_records = 0\n",
    "        \n",
    "        for i in range(0, len(input_data), batch_size):\n",
    "            batch = input_data[i:i + batch_size]\n",
    "            \n",
    "            try:\n",
    "                # Convert to Customer.IO format\n",
    "                customerio_batch = []\n",
    "                \n",
    "                for record in batch:\n",
    "                    if record.get(\"email_valid\", False):\n",
    "                        # Create person record\n",
    "                        person_data = {\n",
    "                            \"type\": \"person\",\n",
    "                            \"action\": \"identify\",\n",
    "                            \"identifiers\": {\n",
    "                                \"id\": record[\"user_id\"],\n",
    "                                \"email\": record[\"email\"]\n",
    "                            },\n",
    "                            \"attributes\": {\n",
    "                                \"signup_date\": record.get(\"signup_date\"),\n",
    "                                \"user_segment\": record.get(\"user_segment\"),\n",
    "                                \"data_quality_score\": record.get(\"data_quality_score\"),\n",
    "                                \"last_updated\": record.get(\"transformed_at\")\n",
    "                            }\n",
    "                        }\n",
    "                        customerio_batch.append(person_data)\n",
    "                \n",
    "                # Send batch to Customer.IO\n",
    "                if ENVIRONMENT == \"test\":\n",
    "                    # Simulate successful processing\n",
    "                    successful_records += len(customerio_batch)\n",
    "                else:\n",
    "                    response = self.client.batch(customerio_batch)\n",
    "                    successful_records += len(customerio_batch)\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(\n",
    "                    \"Batch processing failed\",\n",
    "                    batch_size=len(batch),\n",
    "                    error=str(e)\n",
    "                )\n",
    "                failed_records += len(batch)\n",
    "        \n",
    "        # Calculate quality metrics\n",
    "        quality_metrics = DataQualityMetrics(\n",
    "            total_records=len(input_data),\n",
    "            valid_records=successful_records,\n",
    "            invalid_records=failed_records,\n",
    "            schema_compliance_percent=100.0\n",
    "        )\n",
    "        quality_metrics.calculate_derived_metrics()\n",
    "        \n",
    "        self.stage.quality_metrics = quality_metrics\n",
    "        \n",
    "        return {\n",
    "            \"loaded_records\": successful_records,\n",
    "            \"failed_records\": failed_records,\n",
    "            \"total_processed\": len(input_data)\n",
    "        }\n",
    "\n",
    "print(\"SUCCESS: Pipeline stage executors defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline orchestration engine\n",
    "class PipelineOrchestrator:\n",
    "    \"\"\"Advanced pipeline orchestration engine.\"\"\"\n",
    "    \n",
    "    def __init__(self, client: CustomerIOClient):\n",
    "        self.client = client\n",
    "        self.logger = structlog.get_logger(\"pipeline_orchestrator\")\n",
    "        self.stage_executors = {}\n",
    "        self.execution_context = {}\n",
    "        \n",
    "    def register_stage_executor(self, stage_type: PipelineStageType, executor_class: type) -> None:\n",
    "        \"\"\"Register a stage executor for a specific stage type.\"\"\"\n",
    "        self.stage_executors[stage_type] = executor_class\n",
    "    \n",
    "    def execute_pipeline(self, execution: PipelineExecution) -> PipelineExecution:\n",
    "        \"\"\"Execute a complete pipeline.\"\"\"\n",
    "        \n",
    "        execution.started_at = datetime.now(timezone.utc)\n",
    "        execution.status = PipelineStatus.RUNNING\n",
    "        \n",
    "        self.logger.info(\n",
    "            \"Starting pipeline execution\",\n",
    "            execution_id=execution.execution_id,\n",
    "            pipeline_id=execution.pipeline_id,\n",
    "            total_stages=len(execution.stages)\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            while not execution.is_complete():\n",
    "                # Get stages ready for execution\n",
    "                executable_stages = execution.get_executable_stages()\n",
    "                \n",
    "                if not executable_stages:\n",
    "                    # Check if we're stuck (no executable stages but not complete)\n",
    "                    pending_stages = [\n",
    "                        s for s in execution.stages \n",
    "                        if s.status == PipelineStatus.PENDING\n",
    "                    ]\n",
    "                    \n",
    "                    if pending_stages:\n",
    "                        # Pipeline is stuck due to failed dependencies\n",
    "                        execution.status = PipelineStatus.FAILED\n",
    "                        self.logger.error(\n",
    "                            \"Pipeline stuck - no executable stages\",\n",
    "                            pending_stages=[s.stage_id for s in pending_stages]\n",
    "                        )\n",
    "                        break\n",
    "                    else:\n",
    "                        # All stages are complete\n",
    "                        execution.status = PipelineStatus.COMPLETED\n",
    "                        break\n",
    "                \n",
    "                # Execute stages based on strategy\n",
    "                if execution.execution_strategy == ExecutionStrategy.PARALLEL:\n",
    "                    self._execute_stages_parallel(executable_stages, execution)\n",
    "                else:\n",
    "                    self._execute_stages_sequential(executable_stages, execution)\n",
    "            \n",
    "            # Finalize execution\n",
    "            execution.completed_at = datetime.now(timezone.utc)\n",
    "            execution.calculate_overall_metrics()\n",
    "            \n",
    "            # Determine final status if not already set\n",
    "            if execution.status == PipelineStatus.RUNNING:\n",
    "                if execution.failed_stages:\n",
    "                    execution.status = PipelineStatus.FAILED\n",
    "                else:\n",
    "                    execution.status = PipelineStatus.COMPLETED\n",
    "            \n",
    "            self.logger.info(\n",
    "                \"Pipeline execution completed\",\n",
    "                execution_id=execution.execution_id,\n",
    "                status=execution.status,\n",
    "                total_time_minutes=execution.total_execution_time_minutes,\n",
    "                records_processed=execution.total_records_processed\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            execution.status = PipelineStatus.FAILED\n",
    "            execution.completed_at = datetime.now(timezone.utc)\n",
    "            \n",
    "            self.logger.error(\n",
    "                \"Pipeline execution failed\",\n",
    "                execution_id=execution.execution_id,\n",
    "                error=str(e)\n",
    "            )\n",
    "        \n",
    "        return execution\n",
    "    \n",
    "    def _execute_stages_sequential(self, stages: List[PipelineStage], execution: PipelineExecution) -> None:\n",
    "        \"\"\"Execute stages sequentially.\"\"\"\n",
    "        for stage in stages:\n",
    "            self._execute_single_stage(stage, execution)\n",
    "    \n",
    "    def _execute_stages_parallel(self, stages: List[PipelineStage], execution: PipelineExecution) -> None:\n",
    "        \"\"\"Execute stages in parallel.\"\"\"\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=min(len(stages), 4)) as executor:\n",
    "            futures = {\n",
    "                executor.submit(self._execute_single_stage, stage, execution): stage \n",
    "                for stage in stages\n",
    "            }\n",
    "            \n",
    "            # Wait for all stages to complete\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                stage = futures[future]\n",
    "                try:\n",
    "                    future.result()\n",
    "                except Exception as e:\n",
    "                    self.logger.error(\n",
    "                        \"Stage execution failed in parallel execution\",\n",
    "                        stage_id=stage.stage_id,\n",
    "                        error=str(e)\n",
    "                    )\n",
    "    \n",
    "    def _execute_single_stage(self, stage: PipelineStage, execution: PipelineExecution) -> None:\n",
    "        \"\"\"Execute a single pipeline stage.\"\"\"\n",
    "        \n",
    "        stage.started_at = datetime.now(timezone.utc)\n",
    "        stage.status = PipelineStatus.RUNNING\n",
    "        \n",
    "        self.logger.info(\n",
    "            \"Starting stage execution\",\n",
    "            stage_id=stage.stage_id,\n",
    "            stage_type=stage.stage_type\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Get executor for stage type\n",
    "            executor_class = self.stage_executors.get(stage.stage_type)\n",
    "            if not executor_class:\n",
    "                raise ValueError(f\"No executor registered for stage type: {stage.stage_type}\")\n",
    "            \n",
    "            # Create executor instance\n",
    "            if stage.stage_type == PipelineStageType.LOAD:\n",
    "                executor = executor_class(stage, self.client)\n",
    "            else:\n",
    "                executor = executor_class(stage)\n",
    "            \n",
    "            # Validate inputs\n",
    "            if not executor.validate_inputs(self.execution_context):\n",
    "                raise ValueError(f\"Input validation failed for stage: {stage.stage_id}\")\n",
    "            \n",
    "            # Execute stage\n",
    "            result = executor.execute(self.execution_context)\n",
    "            \n",
    "            # Update execution context with results\n",
    "            self.execution_context.update(result)\n",
    "            \n",
    "            # Mark stage as completed\n",
    "            stage.completed_at = datetime.now(timezone.utc)\n",
    "            stage.status = PipelineStatus.COMPLETED\n",
    "            stage.execution_time_minutes = (\n",
    "                stage.completed_at - stage.started_at\n",
    "            ).total_seconds() / 60\n",
    "            \n",
    "            execution.completed_stages.add(stage.stage_id)\n",
    "            \n",
    "            # Cleanup\n",
    "            executor.cleanup(self.execution_context)\n",
    "            \n",
    "            self.logger.info(\n",
    "                \"Stage execution completed\",\n",
    "                stage_id=stage.stage_id,\n",
    "                execution_time_minutes=stage.execution_time_minutes,\n",
    "                quality_score=stage.quality_metrics.quality_score if stage.quality_metrics else None\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            stage.status = PipelineStatus.FAILED\n",
    "            stage.completed_at = datetime.now(timezone.utc)\n",
    "            stage.error_message = str(e)\n",
    "            \n",
    "            execution.failed_stages.add(stage.stage_id)\n",
    "            \n",
    "            self.logger.error(\n",
    "                \"Stage execution failed\",\n",
    "                stage_id=stage.stage_id,\n",
    "                error=str(e)\n",
    "            )\n",
    "            \n",
    "            # Check if stage should retry\n",
    "            if stage.retry_count < stage.retry_attempts:\n",
    "                stage.retry_count += 1\n",
    "                stage.status = PipelineStatus.PENDING\n",
    "                execution.failed_stages.discard(stage.stage_id)\n",
    "                \n",
    "                self.logger.info(\n",
    "                    \"Retrying stage execution\",\n",
    "                    stage_id=stage.stage_id,\n",
    "                    retry_count=stage.retry_count\n",
    "                )\n",
    "\n",
    "print(\"SUCCESS: PipelineOrchestrator class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Definition and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive data pipeline\n",
    "def create_customer_data_pipeline() -> PipelineExecution:\n",
    "    \"\"\"Create a comprehensive customer data ingestion pipeline.\"\"\"\n",
    "    \n",
    "    # Define pipeline stages\n",
    "    stages = [\n",
    "        PipelineStage(\n",
    "            stage_id=\"extract_customer_data\",\n",
    "            stage_name=\"Extract Customer Data\",\n",
    "            stage_type=PipelineStageType.EXTRACT,\n",
    "            dependencies=[],\n",
    "            timeout_minutes=15,\n",
    "            configuration={\n",
    "                \"source_type\": \"database\",\n",
    "                \"batch_size\": 1000,\n",
    "                \"table_name\": \"customers\"\n",
    "            },\n",
    "            expected_runtime_minutes=5.0\n",
    "        ),\n",
    "        PipelineStage(\n",
    "            stage_id=\"transform_customer_data\",\n",
    "            stage_name=\"Transform Customer Data\",\n",
    "            stage_type=PipelineStageType.TRANSFORM,\n",
    "            dependencies=[\"extract_customer_data\"],\n",
    "            timeout_minutes=20,\n",
    "            configuration={\n",
    "                \"rules\": [\n",
    "                    \"validate_email_format\",\n",
    "                    \"enrich_user_segment\",\n",
    "                    \"standardize_dates\"\n",
    "                ],\n",
    "                \"quality_threshold\": 0.9\n",
    "            },\n",
    "            expected_runtime_minutes=8.0\n",
    "        ),\n",
    "        PipelineStage(\n",
    "            stage_id=\"validate_data_quality\",\n",
    "            stage_name=\"Validate Data Quality\",\n",
    "            stage_type=PipelineStageType.VALIDATE,\n",
    "            dependencies=[\"transform_customer_data\"],\n",
    "            timeout_minutes=10,\n",
    "            configuration={\n",
    "                \"quality_rules\": [\n",
    "                    \"email_format_check\",\n",
    "                    \"required_fields_check\",\n",
    "                    \"duplicate_check\"\n",
    "                ],\n",
    "                \"min_quality_score\": 0.85\n",
    "            },\n",
    "            expected_runtime_minutes=3.0\n",
    "        ),\n",
    "        PipelineStage(\n",
    "            stage_id=\"load_to_customerio\",\n",
    "            stage_name=\"Load to Customer.IO\",\n",
    "            stage_type=PipelineStageType.LOAD,\n",
    "            dependencies=[\"validate_data_quality\"],\n",
    "            timeout_minutes=30,\n",
    "            configuration={\n",
    "                \"batch_size\": 100,\n",
    "                \"rate_limit_rps\": 50,\n",
    "                \"enable_upsert\": True\n",
    "            },\n",
    "            expected_runtime_minutes=12.0\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Create pipeline execution\n",
    "    pipeline_execution = PipelineExecution(\n",
    "        pipeline_id=\"customer_data_pipeline_v1\",\n",
    "        pipeline_name=\"Customer Data Ingestion Pipeline\",\n",
    "        pipeline_version=\"1.0\",\n",
    "        stages=stages,\n",
    "        execution_strategy=ExecutionStrategy.HYBRID,\n",
    "        trigger_type=TriggerType.MANUAL,\n",
    "        metadata={\n",
    "            \"environment\": ENVIRONMENT,\n",
    "            \"created_by\": \"pipeline_integration_notebook\",\n",
    "            \"purpose\": \"customer_data_ingestion\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return pipeline_execution\n",
    "\n",
    "# Create the pipeline\n",
    "customer_pipeline = create_customer_data_pipeline()\n",
    "\n",
    "print(f\"Created customer data pipeline:\")\n",
    "print(f\"  Pipeline ID: {customer_pipeline.pipeline_id}\")\n",
    "print(f\"  Total Stages: {len(customer_pipeline.stages)}\")\n",
    "print(f\"  Execution Strategy: {customer_pipeline.execution_strategy}\")\n",
    "print(f\"  Expected Runtime: {sum(s.expected_runtime_minutes or 0 for s in customer_pipeline.stages)} minutes\")\n",
    "\n",
    "print(f\"\\nPipeline Stages:\")\n",
    "for stage in customer_pipeline.stages:\n",
    "    deps = \", \".join(stage.dependencies) if stage.dependencies else \"None\"\n",
    "    print(f\"  {stage.stage_id}: {stage.stage_type} (deps: {deps})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up and execute the pipeline\n",
    "orchestrator = PipelineOrchestrator(client)\n",
    "\n",
    "# Register stage executors\n",
    "orchestrator.register_stage_executor(PipelineStageType.EXTRACT, DataExtractionStage)\n",
    "orchestrator.register_stage_executor(PipelineStageType.TRANSFORM, DataTransformationStage)\n",
    "orchestrator.register_stage_executor(PipelineStageType.VALIDATE, DataTransformationStage)  # Reuse transform logic\n",
    "orchestrator.register_stage_executor(PipelineStageType.LOAD, CustomerIOLoadStage)\n",
    "\n",
    "print(\"=== Executing Customer Data Pipeline ===\")\n",
    "\n",
    "# Execute the pipeline\n",
    "start_time = time.time()\n",
    "completed_pipeline = orchestrator.execute_pipeline(customer_pipeline)\n",
    "end_time = time.time()\n",
    "\n",
    "actual_execution_time = end_time - start_time\n",
    "\n",
    "print(f\"\\n=== Pipeline Execution Results ===\")\n",
    "print(f\"Execution ID: {completed_pipeline.execution_id}\")\n",
    "print(f\"Final Status: {completed_pipeline.status}\")\n",
    "print(f\"Progress: {completed_pipeline.get_progress_percent():.1f}%\")\n",
    "print(f\"Total Execution Time: {completed_pipeline.total_execution_time_minutes:.2f} minutes\")\n",
    "print(f\"Records Processed: {completed_pipeline.total_records_processed:,}\")\n",
    "print(f\"Overall Quality Score: {completed_pipeline.overall_quality_score:.3f}\" if completed_pipeline.overall_quality_score else \"N/A\")\n",
    "\n",
    "print(f\"\\n=== Stage Results ===\")\n",
    "for stage in completed_pipeline.stages:\n",
    "    status_icon = \"\" if stage.status == PipelineStatus.COMPLETED else \"\" if stage.status == PipelineStatus.FAILED else \"\"\n",
    "    runtime = f\"{stage.execution_time_minutes:.2f}m\" if stage.execution_time_minutes else \"N/A\"\n",
    "    quality = f\"{stage.quality_metrics.quality_score:.3f}\" if stage.quality_metrics else \"N/A\"\n",
    "    \n",
    "    print(f\"  {status_icon} {stage.stage_name}: {stage.status} ({runtime}, quality: {quality})\")\n",
    "    \n",
    "    if stage.error_message:\n",
    "        print(f\"    Error: {stage.error_message}\")\n",
    "    \n",
    "    if stage.quality_metrics:\n",
    "        qm = stage.quality_metrics\n",
    "        print(f\"    Records: {qm.total_records} total, {qm.valid_records} valid, {qm.invalid_records} invalid\")\n",
    "\n",
    "print(f\"\\n=== Execution Summary ===\")\n",
    "print(f\"Completed Stages: {len(completed_pipeline.completed_stages)}\")\n",
    "print(f\"Failed Stages: {len(completed_pipeline.failed_stages)}\")\n",
    "print(f\"Skipped Stages: {len(completed_pipeline.skipped_stages)}\")\n",
    "\n",
    "if completed_pipeline.failed_stages:\n",
    "    print(f\"Failed Stage IDs: {', '.join(completed_pipeline.failed_stages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Pipeline Monitoring and Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation: Pipeline monitoring and analytics\n",
    "class PipelineMonitor:\n",
    "    \"\"\"Advanced pipeline monitoring and analytics.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = structlog.get_logger(\"pipeline_monitor\")\n",
    "        self.execution_history = deque(maxlen=1000)\n",
    "        self.performance_metrics = defaultdict(list)\n",
    "        \n",
    "    def record_execution(self, execution: PipelineExecution) -> None:\n",
    "        \"\"\"Record pipeline execution for monitoring.\"\"\"\n",
    "        \n",
    "        execution_record = {\n",
    "            \"execution_id\": execution.execution_id,\n",
    "            \"pipeline_id\": execution.pipeline_id,\n",
    "            \"status\": execution.status,\n",
    "            \"started_at\": execution.started_at,\n",
    "            \"completed_at\": execution.completed_at,\n",
    "            \"total_execution_time_minutes\": execution.total_execution_time_minutes,\n",
    "            \"records_processed\": execution.total_records_processed,\n",
    "            \"overall_quality_score\": execution.overall_quality_score,\n",
    "            \"stage_count\": len(execution.stages),\n",
    "            \"completed_stages\": len(execution.completed_stages),\n",
    "            \"failed_stages\": len(execution.failed_stages)\n",
    "        }\n",
    "        \n",
    "        self.execution_history.append(execution_record)\n",
    "        \n",
    "        # Record performance metrics\n",
    "        pipeline_id = execution.pipeline_id\n",
    "        if execution.total_execution_time_minutes:\n",
    "            self.performance_metrics[f\"{pipeline_id}_execution_time\"].append(\n",
    "                execution.total_execution_time_minutes\n",
    "            )\n",
    "        \n",
    "        if execution.overall_quality_score:\n",
    "            self.performance_metrics[f\"{pipeline_id}_quality_score\"].append(\n",
    "                execution.overall_quality_score\n",
    "            )\n",
    "        \n",
    "        if execution.total_records_processed:\n",
    "            self.performance_metrics[f\"{pipeline_id}_throughput\"].append(\n",
    "                execution.total_records_processed / (execution.total_execution_time_minutes or 1)\n",
    "            )\n",
    "    \n",
    "    def get_pipeline_analytics(self, pipeline_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive analytics for a pipeline.\"\"\"\n",
    "        \n",
    "        # Filter executions for this pipeline\n",
    "        pipeline_executions = [\n",
    "            exec_record for exec_record in self.execution_history\n",
    "            if exec_record[\"pipeline_id\"] == pipeline_id\n",
    "        ]\n",
    "        \n",
    "        if not pipeline_executions:\n",
    "            return {\"error\": \"No execution data found for pipeline\"}\n",
    "        \n",
    "        # Calculate success/failure rates\n",
    "        total_executions = len(pipeline_executions)\n",
    "        successful_executions = len([\n",
    "            e for e in pipeline_executions \n",
    "            if e[\"status\"] == PipelineStatus.COMPLETED\n",
    "        ])\n",
    "        failed_executions = len([\n",
    "            e for e in pipeline_executions \n",
    "            if e[\"status\"] == PipelineStatus.FAILED\n",
    "        ])\n",
    "        \n",
    "        success_rate = (successful_executions / total_executions * 100) if total_executions > 0 else 0\n",
    "        \n",
    "        # Calculate performance statistics\n",
    "        execution_times = [\n",
    "            e[\"total_execution_time_minutes\"] for e in pipeline_executions\n",
    "            if e[\"total_execution_time_minutes\"]\n",
    "        ]\n",
    "        \n",
    "        quality_scores = [\n",
    "            e[\"overall_quality_score\"] for e in pipeline_executions\n",
    "            if e[\"overall_quality_score\"]\n",
    "        ]\n",
    "        \n",
    "        records_processed = [\n",
    "            e[\"records_processed\"] for e in pipeline_executions\n",
    "            if e[\"records_processed\"]\n",
    "        ]\n",
    "        \n",
    "        # Recent performance (last 10 executions)\n",
    "        recent_executions = sorted(\n",
    "            pipeline_executions, \n",
    "            key=lambda x: x[\"started_at\"] or datetime.min.replace(tzinfo=timezone.utc),\n",
    "            reverse=True\n",
    "        )[:10]\n",
    "        \n",
    "        analytics = {\n",
    "            \"pipeline_id\": pipeline_id,\n",
    "            \"total_executions\": total_executions,\n",
    "            \"successful_executions\": successful_executions,\n",
    "            \"failed_executions\": failed_executions,\n",
    "            \"success_rate_percent\": round(success_rate, 2),\n",
    "            \"performance_stats\": {\n",
    "                \"avg_execution_time_minutes\": round(statistics.mean(execution_times), 2) if execution_times else 0,\n",
    "                \"min_execution_time_minutes\": round(min(execution_times), 2) if execution_times else 0,\n",
    "                \"max_execution_time_minutes\": round(max(execution_times), 2) if execution_times else 0,\n",
    "                \"avg_quality_score\": round(statistics.mean(quality_scores), 3) if quality_scores else 0,\n",
    "                \"avg_records_per_execution\": round(statistics.mean(records_processed), 0) if records_processed else 0,\n",
    "                \"total_records_processed\": sum(records_processed) if records_processed else 0\n",
    "            },\n",
    "            \"recent_trend\": {\n",
    "                \"last_10_executions\": len(recent_executions),\n",
    "                \"recent_success_rate\": (\n",
    "                    len([e for e in recent_executions if e[\"status\"] == PipelineStatus.COMPLETED]) /\n",
    "                    len(recent_executions) * 100\n",
    "                ) if recent_executions else 0,\n",
    "                \"recent_avg_time\": round(\n",
    "                    statistics.mean([\n",
    "                        e[\"total_execution_time_minutes\"] for e in recent_executions\n",
    "                        if e[\"total_execution_time_minutes\"]\n",
    "                    ]), 2\n",
    "                ) if recent_executions else 0\n",
    "            },\n",
    "            \"last_execution\": recent_executions[0] if recent_executions else None,\n",
    "            \"analyzed_at\": datetime.now(timezone.utc).isoformat()\n",
    "        }\n",
    "        \n",
    "        return analytics\n",
    "    \n",
    "    def detect_anomalies(self, pipeline_id: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Detect performance anomalies in pipeline executions.\"\"\"\n",
    "        \n",
    "        anomalies = []\n",
    "        \n",
    "        # Get performance metrics\n",
    "        execution_times = self.performance_metrics.get(f\"{pipeline_id}_execution_time\", [])\n",
    "        quality_scores = self.performance_metrics.get(f\"{pipeline_id}_quality_score\", [])\n",
    "        throughputs = self.performance_metrics.get(f\"{pipeline_id}_throughput\", [])\n",
    "        \n",
    "        # Detect execution time anomalies\n",
    "        if len(execution_times) >= 5:\n",
    "            avg_time = statistics.mean(execution_times)\n",
    "            std_time = statistics.stdev(execution_times)\n",
    "            recent_time = execution_times[-1]\n",
    "            \n",
    "            if recent_time > avg_time + (2 * std_time):  # 2 standard deviations\n",
    "                anomalies.append({\n",
    "                    \"type\": \"slow_execution\",\n",
    "                    \"severity\": \"warning\",\n",
    "                    \"description\": f\"Execution time {recent_time:.2f}m is significantly above average {avg_time:.2f}m\",\n",
    "                    \"metric\": \"execution_time\",\n",
    "                    \"value\": recent_time,\n",
    "                    \"threshold\": avg_time + (2 * std_time)\n",
    "                })\n",
    "        \n",
    "        # Detect quality score anomalies\n",
    "        if len(quality_scores) >= 5:\n",
    "            avg_quality = statistics.mean(quality_scores)\n",
    "            recent_quality = quality_scores[-1]\n",
    "            \n",
    "            if recent_quality < avg_quality - 0.1:  # Quality drop > 10%\n",
    "                anomalies.append({\n",
    "                    \"type\": \"quality_degradation\",\n",
    "                    \"severity\": \"critical\",\n",
    "                    \"description\": f\"Quality score {recent_quality:.3f} is significantly below average {avg_quality:.3f}\",\n",
    "                    \"metric\": \"quality_score\",\n",
    "                    \"value\": recent_quality,\n",
    "                    \"threshold\": avg_quality - 0.1\n",
    "                })\n",
    "        \n",
    "        # Detect throughput anomalies\n",
    "        if len(throughputs) >= 5:\n",
    "            avg_throughput = statistics.mean(throughputs)\n",
    "            recent_throughput = throughputs[-1]\n",
    "            \n",
    "            if recent_throughput < avg_throughput * 0.7:  # 30% drop in throughput\n",
    "                anomalies.append({\n",
    "                    \"type\": \"low_throughput\",\n",
    "                    \"severity\": \"warning\",\n",
    "                    \"description\": f\"Throughput {recent_throughput:.1f} records/min is significantly below average {avg_throughput:.1f}\",\n",
    "                    \"metric\": \"throughput\",\n",
    "                    \"value\": recent_throughput,\n",
    "                    \"threshold\": avg_throughput * 0.7\n",
    "                })\n",
    "        \n",
    "        return anomalies\n",
    "    \n",
    "    def get_system_health(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get overall system health metrics.\"\"\"\n",
    "        \n",
    "        if not self.execution_history:\n",
    "            return {\"status\": \"no_data\"}\n",
    "        \n",
    "        # Recent executions (last hour)\n",
    "        cutoff_time = datetime.now(timezone.utc) - timedelta(hours=1)\n",
    "        recent_executions = [\n",
    "            e for e in self.execution_history\n",
    "            if e[\"started_at\"] and e[\"started_at\"] > cutoff_time\n",
    "        ]\n",
    "        \n",
    "        # Calculate health metrics\n",
    "        total_recent = len(recent_executions)\n",
    "        successful_recent = len([\n",
    "            e for e in recent_executions \n",
    "            if e[\"status\"] == PipelineStatus.COMPLETED\n",
    "        ])\n",
    "        \n",
    "        health_score = (successful_recent / total_recent * 100) if total_recent > 0 else 100\n",
    "        \n",
    "        # Determine overall status\n",
    "        if health_score >= 95:\n",
    "            status = \"healthy\"\n",
    "        elif health_score >= 80:\n",
    "            status = \"degraded\"\n",
    "        else:\n",
    "            status = \"unhealthy\"\n",
    "        \n",
    "        return {\n",
    "            \"status\": status,\n",
    "            \"health_score_percent\": round(health_score, 2),\n",
    "            \"recent_executions_1h\": total_recent,\n",
    "            \"successful_executions_1h\": successful_recent,\n",
    "            \"failed_executions_1h\": total_recent - successful_recent,\n",
    "            \"total_pipelines_monitored\": len(set(e[\"pipeline_id\"] for e in self.execution_history)),\n",
    "            \"last_updated\": datetime.now(timezone.utc).isoformat()\n",
    "        }\n",
    "\n",
    "print(\"SUCCESS: PipelineMonitor class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pipeline monitoring and analytics\n",
    "monitor = PipelineMonitor()\n",
    "\n",
    "# Record the completed pipeline execution\n",
    "monitor.record_execution(completed_pipeline)\n",
    "\n",
    "# Simulate additional executions for analytics\n",
    "print(\"=== Simulating Additional Pipeline Executions ===\")\n",
    "\n",
    "for i in range(5):\n",
    "    # Create a copy of the pipeline for simulation\n",
    "    sim_pipeline = create_customer_data_pipeline()\n",
    "    sim_pipeline.execution_id = str(uuid.uuid4())\n",
    "    \n",
    "    # Simulate execution metrics\n",
    "    sim_pipeline.started_at = datetime.now(timezone.utc) - timedelta(hours=i+1)\n",
    "    sim_pipeline.completed_at = sim_pipeline.started_at + timedelta(minutes=15 + i*2)\n",
    "    sim_pipeline.status = PipelineStatus.COMPLETED if i < 4 else PipelineStatus.FAILED\n",
    "    sim_pipeline.total_records_processed = 1000 - i*50\n",
    "    sim_pipeline.overall_quality_score = 0.95 - i*0.02\n",
    "    sim_pipeline.total_execution_time_minutes = 15 + i*2\n",
    "    sim_pipeline.completed_stages = set([s.stage_id for s in sim_pipeline.stages[:3+i]])\n",
    "    \n",
    "    if sim_pipeline.status == PipelineStatus.FAILED:\n",
    "        sim_pipeline.failed_stages = {sim_pipeline.stages[-1].stage_id}\n",
    "    \n",
    "    monitor.record_execution(sim_pipeline)\n",
    "\n",
    "# Get pipeline analytics\n",
    "analytics = monitor.get_pipeline_analytics(\"customer_data_pipeline_v1\")\n",
    "\n",
    "print(f\"\\n=== Pipeline Analytics ===\")\n",
    "print(f\"Pipeline: {analytics['pipeline_id']}\")\n",
    "print(f\"Total Executions: {analytics['total_executions']}\")\n",
    "print(f\"Success Rate: {analytics['success_rate_percent']}%\")\n",
    "print(f\"Successful: {analytics['successful_executions']}\")\n",
    "print(f\"Failed: {analytics['failed_executions']}\")\n",
    "\n",
    "print(f\"\\n=== Performance Statistics ===\")\n",
    "perf_stats = analytics['performance_stats']\n",
    "print(f\"Average Execution Time: {perf_stats['avg_execution_time_minutes']} minutes\")\n",
    "print(f\"Execution Time Range: {perf_stats['min_execution_time_minutes']} - {perf_stats['max_execution_time_minutes']} minutes\")\n",
    "print(f\"Average Quality Score: {perf_stats['avg_quality_score']}\")\n",
    "print(f\"Average Records per Execution: {perf_stats['avg_records_per_execution']:,.0f}\")\n",
    "print(f\"Total Records Processed: {perf_stats['total_records_processed']:,.0f}\")\n",
    "\n",
    "print(f\"\\n=== Recent Trend (Last 10 Executions) ===\")\n",
    "trend = analytics['recent_trend']\n",
    "print(f\"Recent Executions: {trend['last_10_executions']}\")\n",
    "print(f\"Recent Success Rate: {trend['recent_success_rate']:.1f}%\")\n",
    "print(f\"Recent Average Time: {trend['recent_avg_time']} minutes\")\n",
    "\n",
    "# Detect anomalies\n",
    "anomalies = monitor.detect_anomalies(\"customer_data_pipeline_v1\")\n",
    "\n",
    "print(f\"\\n=== Anomaly Detection ===\")\n",
    "if anomalies:\n",
    "    for anomaly in anomalies:\n",
    "        print(f\"[{anomaly['severity'].upper()}] {anomaly['type']}: {anomaly['description']}\")\n",
    "else:\n",
    "    print(\"No anomalies detected\")\n",
    "\n",
    "# Get system health\n",
    "health = monitor.get_system_health()\n",
    "\n",
    "print(f\"\\n=== System Health ===\")\n",
    "print(f\"Overall Status: {health['status'].upper()}\")\n",
    "print(f\"Health Score: {health['health_score_percent']}%\")\n",
    "print(f\"Recent Executions (1h): {health['recent_executions_1h']}\")\n",
    "print(f\"Successful (1h): {health['successful_executions_1h']}\")\n",
    "print(f\"Failed (1h): {health['failed_executions_1h']}\")\n",
    "print(f\"Total Pipelines Monitored: {health['total_pipelines_monitored']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Data from Spark Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline execution data from Delta table\n",
    "print(\"=== Pipeline Data Integration ===\")\n",
    "\n",
    "# Create pipeline executions table if it doesn't exist\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {CATALOG_NAME}.{DATABASE_NAME}.pipeline_executions (\n",
    "    execution_id STRING,\n",
    "    pipeline_id STRING,\n",
    "    pipeline_name STRING,\n",
    "    status STRING,\n",
    "    trigger_type STRING,\n",
    "    started_at TIMESTAMP,\n",
    "    completed_at TIMESTAMP,\n",
    "    total_execution_time_minutes DOUBLE,\n",
    "    records_processed INT,\n",
    "    overall_quality_score DOUBLE,\n",
    "    completed_stages INT,\n",
    "    failed_stages INT,\n",
    "    metadata MAP<STRING, STRING>\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Insert sample pipeline execution data\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {CATALOG_NAME}.{DATABASE_NAME}.pipeline_executions\n",
    "SELECT * FROM VALUES\n",
    "    ('exec_001', 'customer_data_pipeline_v1', 'Customer Data Ingestion Pipeline', 'completed', 'schedule', current_timestamp() - INTERVAL 4 HOURS, current_timestamp() - INTERVAL 4 HOURS + INTERVAL 18 MINUTES, 18.5, 1000, 0.95, 4, 0, map('environment', 'test')),\n",
    "    ('exec_002', 'customer_data_pipeline_v1', 'Customer Data Ingestion Pipeline', 'completed', 'schedule', current_timestamp() - INTERVAL 8 HOURS, current_timestamp() - INTERVAL 8 HOURS + INTERVAL 16 MINUTES, 16.2, 950, 0.93, 4, 0, map('environment', 'test')),\n",
    "    ('exec_003', 'customer_data_pipeline_v1', 'Customer Data Ingestion Pipeline', 'failed', 'schedule', current_timestamp() - INTERVAL 12 HOURS, current_timestamp() - INTERVAL 12 HOURS + INTERVAL 25 MINUTES, 25.1, 750, 0.87, 3, 1, map('environment', 'test')),\n",
    "    ('exec_004', 'event_processing_pipeline_v1', 'Event Processing Pipeline', 'completed', 'event', current_timestamp() - INTERVAL 2 HOURS, current_timestamp() - INTERVAL 2 HOURS + INTERVAL 12 MINUTES, 12.3, 2500, 0.98, 3, 0, map('environment', 'test')),\n",
    "    ('exec_005', 'data_quality_pipeline_v1', 'Data Quality Validation Pipeline', 'completed', 'manual', current_timestamp() - INTERVAL 6 HOURS, current_timestamp() - INTERVAL 6 HOURS + INTERVAL 8 MINUTES, 8.7, 500, 0.99, 2, 0, map('environment', 'test'))\n",
    "WHERE NOT EXISTS (\n",
    "    SELECT 1 FROM {CATALOG_NAME}.{DATABASE_NAME}.pipeline_executions \n",
    "    WHERE execution_id = 'exec_001'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Load pipeline executions\n",
    "pipeline_executions_df = spark.table(f\"{CATALOG_NAME}.{DATABASE_NAME}.pipeline_executions\")\n",
    "print(\"Sample pipeline executions from Spark:\")\n",
    "pipeline_executions_df.show(truncate=False)\n",
    "\n",
    "# Analyze pipeline performance trends\n",
    "print(\"\\n=== Pipeline Performance Analysis ===\")\n",
    "\n",
    "# Performance by pipeline\n",
    "pipeline_performance = pipeline_executions_df.groupBy(\"pipeline_id\", \"status\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"execution_count\"),\n",
    "        F.avg(\"total_execution_time_minutes\").alias(\"avg_execution_time\"),\n",
    "        F.avg(\"overall_quality_score\").alias(\"avg_quality_score\"),\n",
    "        F.sum(\"records_processed\").alias(\"total_records\")\n",
    "    ) \\\n",
    "    .orderBy(\"pipeline_id\", \"status\")\n",
    "\n",
    "print(\"Performance by pipeline and status:\")\n",
    "pipeline_performance.show()\n",
    "\n",
    "# Recent pipeline trends\n",
    "recent_executions = pipeline_executions_df.filter(\n",
    "    F.col(\"started_at\") >= F.date_sub(F.current_timestamp(), 1)\n",
    ").select(\n",
    "    \"execution_id\", \"pipeline_id\", \"status\", \"trigger_type\",\n",
    "    \"total_execution_time_minutes\", \"records_processed\", \"overall_quality_score\"\n",
    ")\n",
    "\n",
    "print(\"\\nRecent pipeline executions (last 24 hours):\")\n",
    "recent_executions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive pipeline analytics from Spark data\n",
    "def analyze_pipeline_trends_with_spark():\n",
    "    \"\"\"Analyze pipeline trends using Spark for large-scale analytics.\"\"\"\n",
    "    \n",
    "    print(\"=== Advanced Pipeline Analytics with Spark ===\")\n",
    "    \n",
    "    # Success rate by pipeline\n",
    "    success_rates = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            pipeline_id,\n",
    "            pipeline_name,\n",
    "            COUNT(*) as total_executions,\n",
    "            SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as successful_executions,\n",
    "            ROUND(SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as success_rate_percent\n",
    "        FROM {CATALOG_NAME}.{DATABASE_NAME}.pipeline_executions\n",
    "        GROUP BY pipeline_id, pipeline_name\n",
    "        ORDER BY success_rate_percent DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\nPipeline Success Rates:\")\n",
    "    success_rates.show()\n",
    "    \n",
    "    # Performance trends by hour\n",
    "    hourly_trends = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            DATE_TRUNC('hour', started_at) as execution_hour,\n",
    "            COUNT(*) as executions_count,\n",
    "            AVG(total_execution_time_minutes) as avg_execution_time,\n",
    "            AVG(overall_quality_score) as avg_quality_score,\n",
    "            SUM(records_processed) as total_records_processed\n",
    "        FROM {CATALOG_NAME}.{DATABASE_NAME}.pipeline_executions\n",
    "        WHERE started_at >= current_timestamp() - INTERVAL 24 HOURS\n",
    "        GROUP BY DATE_TRUNC('hour', started_at)\n",
    "        ORDER BY execution_hour\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\nHourly Performance Trends (Last 24 Hours):\")\n",
    "    hourly_trends.show()\n",
    "    \n",
    "    # Quality score distribution\n",
    "    quality_distribution = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            CASE \n",
    "                WHEN overall_quality_score >= 0.95 THEN 'Excellent (0.95+)'\n",
    "                WHEN overall_quality_score >= 0.90 THEN 'Good (0.90-0.95)'\n",
    "                WHEN overall_quality_score >= 0.80 THEN 'Fair (0.80-0.90)'\n",
    "                ELSE 'Poor (<0.80)'\n",
    "            END as quality_category,\n",
    "            COUNT(*) as execution_count,\n",
    "            ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentage\n",
    "        FROM {CATALOG_NAME}.{DATABASE_NAME}.pipeline_executions\n",
    "        WHERE overall_quality_score IS NOT NULL\n",
    "        GROUP BY \n",
    "            CASE \n",
    "                WHEN overall_quality_score >= 0.95 THEN 'Excellent (0.95+)'\n",
    "                WHEN overall_quality_score >= 0.90 THEN 'Good (0.90-0.95)'\n",
    "                WHEN overall_quality_score >= 0.80 THEN 'Fair (0.80-0.90)'\n",
    "                ELSE 'Poor (<0.80)'\n",
    "            END\n",
    "        ORDER BY \n",
    "            CASE \n",
    "                WHEN quality_category = 'Excellent (0.95+)' THEN 1\n",
    "                WHEN quality_category = 'Good (0.90-0.95)' THEN 2\n",
    "                WHEN quality_category = 'Fair (0.80-0.90)' THEN 3\n",
    "                ELSE 4\n",
    "            END\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\nData Quality Score Distribution:\")\n",
    "    quality_distribution.show()\n",
    "    \n",
    "    # Pipeline efficiency metrics\n",
    "    efficiency_metrics = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            pipeline_id,\n",
    "            AVG(records_processed / total_execution_time_minutes) as avg_throughput_per_minute,\n",
    "            MIN(total_execution_time_minutes) as fastest_execution,\n",
    "            MAX(total_execution_time_minutes) as slowest_execution,\n",
    "            STDDEV(total_execution_time_minutes) as execution_time_variance\n",
    "        FROM {CATALOG_NAME}.{DATABASE_NAME}.pipeline_executions\n",
    "        WHERE status = 'completed' AND total_execution_time_minutes > 0\n",
    "        GROUP BY pipeline_id\n",
    "        ORDER BY avg_throughput_per_minute DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\nPipeline Efficiency Metrics:\")\n",
    "    efficiency_metrics.show()\n",
    "    \n",
    "    return {\n",
    "        \"success_rates\": success_rates.collect(),\n",
    "        \"hourly_trends\": hourly_trends.collect(),\n",
    "        \"quality_distribution\": quality_distribution.collect(),\n",
    "        \"efficiency_metrics\": efficiency_metrics.collect()\n",
    "    }\n",
    "\n",
    "# Run comprehensive analytics\n",
    "spark_analytics = analyze_pipeline_trends_with_spark()\n",
    "\n",
    "print(f\"\\n=== Key Insights from Spark Analytics ===\")\n",
    "\n",
    "# Extract key insights\n",
    "if spark_analytics[\"success_rates\"]:\n",
    "    best_pipeline = spark_analytics[\"success_rates\"][0]\n",
    "    print(f\"Best Performing Pipeline: {best_pipeline['pipeline_name']} ({best_pipeline['success_rate_percent']}% success rate)\")\n",
    "\n",
    "if spark_analytics[\"quality_distribution\"]:\n",
    "    excellent_quality = next((q for q in spark_analytics[\"quality_distribution\"] if \"Excellent\" in q['quality_category']), None)\n",
    "    if excellent_quality:\n",
    "        print(f\"Excellent Quality Executions: {excellent_quality['percentage']}% of all executions\")\n",
    "\n",
    "if spark_analytics[\"efficiency_metrics\"]:\n",
    "    most_efficient = spark_analytics[\"efficiency_metrics\"][0]\n",
    "    print(f\"Most Efficient Pipeline: {most_efficient['pipeline_id']} ({most_efficient['avg_throughput_per_minute']:.1f} records/minute)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=== Data Pipeline Integration Summary ===\")\n",
    "\n",
    "print(\"\\n=== Pipeline Orchestration ====\")\n",
    "print(\"SUCCESS: Advanced pipeline orchestration engine with dependency management\")\n",
    "print(\"SUCCESS: Parallel and sequential execution strategies\")\n",
    "print(\"SUCCESS: Retry mechanisms and error handling at stage level\")\n",
    "print(\"SUCCESS: Type-safe pipeline definitions with comprehensive validation\")\n",
    "\n",
    "print(\"\\n=== Data Quality Management ====\")\n",
    "print(\"SUCCESS: Comprehensive data quality metrics and monitoring\")\n",
    "print(\"SUCCESS: Automated quality scoring and threshold validation\")\n",
    "print(\"SUCCESS: Quality trend analysis and anomaly detection\")\n",
    "print(\"SUCCESS: Data completeness, accuracy, and compliance tracking\")\n",
    "\n",
    "print(\"\\n=== Stage Execution Framework ====\")\n",
    "print(\"SUCCESS: Pluggable stage executor architecture\")\n",
    "print(\"SUCCESS: Data extraction, transformation, and loading stages\")\n",
    "print(\"SUCCESS: Customer.IO integration with batch optimization\")\n",
    "print(\"SUCCESS: Resource management and timeout handling\")\n",
    "\n",
    "print(\"\\n=== Monitoring and Analytics ====\")\n",
    "print(\"SUCCESS: Real-time pipeline monitoring and performance tracking\")\n",
    "print(\"SUCCESS: Historical analytics and trend analysis\")\n",
    "print(\"SUCCESS: Anomaly detection and proactive alerting\")\n",
    "print(\"SUCCESS: System health metrics and dashboard capabilities\")\n",
    "\n",
    "print(\"\\n=== Spark Integration ====\")\n",
    "print(\"SUCCESS: Large-scale pipeline analytics with Spark SQL\")\n",
    "print(\"SUCCESS: Performance trend analysis across time dimensions\")\n",
    "print(\"SUCCESS: Data quality distribution and efficiency metrics\")\n",
    "print(\"SUCCESS: Scalable pipeline execution history and reporting\")\n",
    "\n",
    "print(\"\\n=== Advanced Features ====\")\n",
    "print(\"SUCCESS: Dependency graph validation and cycle detection\")\n",
    "print(\"SUCCESS: Conditional execution and hybrid strategies\")\n",
    "print(\"SUCCESS: Multi-trigger support (schedule, event, manual, data arrival)\")\n",
    "print(\"SUCCESS: Resource requirement planning and optimization\")\n",
    "\n",
    "print(\"\\n=== Key Capabilities Demonstrated ====\")\n",
    "print(\"SUCCESS: Enterprise-grade pipeline orchestration with full lifecycle management\")\n",
    "print(\"SUCCESS: Production-ready data quality monitoring and validation\")\n",
    "print(\"SUCCESS: Scalable analytics and performance optimization\")\n",
    "print(\"SUCCESS: Comprehensive error handling and recovery mechanisms\")\n",
    "print(\"SUCCESS: Real-time monitoring with historical trend analysis\")\n",
    "print(\"SUCCESS: Seamless Customer.IO integration with batch processing optimization\")\n",
    "print(\"SUCCESS: Type-safe architecture with extensive validation and quality controls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the API client connection\n",
    "client.close()\n",
    "print(\"SUCCESS: API client connection closed\")\n",
    "\n",
    "print(\"\\nCOMPLETED: Data pipeline integration notebook finished successfully!\")\n",
    "print(\"Ready for monitoring and observability in the next notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This notebook has successfully demonstrated advanced data pipeline integration with Customer.IO:\n",
    "\n",
    "### Key Accomplishments:\n",
    "\n",
    "**Pipeline Orchestration**: Advanced orchestration engine with dependency management and execution strategies\n",
    "\n",
    "**Data Quality Management**: Comprehensive quality metrics, monitoring, and automated validation\n",
    "\n",
    "**Stage Execution Framework**: Pluggable architecture with extract, transform, load, and validation stages\n",
    "\n",
    "**Monitoring & Analytics**: Real-time monitoring, historical analysis, and anomaly detection\n",
    "\n",
    "**Spark Integration**: Large-scale analytics, trend analysis, and performance optimization\n",
    "\n",
    "**Error Handling**: Advanced retry mechanisms, failure recovery, and dependency resolution\n",
    "\n",
    "### Pipeline Integration Features Implemented:\n",
    "\n",
    "1. **Pipeline Design**: Multi-stage pipelines with complex dependency graphs\n",
    "2. **Data Quality**: Comprehensive quality scoring, validation rules, and compliance tracking\n",
    "3. **Orchestration**: Parallel, sequential, and hybrid execution strategies\n",
    "4. **Monitoring**: Real-time performance tracking, anomaly detection, and system health\n",
    "5. **Analytics**: Historical trend analysis, efficiency metrics, and optimization insights\n",
    "6. **Integration**: Seamless Customer.IO integration with batch processing optimization\n",
    "\n",
    "### Ready for Next Notebooks:\n",
    "\n",
    "1. **11_monitoring_and_observability.ipynb** - Production monitoring and alerting\n",
    "2. **12_production_deployment.ipynb** - Deployment strategies and best practices\n",
    "\n",
    "The data pipeline integration foundation provides enterprise-grade orchestration capabilities for complex Customer.IO data workflows with comprehensive quality management and monitoring!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}