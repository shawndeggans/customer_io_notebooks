{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer.IO Data Pipelines API - Advanced Pipeline Integration\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook demonstrates advanced data pipeline integration and orchestration with Customer.IO's Data Pipelines API.\n",
    "It covers pipeline design patterns, data flow orchestration, transformation pipelines, scheduling, dependency management, and integration with external data sources.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Complete setup from `00_setup_and_configuration.ipynb`\n",
    "- Complete authentication setup from `01_authentication_and_utilities.ipynb`\n",
    "- Understanding of batch operations from `09_batch_operations.ipynb`\n",
    "- Customer.IO API key configured in Databricks secrets\n",
    "- Understanding of data pipeline concepts and ETL processes\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **Pipeline Orchestration**: Coordinating complex data workflows\n",
    "- **Data Transformation**: ETL processes and data quality management\n",
    "- **Dependency Management**: Pipeline dependencies and execution order\n",
    "- **Scheduling**: Time-based and event-driven pipeline execution\n",
    "- **Data Quality**: Validation, cleansing, and enrichment\n",
    "- **Integration Patterns**: Connecting multiple data sources and systems\n",
    "\n",
    "## Pipeline Operations Covered\n",
    "\n",
    "1. **Pipeline Design**: Multi-stage pipelines with dependencies\n",
    "2. **Data Ingestion**: Batch and streaming data ingestion patterns\n",
    "3. **Transformation**: Data cleansing, enrichment, and standardization\n",
    "4. **Orchestration**: Workflow management and execution coordination\n",
    "5. **Monitoring**: Pipeline health, performance, and data quality metrics\n",
    "6. **Integration**: External systems, APIs, and data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Standard library imports\nimport sys\nimport os\nimport json\nimport time\nfrom datetime import datetime, timezone, timedelta\nfrom typing import Dict, List, Optional, Any, Union, Tuple\nimport uuid\nfrom collections import defaultdict\nimport statistics\n\n# Import warnings handler\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"SUCCESS: Standard libraries imported\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add utils directory to Python path\n",
    "sys.path.append('/Workspace/Repos/customer_io_notebooks/utils')\n",
    "print(\"SUCCESS: Utils directory added to Python path\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Import Customer.IO utilities including PipelineManager\nfrom utils.api_client import CustomerIOClient\nfrom utils.authentication_manager import AuthenticationManager, AuthenticationConfig\nfrom utils.pipeline_manager import (\n    PipelineManager,\n    PipelineStageType,\n    PipelineStatus,\n    ExecutionStrategy,\n    DataSourceType,\n    DataQualityRule,\n    TriggerType,\n    DataQualityMetrics,\n    PipelineExecution,\n    PipelineStage\n)\nfrom utils.event_manager import EventManager\nfrom utils.people_manager import PeopleManager\n\nprint(\"SUCCESS: Customer.IO utilities imported with existing PipelineManager\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Import validation and transformation utilities\nfrom utils.validators import validate_request_size, create_context\nfrom utils.transformers import BatchTransformer, ContextTransformer\nfrom utils.error_handlers import (\n    CustomerIOError,\n    RateLimitError,\n    ValidationError,\n    NetworkError,\n    retry_on_error,\n    ErrorContext\n)\n\nprint(\"SUCCESS: Validation and transformation utilities imported\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Import Databricks and Spark utilities\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import *\nfrom delta.tables import DeltaTable\n\n# Import validation and logging\nimport structlog\nfrom pydantic import ValidationError as PydanticValidationError\n\n# Initialize logger\nlogger = structlog.get_logger(\"pipeline_integration\")\n\nprint(\"SUCCESS: Databricks, Spark, and logging utilities imported\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Load configuration from setup notebook\ntry:\n    CUSTOMERIO_REGION = dbutils.widgets.get(\"customerio_region\") or \"us\"\n    DATABASE_NAME = dbutils.widgets.get(\"database_name\") or \"customerio_demo\"\n    CATALOG_NAME = dbutils.widgets.get(\"catalog_name\") or \"main\"\n    ENVIRONMENT = dbutils.widgets.get(\"environment\") or \"test\"\n    \n    print(f\"Configuration loaded from setup notebook:\")\n    print(f\"  Region: {CUSTOMERIO_REGION}\")\n    print(f\"  Database: {CATALOG_NAME}.{DATABASE_NAME}\")\n    print(f\"  Environment: {ENVIRONMENT}\")\n    \nexcept Exception as e:\n    print(f\"WARNING: Could not load configuration from setup notebook: {str(e)}\")\n    print(\"INFO: Using fallback configuration\")\n    CUSTOMERIO_REGION = \"us\"\n    DATABASE_NAME = \"customerio_demo\"\n    CATALOG_NAME = \"main\"\n    ENVIRONMENT = \"test\"\n\n# Configure Spark to use the specified database\nspark.sql(f\"USE {CATALOG_NAME}.{DATABASE_NAME}\")\nprint(\"SUCCESS: Database configured\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-Driven Development: Pipeline Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Initialize authentication and PipelineManager\ntry:\n    # Get API key from secure storage\n    CUSTOMERIO_API_KEY = dbutils.secrets.get(\"customerio\", \"api_key\")\n    \n    # Create authentication configuration\n    auth_config = AuthenticationConfig(\n        api_key=CUSTOMERIO_API_KEY,\n        region=CUSTOMERIO_REGION,\n        environment=ENVIRONMENT\n    )\n    \n    # Initialize authentication manager\n    auth_manager = AuthenticationManager(auth_config, spark)\n    client = auth_manager.connect()\n    \n    # Initialize PipelineManager using existing implementation\n    pipeline_manager = PipelineManager(client, spark)\n    \n    # Initialize other managers\n    event_manager = EventManager(client)\n    people_manager = PeopleManager(client)\n    \n    print(\"SUCCESS: PipelineManager and other managers initialized\")\n    print(f\"  PipelineManager ready for orchestration\")\n    print(f\"  Event and People managers available for data operations\")\n    \nexcept Exception as e:\n    print(f\"ERROR: Failed to initialize managers: {str(e)}\")\n    raise",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create a customer data pipeline using existing PipelineManager\nprint(\"=== Creating Customer Data Pipeline ===\")\n\n# Define data sources for the pipeline\ndata_sources = [\n    {\n        \"source_id\": \"customer_database\",\n        \"source_type\": DataSourceType.DATABASE,\n        \"config\": {\n            \"table_name\": \"customers\",\n            \"batch_size\": 1000,\n            \"quality_checks\": True\n        }\n    }\n]\n\n# Create pipeline configuration\npipeline_config = {\n    \"pipeline_id\": \"customer_data_pipeline_v2\",\n    \"pipeline_name\": \"Customer Data Integration Pipeline\",\n    \"description\": \"Comprehensive customer data ingestion and processing\",\n    \"version\": \"2.0\",\n    \"environment\": ENVIRONMENT,\n    \"data_sources\": data_sources,\n    \"quality_threshold\": 0.85,\n    \"enable_monitoring\": True,\n    \"retry_failed_stages\": True,\n    \"max_parallel_stages\": 3\n}\n\n# Create the pipeline using PipelineManager\npipeline_definition = pipeline_manager.create_pipeline(pipeline_config)\n\nprint(f\"Pipeline created successfully:\")\nprint(f\"  Pipeline ID: {pipeline_definition['pipeline_id']}\")\nprint(f\"  Version: {pipeline_definition['version']}\")\nprint(f\"  Total Stages: {len(pipeline_definition['stages'])}\")\nprint(f\"  Data Sources: {len(pipeline_definition['data_sources'])}\")\n\n# Display pipeline stages\nprint(f\"\\nPipeline Stages:\")\nfor i, stage in enumerate(pipeline_definition['stages']):\n    deps = \", \".join(stage.get('dependencies', [])) if stage.get('dependencies') else \"None\"\n    print(f\"  {i+1}. {stage['stage_name']} ({stage['stage_type']}) - Dependencies: {deps}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Execute the pipeline using PipelineManager\nprint(\"=== Pipeline Execution ===\")\n\n# Configure execution parameters\nexecution_config = {\n    \"execution_strategy\": ExecutionStrategy.HYBRID,\n    \"trigger_type\": TriggerType.MANUAL,\n    \"enable_monitoring\": True,\n    \"enable_alerts\": True,\n    \"timeout_minutes\": 60,\n    \"max_retries\": 3\n}\n\n# Execute the pipeline\nprint(\"Starting pipeline execution...\")\nstart_time = time.time()\n\nexecution_result = pipeline_manager.execute_pipeline(\n    pipeline_definition['pipeline_id'],\n    execution_config\n)\n\nend_time = time.time()\nexecution_time = end_time - start_time\n\nprint(f\"\\nPipeline execution completed:\")\nprint(f\"  Execution ID: {execution_result['execution_id']}\")\nprint(f\"  Status: {execution_result['status']}\")\nprint(f\"  Total Time: {execution_time:.2f} seconds\")\nprint(f\"  Stages Completed: {execution_result['completed_stages']}\")\nprint(f\"  Records Processed: {execution_result.get('records_processed', 0):,}\")\n\nif execution_result.get('quality_metrics'):\n    qm = execution_result['quality_metrics']\n    print(f\"  Overall Quality Score: {qm.get('overall_score', 0):.3f}\")\n    print(f\"  Valid Records: {qm.get('valid_records', 0):,}\")\n    print(f\"  Invalid Records: {qm.get('invalid_records', 0):,}\")\n\nprint(\"SUCCESS: Pipeline executed using PipelineManager\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Validate pipeline configuration using PipelineManager\nprint(\"=== Pipeline Validation ===\")\n\n# Validate the pipeline definition\nvalidation_result = pipeline_manager.validate_pipeline(pipeline_definition)\n\nprint(f\"Pipeline validation results:\")\nprint(f\"  Valid: {validation_result['is_valid']}\")\nprint(f\"  Stage count: {validation_result['stage_count']}\")\nprint(f\"  Dependency validation: {validation_result['dependencies_valid']}\")\n\nif validation_result.get('warnings'):\n    print(f\"  Warnings: {len(validation_result['warnings'])}\")\n    for warning in validation_result['warnings']:\n        print(f\"    - {warning}\")\n\nif validation_result.get('errors'):\n    print(f\"  Errors: {len(validation_result['errors'])}\")\n    for error in validation_result['errors']:\n        print(f\"    - {error}\")\n\nprint(\"SUCCESS: Pipeline validation completed\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": "## Configuration and Authentication"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Get pipeline monitoring and analytics using PipelineManager\nprint(\"=== Pipeline Monitoring ===\")\n\n# Get execution history\nexecution_history = pipeline_manager.get_execution_history(\n    pipeline_id=pipeline_definition['pipeline_id'],\n    limit=10\n)\n\nprint(f\"Recent executions for {pipeline_definition['pipeline_id']}:\")\nprint(f\"  Total executions found: {len(execution_history)}\")\n\nfor i, execution in enumerate(execution_history[:5]):  # Show first 5\n    status_icon = \"✓\" if execution['status'] == 'completed' else \"✗\" if execution['status'] == 'failed' else \"⏳\"\n    print(f\"  {i+1}. {status_icon} {execution['execution_id'][:8]}... - {execution['status']} ({execution.get('execution_time', 'N/A')})\")\n\n# Get pipeline metrics\nmetrics = pipeline_manager.get_pipeline_metrics(pipeline_definition['pipeline_id'])\n\nprint(f\"\\nPipeline Performance Metrics:\")\nprint(f\"  Success Rate: {metrics.get('success_rate_percent', 0):.1f}%\")\nprint(f\"  Average Execution Time: {metrics.get('avg_execution_time_minutes', 0):.2f} minutes\")\nprint(f\"  Average Quality Score: {metrics.get('avg_quality_score', 0):.3f}\")\nprint(f\"  Total Records Processed: {metrics.get('total_records_processed', 0):,}\")\n\n# Check for performance anomalies\nanomalies = pipeline_manager.detect_performance_anomalies(pipeline_definition['pipeline_id'])\n\nprint(f\"\\nAnomaly Detection:\")\nif anomalies:\n    print(f\"  Found {len(anomalies)} potential issues:\")\n    for anomaly in anomalies:\n        print(f\"    [{anomaly['severity']}] {anomaly['description']}\")\nelse:\n    print(\"  No performance anomalies detected\")\n\nprint(\"SUCCESS: Pipeline monitoring data retrieved\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": "## Data Quality Management"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Validate pipeline configuration using PipelineManager\nprint(\"=== Pipeline Validation ===\")\n\n# Validate the pipeline definition\nvalidation_result = pipeline_manager.validate_pipeline(pipeline_definition)\n\nprint(f\"Pipeline validation results:\")\nprint(f\"  Valid: {validation_result['is_valid']}\")\nprint(f\"  Stage count: {validation_result['stage_count']}\")\nprint(f\"  Dependency validation: {validation_result['dependencies_valid']}\")\n\nif validation_result.get('warnings'):\n    print(f\"  Warnings: {len(validation_result['warnings'])}\")\n    for warning in validation_result['warnings']:\n        print(f\"    - {warning}\")\n\nif validation_result.get('errors'):\n    print(f\"  Errors: {len(validation_result['errors'])}\")\n    for error in validation_result['errors']:\n        print(f\"    - {error}\")\n\nprint(\"SUCCESS: Pipeline validation completed\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Advanced Pipeline Features",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate advanced pipeline features\nprint(\"=== Advanced Pipeline Features ===\")\n\n# 1. Pipeline scheduling\nschedule_config = {\n    \"trigger_type\": TriggerType.SCHEDULE,\n    \"cron_expression\": \"0 2 * * *\",  # Daily at 2 AM\n    \"timezone\": \"UTC\",\n    \"enabled\": True\n}\n\npipeline_manager.schedule_pipeline(\n    pipeline_definition['pipeline_id'],\n    schedule_config\n)\n\nprint(f\"Pipeline scheduled:\")\nprint(f\"  Schedule: Daily at 2:00 AM UTC\")\nprint(f\"  Status: Enabled\")\n\n# 2. Pipeline dependency management\ndependent_pipeline_config = {\n    \"pipeline_id\": \"event_processing_pipeline\",\n    \"pipeline_name\": \"Event Processing Pipeline\",\n    \"description\": \"Process events after customer data is loaded\",\n    \"dependencies\": [pipeline_definition['pipeline_id']],\n    \"data_sources\": [],\n    \"quality_threshold\": 0.9\n}\n\ndependent_pipeline = pipeline_manager.create_pipeline(dependent_pipeline_config)\n\nprint(f\"\\nDependent pipeline created:\")\nprint(f\"  Pipeline: {dependent_pipeline['pipeline_name']}\")\nprint(f\"  Depends on: {', '.join(dependent_pipeline['dependencies'])}\")\n\n# 3. Pipeline versioning\nnew_version = pipeline_manager.create_pipeline_version(\n    pipeline_definition['pipeline_id'],\n    {\n        \"version\": \"2.1\",\n        \"changes\": [\"Improved data quality rules\", \"Added monitoring\"],\n        \"backward_compatible\": True\n    }\n)\n\nprint(f\"\\nPipeline versioning:\")\nprint(f\"  New version: {new_version['version']}\")\nprint(f\"  Changes: {len(new_version['changes'])} improvements\")\nprint(f\"  Backward compatible: {new_version['backward_compatible']}\")\n\nprint(\"SUCCESS: Advanced pipeline features demonstrated\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up and execute the pipeline\n",
    "orchestrator = PipelineOrchestrator(client)\n",
    "\n",
    "# Register stage executors\n",
    "orchestrator.register_stage_executor(PipelineStageType.EXTRACT, DataExtractionStage)\n",
    "orchestrator.register_stage_executor(PipelineStageType.TRANSFORM, DataTransformationStage)\n",
    "orchestrator.register_stage_executor(PipelineStageType.VALIDATE, DataTransformationStage)  # Reuse transform logic\n",
    "orchestrator.register_stage_executor(PipelineStageType.LOAD, CustomerIOLoadStage)\n",
    "\n",
    "print(\"=== Executing Customer Data Pipeline ===\")\n",
    "\n",
    "# Execute the pipeline\n",
    "start_time = time.time()\n",
    "completed_pipeline = orchestrator.execute_pipeline(customer_pipeline)\n",
    "end_time = time.time()\n",
    "\n",
    "actual_execution_time = end_time - start_time\n",
    "\n",
    "print(f\"\\n=== Pipeline Execution Results ===\")\n",
    "print(f\"Execution ID: {completed_pipeline.execution_id}\")\n",
    "print(f\"Final Status: {completed_pipeline.status}\")\n",
    "print(f\"Progress: {completed_pipeline.get_progress_percent():.1f}%\")\n",
    "print(f\"Total Execution Time: {completed_pipeline.total_execution_time_minutes:.2f} minutes\")\n",
    "print(f\"Records Processed: {completed_pipeline.total_records_processed:,}\")\n",
    "print(f\"Overall Quality Score: {completed_pipeline.overall_quality_score:.3f}\" if completed_pipeline.overall_quality_score else \"N/A\")\n",
    "\n",
    "print(f\"\\n=== Stage Results ===\")\n",
    "for stage in completed_pipeline.stages:\n",
    "    status_icon = \"✓\" if stage.status == PipelineStatus.COMPLETED else \"✗\" if stage.status == PipelineStatus.FAILED else \"⏳\"\n",
    "    runtime = f\"{stage.execution_time_minutes:.2f}m\" if stage.execution_time_minutes else \"N/A\"\n",
    "    quality = f\"{stage.quality_metrics.quality_score:.3f}\" if stage.quality_metrics else \"N/A\"\n",
    "    \n",
    "    print(f\"  {status_icon} {stage.stage_name}: {stage.status} ({runtime}, quality: {quality})\")\n",
    "    \n",
    "    if stage.error_message:\n",
    "        print(f\"    Error: {stage.error_message}\")\n",
    "    \n",
    "    if stage.quality_metrics:\n",
    "        qm = stage.quality_metrics\n",
    "        print(f\"    Records: {qm.total_records} total, {qm.valid_records} valid, {qm.invalid_records} invalid\")\n",
    "\n",
    "print(f\"\\n=== Execution Summary ===\")\n",
    "print(f\"Completed Stages: {len(completed_pipeline.completed_stages)}\")\n",
    "print(f\"Failed Stages: {len(completed_pipeline.failed_stages)}\")\n",
    "print(f\"Skipped Stages: {len(completed_pipeline.skipped_stages)}\")\n",
    "\n",
    "if completed_pipeline.failed_stages:\n",
    "    print(f\"Failed Stage IDs: {', '.join(completed_pipeline.failed_stages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation: Pipeline monitoring and analytics\n",
    "class PipelineMonitor:\n",
    "    \"\"\"Advanced pipeline monitoring and analytics.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = structlog.get_logger(\"pipeline_monitor\")\n",
    "        self.execution_history = deque(maxlen=1000)\n",
    "        self.performance_metrics = defaultdict(list)\n",
    "        \n",
    "    def record_execution(self, execution: PipelineExecution) -> None:\n",
    "        \"\"\"Record pipeline execution for monitoring.\"\"\"\n",
    "        \n",
    "        execution_record = {\n",
    "            \"execution_id\": execution.execution_id,\n",
    "            \"pipeline_id\": execution.pipeline_id,\n",
    "            \"status\": execution.status,\n",
    "            \"started_at\": execution.started_at,\n",
    "            \"completed_at\": execution.completed_at,\n",
    "            \"total_execution_time_minutes\": execution.total_execution_time_minutes,\n",
    "            \"records_processed\": execution.total_records_processed,\n",
    "            \"overall_quality_score\": execution.overall_quality_score,\n",
    "            \"stage_count\": len(execution.stages),\n",
    "            \"completed_stages\": len(execution.completed_stages),\n",
    "            \"failed_stages\": len(execution.failed_stages)\n",
    "        }\n",
    "        \n",
    "        self.execution_history.append(execution_record)\n",
    "        \n",
    "        # Record performance metrics\n",
    "        pipeline_id = execution.pipeline_id\n",
    "        if execution.total_execution_time_minutes:\n",
    "            self.performance_metrics[f\"{pipeline_id}_execution_time\"].append(\n",
    "                execution.total_execution_time_minutes\n",
    "            )\n",
    "        \n",
    "        if execution.overall_quality_score:\n",
    "            self.performance_metrics[f\"{pipeline_id}_quality_score\"].append(\n",
    "                execution.overall_quality_score\n",
    "            )\n",
    "        \n",
    "        if execution.total_records_processed:\n",
    "            self.performance_metrics[f\"{pipeline_id}_throughput\"].append(\n",
    "                execution.total_records_processed / (execution.total_execution_time_minutes or 1)\n",
    "            )\n",
    "    \n",
    "    def get_pipeline_analytics(self, pipeline_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive analytics for a pipeline.\"\"\"\n",
    "        \n",
    "        # Filter executions for this pipeline\n",
    "        pipeline_executions = [\n",
    "            exec_record for exec_record in self.execution_history\n",
    "            if exec_record[\"pipeline_id\"] == pipeline_id\n",
    "        ]\n",
    "        \n",
    "        if not pipeline_executions:\n",
    "            return {\"error\": \"No execution data found for pipeline\"}\n",
    "        \n",
    "        # Calculate success/failure rates\n",
    "        total_executions = len(pipeline_executions)\n",
    "        successful_executions = len([\n",
    "            e for e in pipeline_executions \n",
    "            if e[\"status\"] == PipelineStatus.COMPLETED\n",
    "        ])\n",
    "        failed_executions = len([\n",
    "            e for e in pipeline_executions \n",
    "            if e[\"status\"] == PipelineStatus.FAILED\n",
    "        ])\n",
    "        \n",
    "        success_rate = (successful_executions / total_executions * 100) if total_executions > 0 else 0\n",
    "        \n",
    "        # Calculate performance statistics\n",
    "        execution_times = [\n",
    "            e[\"total_execution_time_minutes\"] for e in pipeline_executions\n",
    "            if e[\"total_execution_time_minutes\"]\n",
    "        ]\n",
    "        \n",
    "        quality_scores = [\n",
    "            e[\"overall_quality_score\"] for e in pipeline_executions\n",
    "            if e[\"overall_quality_score\"]\n",
    "        ]\n",
    "        \n",
    "        records_processed = [\n",
    "            e[\"records_processed\"] for e in pipeline_executions\n",
    "            if e[\"records_processed\"]\n",
    "        ]\n",
    "        \n",
    "        # Recent performance (last 10 executions)\n",
    "        recent_executions = sorted(\n",
    "            pipeline_executions, \n",
    "            key=lambda x: x[\"started_at\"] or datetime.min.replace(tzinfo=timezone.utc),\n",
    "            reverse=True\n",
    "        )[:10]\n",
    "        \n",
    "        analytics = {\n",
    "            \"pipeline_id\": pipeline_id,\n",
    "            \"total_executions\": total_executions,\n",
    "            \"successful_executions\": successful_executions,\n",
    "            \"failed_executions\": failed_executions,\n",
    "            \"success_rate_percent\": round(success_rate, 2),\n",
    "            \"performance_stats\": {\n",
    "                \"avg_execution_time_minutes\": round(statistics.mean(execution_times), 2) if execution_times else 0,\n",
    "                \"min_execution_time_minutes\": round(min(execution_times), 2) if execution_times else 0,\n",
    "                \"max_execution_time_minutes\": round(max(execution_times), 2) if execution_times else 0,\n",
    "                \"avg_quality_score\": round(statistics.mean(quality_scores), 3) if quality_scores else 0,\n",
    "                \"avg_records_per_execution\": round(statistics.mean(records_processed), 0) if records_processed else 0,\n",
    "                \"total_records_processed\": sum(records_processed) if records_processed else 0\n",
    "            },\n",
    "            \"recent_trend\": {\n",
    "                \"last_10_executions\": len(recent_executions),\n",
    "                \"recent_success_rate\": (\n",
    "                    len([e for e in recent_executions if e[\"status\"] == PipelineStatus.COMPLETED]) /\n",
    "                    len(recent_executions) * 100\n",
    "                ) if recent_executions else 0,\n",
    "                \"recent_avg_time\": round(\n",
    "                    statistics.mean([\n",
    "                        e[\"total_execution_time_minutes\"] for e in recent_executions\n",
    "                        if e[\"total_execution_time_minutes\"]\n",
    "                    ]), 2\n",
    "                ) if recent_executions else 0\n",
    "            },\n",
    "            \"last_execution\": recent_executions[0] if recent_executions else None,\n",
    "            \"analyzed_at\": datetime.now(timezone.utc).isoformat()\n",
    "        }\n",
    "        \n",
    "        return analytics\n",
    "    \n",
    "    def detect_anomalies(self, pipeline_id: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Detect performance anomalies in pipeline executions.\"\"\"\n",
    "        \n",
    "        anomalies = []\n",
    "        \n",
    "        # Get performance metrics\n",
    "        execution_times = self.performance_metrics.get(f\"{pipeline_id}_execution_time\", [])\n",
    "        quality_scores = self.performance_metrics.get(f\"{pipeline_id}_quality_score\", [])\n",
    "        throughputs = self.performance_metrics.get(f\"{pipeline_id}_throughput\", [])\n",
    "        \n",
    "        # Detect execution time anomalies\n",
    "        if len(execution_times) >= 5:\n",
    "            avg_time = statistics.mean(execution_times)\n",
    "            std_time = statistics.stdev(execution_times)\n",
    "            recent_time = execution_times[-1]\n",
    "            \n",
    "            if recent_time > avg_time + (2 * std_time):  # 2 standard deviations\n",
    "                anomalies.append({\n",
    "                    \"type\": \"slow_execution\",\n",
    "                    \"severity\": \"warning\",\n",
    "                    \"description\": f\"Execution time {recent_time:.2f}m is significantly above average {avg_time:.2f}m\",\n",
    "                    \"metric\": \"execution_time\",\n",
    "                    \"value\": recent_time,\n",
    "                    \"threshold\": avg_time + (2 * std_time)\n",
    "                })\n",
    "        \n",
    "        # Detect quality score anomalies\n",
    "        if len(quality_scores) >= 5:\n",
    "            avg_quality = statistics.mean(quality_scores)\n",
    "            recent_quality = quality_scores[-1]\n",
    "            \n",
    "            if recent_quality < avg_quality - 0.1:  # Quality drop > 10%\n",
    "                anomalies.append({\n",
    "                    \"type\": \"quality_degradation\",\n",
    "                    \"severity\": \"critical\",\n",
    "                    \"description\": f\"Quality score {recent_quality:.3f} is significantly below average {avg_quality:.3f}\",\n",
    "                    \"metric\": \"quality_score\",\n",
    "                    \"value\": recent_quality,\n",
    "                    \"threshold\": avg_quality - 0.1\n",
    "                })\n",
    "        \n",
    "        # Detect throughput anomalies\n",
    "        if len(throughputs) >= 5:\n",
    "            avg_throughput = statistics.mean(throughputs)\n",
    "            recent_throughput = throughputs[-1]\n",
    "            \n",
    "            if recent_throughput < avg_throughput * 0.7:  # 30% drop in throughput\n",
    "                anomalies.append({\n",
    "                    \"type\": \"low_throughput\",\n",
    "                    \"severity\": \"warning\",\n",
    "                    \"description\": f\"Throughput {recent_throughput:.1f} records/min is significantly below average {avg_throughput:.1f}\",\n",
    "                    \"metric\": \"throughput\",\n",
    "                    \"value\": recent_throughput,\n",
    "                    \"threshold\": avg_throughput * 0.7\n",
    "                })\n",
    "        \n",
    "        return anomalies\n",
    "    \n",
    "    def get_system_health(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get overall system health metrics.\"\"\"\n",
    "        \n",
    "        if not self.execution_history:\n",
    "            return {\"status\": \"no_data\"}\n",
    "        \n",
    "        # Recent executions (last hour)\n",
    "        cutoff_time = datetime.now(timezone.utc) - timedelta(hours=1)\n",
    "        recent_executions = [\n",
    "            e for e in self.execution_history\n",
    "            if e[\"started_at\"] and e[\"started_at\"] > cutoff_time\n",
    "        ]\n",
    "        \n",
    "        # Calculate health metrics\n",
    "        total_recent = len(recent_executions)\n",
    "        successful_recent = len([\n",
    "            e for e in recent_executions \n",
    "            if e[\"status\"] == PipelineStatus.COMPLETED\n",
    "        ])\n",
    "        \n",
    "        health_score = (successful_recent / total_recent * 100) if total_recent > 0 else 100\n",
    "        \n",
    "        # Determine overall status\n",
    "        if health_score >= 95:\n",
    "            status = \"healthy\"\n",
    "        elif health_score >= 80:\n",
    "            status = \"degraded\"\n",
    "        else:\n",
    "            status = \"unhealthy\"\n",
    "        \n",
    "        return {\n",
    "            \"status\": status,\n",
    "            \"health_score_percent\": round(health_score, 2),\n",
    "            \"recent_executions_1h\": total_recent,\n",
    "            \"successful_executions_1h\": successful_recent,\n",
    "            \"failed_executions_1h\": total_recent - successful_recent,\n",
    "            \"total_pipelines_monitored\": len(set(e[\"pipeline_id\"] for e in self.execution_history)),\n",
    "            \"last_updated\": datetime.now(timezone.utc).isoformat()\n",
    "        }\n",
    "\n",
    "print(\"SUCCESS: PipelineMonitor class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Data from Spark Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Spark Integration and Analytics",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Clean Up and Summary",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}