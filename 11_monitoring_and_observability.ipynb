{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Customer.IO Monitoring and Observability\n\n## Purpose\n\nThis notebook demonstrates comprehensive monitoring and observability solutions for Customer.IO data pipelines using the existing ObservabilityManager from utils/observability_manager.py.\n\n## Prerequisites\n\n- Complete setup from `00_setup_and_configuration.ipynb`\n- Complete authentication from `01_authentication_and_utilities.ipynb`\n- Customer.IO API key configured in Databricks secrets\n- Understanding of monitoring and observability concepts\n\n## Key Topics Covered\n\n1. **Metrics Collection** - Real-time performance and business metrics\n2. **Alerting** - Intelligent threshold-based alerts and notifications\n3. **Health Monitoring** - System and service health checks\n4. **Distributed Tracing** - Request correlation and performance analysis\n5. **Dashboard Integration** - Comprehensive observability dashboards\n6. **Production Monitoring** - Enterprise-grade monitoring patterns",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for monitoring and observability\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from typing import Dict, List, Optional, Any, Union, Callable, Set\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict, deque\n",
    "from enum import Enum\n",
    "import structlog\n",
    "from pydantic import BaseModel, Field, validator\n",
    "import statistics\n",
    "import uuid\n",
    "import psutil\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import asyncio\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"SUCCESS: Core monitoring imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Import Customer.IO utilities with existing ObservabilityManager\nimport sys\nimport os\n\n# Add utils to path for imports\nsys.path.append(os.path.join(os.getcwd(), 'utils'))\n\nfrom api_client import CustomerIOClient\nfrom observability_manager import (\n    ObservabilityManager,\n    MetricType,\n    AlertSeverity,\n    HealthStatus,\n    TraceStatus,\n    Metric,\n    Alert,\n    HealthCheck,\n    TraceSpan,\n    MetricsCollector,\n    AlertRule,\n    AlertManager,\n    HealthMonitor,\n    TracingContext,\n    DistributedTracer,\n    trace_function\n)\nfrom error_handlers import retry_on_error, ErrorContext, CustomerIOError\nfrom validators import validate_request_size\n\nprint(\"SUCCESS: Customer.IO utilities and ObservabilityManager imported\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Using Existing ObservabilityManager\n\nThe observability components are already implemented in `utils/observability_manager.py`. This notebook demonstrates usage of the existing manager.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Core enums are imported from utils/observability_manager.py\nprint(\"Available metric types:\", [metric_type.value for metric_type in MetricType])\nprint(\"Available alert severities:\", [severity.value for severity in AlertSeverity])\nprint(\"Available health statuses:\", [status.value for status in HealthStatus])\nprint(\"Available trace statuses:\", [status.value for status in TraceStatus])\n\nprint(\"SUCCESS: Using existing monitoring enums from ObservabilityManager\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Example: Creating metrics using existing Metric model\ntry:\n    sample_metric = Metric(\n        name=\"api_response_time\",\n        type=MetricType.TIMER,\n        value=150.5,\n        unit=\"ms\",\n        tags={\"endpoint\": \"/api/track\", \"method\": \"POST\"},\n        source=\"api_server\"\n    )\n    \n    print(\"Created sample metric:\")\n    print(f\"  Name: {sample_metric.name}\")\n    print(f\"  Type: {sample_metric.type}\")\n    print(f\"  Value: {sample_metric.value} {sample_metric.unit}\")\n    print(f\"  Tags: {sample_metric.tags}\")\n    print(f\"  Timestamp: {sample_metric.timestamp}\")\n    \n    # Example alert\n    sample_alert = Alert(\n        name=\"High Response Time\",\n        severity=AlertSeverity.HIGH,\n        message=\"API response time exceeded threshold\",\n        source=\"alert_manager\",\n        metric_name=\"api_response_time\",\n        threshold_value=100.0,\n        current_value=150.5\n    )\n    \n    print(f\"\\nCreated sample alert: {sample_alert.name} ({sample_alert.severity})\")\n    print(f\"Alert ID: {sample_alert.alert_id}\")\n    print(f\"Message: {sample_alert.message}\")\n    print(f\"Is resolved: {sample_alert.is_resolved()}\")\n    \n    print(\"SUCCESS: Using existing Metric and Alert models\")\n    \nexcept Exception as e:\n    print(f\"ERROR: Failed to create models: {e}\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Example: Creating health checks and trace spans\ntry:\n    # Health check example\n    health_check = HealthCheck(\n        check_id=\"api_connectivity\",\n        name=\"Customer.IO API Connectivity\",\n        status=HealthStatus.HEALTHY,\n        response_time_ms=45.2,\n        details={\"endpoint\": \"api.customer.io\", \"region\": \"us\"},\n        consecutive_failures=0\n    )\n    \n    print(\"Created health check:\")\n    print(f\"  Name: {health_check.name}\")\n    print(f\"  Status: {health_check.status}\")\n    print(f\"  Response time: {health_check.response_time_ms}ms\")\n    print(f\"  Is healthy: {health_check.is_healthy()}\")\n    \n    # Trace span example\n    trace_span = TraceSpan(\n        trace_id=\"trace-12345\",\n        operation_name=\"send_customer_event\",\n        service_name=\"customer_io_service\",\n        tags={\"user_id\": \"user_123\", \"event_type\": \"track\"}\n    )\n    \n    print(f\"\\nCreated trace span:\")\n    print(f\"  Trace ID: {trace_span.trace_id}\")\n    print(f\"  Span ID: {trace_span.span_id}\")\n    print(f\"  Operation: {trace_span.operation_name}\")\n    print(f\"  Service: {trace_span.service_name}\")\n    print(f\"  Status: {trace_span.status}\")\n    \n    # Simulate span completion\n    trace_span.add_log(\"info\", \"Starting event processing\")\n    trace_span.finish(TraceStatus.SUCCESS)\n    print(f\"  Duration: {trace_span.duration_ms}ms\")\n    print(f\"  Logs: {len(trace_span.logs)} entries\")\n    \n    print(\"SUCCESS: Using existing HealthCheck and TraceSpan models\")\n    \nexcept Exception as e:\n    print(f\"ERROR: Failed to create models: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Using Existing MetricsCollector\n\nThe MetricsCollector is already implemented in `utils/observability_manager.py` with high-performance buffering and aggregation.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Example: Using existing MetricsCollector\ntry:\n    # Initialize MetricsCollector\n    metrics_collector = MetricsCollector(buffer_size=1000, flush_interval_seconds=30)\n    \n    # Record different types of metrics\n    metrics_collector.record_counter(\"api_requests_total\", value=1, tags={\"endpoint\": \"/track\"})\n    metrics_collector.record_gauge(\"memory_usage_mb\", value=512.3, unit=\"MB\")\n    metrics_collector.record_timer(\"response_time\", duration_ms=125.7, tags={\"operation\": \"track_event\"})\n    \n    # Record custom metrics\n    custom_metric = Metric(\n        name=\"custom_business_metric\",\n        type=MetricType.GAUGE,\n        value=89.5,\n        unit=\"percent\",\n        tags={\"category\": \"conversion\", \"funnel\": \"signup\"},\n        source=\"business_logic\"\n    )\n    metrics_collector.record_metric(custom_metric)\n    \n    # Get statistics\n    timer_stats = metrics_collector.get_metric_statistics(\"response_time\", MetricType.TIMER)\n    buffer_status = metrics_collector.get_buffer_status()\n    \n    print(\"MetricsCollector Usage:\")\n    print(f\"  Buffer utilization: {buffer_status['buffer_utilization']:.1f}%\")\n    print(f\"  Aggregated metric types: {buffer_status['aggregated_metric_types']}\")\n    print(f\"  Last flush: {buffer_status['last_flush']}\")\n    \n    if timer_stats:\n        print(f\"  Response time stats: mean={timer_stats.get('mean', 0):.1f}ms, count={timer_stats.get('count', 0)}\")\n    \n    print(\"SUCCESS: Using existing MetricsCollector with buffering and aggregation\")\n    \nexcept Exception as e:\n    print(f\"ERROR: Failed to use MetricsCollector: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Using Existing AlertManager\n\nThe AlertManager is already implemented with intelligent threshold management and notification routing.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Example: Using existing AlertManager\ntry:\n    # Initialize AlertManager with MetricsCollector\n    alert_manager = AlertManager(metrics_collector)\n    \n    # Create alert rules\n    high_error_rate_rule = AlertRule(\n        name=\"High Error Rate Alert\",\n        metric_name=\"error_rate_percent\",\n        condition=\">\",\n        threshold=5.0,\n        severity=AlertSeverity.HIGH,\n        window_minutes=5,\n        consecutive_violations=2,\n        tags={\"team\": \"platform\", \"priority\": \"urgent\"}\n    )\n    \n    response_time_rule = AlertRule(\n        name=\"Response Time Alert\",\n        metric_name=\"avg_response_time_ms\",\n        condition=\">\",\n        threshold=1000.0,\n        severity=AlertSeverity.MEDIUM,\n        window_minutes=10,\n        consecutive_violations=3\n    )\n    \n    # Add rules to manager\n    alert_manager.add_rule(high_error_rate_rule)\n    alert_manager.add_rule(response_time_rule)\n    \n    # Test rule evaluation\n    test_value = 8.5  # This exceeds the 5.0 threshold\n    rule_violated = high_error_rate_rule.evaluate(test_value)\n    \n    print(\"AlertManager Usage:\")\n    print(f\"  Total rules: {len(alert_manager.rules)}\")\n    print(f\"  Test evaluation (error rate {test_value}%): {'VIOLATED' if rule_violated else 'OK'}\")\n    \n    # Get alert statistics\n    alert_stats = alert_manager.get_alert_statistics()\n    print(f\"  Alert statistics: {alert_stats['total_rules']} total, {alert_stats['enabled_rules']} enabled\")\n    print(f\"  Active alerts: {alert_stats['active_alerts']}\")\n    \n    # Get active alerts\n    active_alerts = alert_manager.get_active_alerts()\n    if active_alerts:\n        print(f\"  Current active alerts: {len(active_alerts)}\")\n        for alert in active_alerts[:3]:  # Show first 3\n            print(f\"    - {alert.name} ({alert.severity}): {alert.message}\")\n    else:\n        print(\"  No active alerts\")\n    \n    print(\"SUCCESS: Using existing AlertManager with rule evaluation\")\n    \nexcept Exception as e:\n    print(f\"ERROR: Failed to use AlertManager: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Using Existing HealthMonitor\n\nThe HealthMonitor is already implemented with comprehensive health checks for services and dependencies.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Example: Using existing HealthMonitor\ntry:\n    # Initialize customer.io client for health monitor\n    client = CustomerIOClient(\n        site_id=\"test_site_123\",\n        api_key=\"test_key_456\",\n        region=\"us\"\n    )\n    \n    # Initialize HealthMonitor\n    health_monitor = HealthMonitor(client)\n    \n    # Register custom health check\n    def check_database_connection():\n        \"\"\"Custom database health check.\"\"\"\n        try:\n            # Simulate database check\n            import random\n            response_time = random.uniform(0.01, 0.1)\n            connection_count = random.randint(5, 50)\n            \n            if connection_count > 45:\n                status = HealthStatus.DEGRADED\n            else:\n                status = HealthStatus.HEALTHY\n            \n            return {\n                \"name\": \"Database Connection\",\n                \"status\": status,\n                \"details\": {\n                    \"response_time_ms\": response_time * 1000,\n                    \"active_connections\": connection_count,\n                    \"max_connections\": 50\n                }\n            }\n        except Exception as e:\n            return {\n                \"name\": \"Database Connection\",\n                \"status\": HealthStatus.UNHEALTHY,\n                \"details\": {\"error\": str(e)}\n            }\n    \n    health_monitor.register_check(\"database\", \"Database Connection\", check_database_connection)\n    \n    # Run specific health check\n    db_check = health_monitor.run_check(\"database\")\n    print(\"Database Health Check:\")\n    print(f\"  Name: {db_check.name}\")\n    print(f\"  Status: {db_check.status}\")\n    print(f\"  Response time: {db_check.response_time_ms:.1f}ms\")\n    print(f\"  Details: {db_check.details}\")\n    print(f\"  Is healthy: {db_check.is_healthy()}\")\n    \n    # Run all health checks\n    all_checks = health_monitor.run_all_checks()\n    print(f\"\\nAll Health Checks ({len(all_checks)} total):\")\n    for check_id, check in all_checks.items():\n        status_icon = \"✓\" if check.is_healthy() else \"✗\"\n        print(f\"  {status_icon} {check.name}: {check.status}\")\n    \n    # Get system health overview\n    system_health = health_monitor.get_system_health()\n    print(f\"\\nSystem Health Overview:\")\n    print(f\"  Overall status: {system_health['status']}\")\n    print(f\"  Health percentage: {system_health['summary']['health_percentage']:.1f}%\")\n    print(f\"  Healthy checks: {system_health['summary']['healthy_checks']}/{system_health['summary']['total_checks']}\")\n    \n    print(\"SUCCESS: Using existing HealthMonitor with custom checks\")\n    \nexcept Exception as e:\n    print(f\"ERROR: Failed to use HealthMonitor: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Using Existing DistributedTracer\n\nThe DistributedTracer is already implemented for complex request flows and performance analysis.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Example: Using existing DistributedTracer\ntry:\n    # Initialize DistributedTracer\n    tracer = DistributedTracer()\n    \n    # Start a trace for complex operation\n    main_trace = tracer.start_trace(\n        \"user_registration_flow\",\n        service_name=\"user_service\",\n        tags={\"user_id\": \"user_12345\", \"flow\": \"registration\"}\n    )\n    \n    print(f\"Started main trace: {main_trace.trace_id}\")\n    print(f\"Root span: {main_trace.span_id}\")\n    \n    # Add child spans\n    validation_span = tracer.start_span(\n        \"validate_user_data\",\n        service_name=\"validation_service\",\n        tags={\"step\": \"validation\"}\n    )\n    \n    tracer.add_span_log(\"info\", \"Starting user data validation\")\n    time.sleep(0.05)  # Simulate work\n    tracer.add_span_log(\"info\", \"Validation completed successfully\")\n    tracer.finish_span(validation_span, TraceStatus.SUCCESS)\n    \n    # Another child span\n    db_span = tracer.start_span(\n        \"create_user_record\",\n        service_name=\"database_service\",\n        tags={\"table\": \"users\", \"operation\": \"insert\"}\n    )\n    \n    tracer.add_span_log(\"info\", \"Creating user record\")\n    time.sleep(0.02)  # Simulate database work\n    tracer.add_span_log(\"info\", \"User record created\", user_id=\"user_12345\")\n    tracer.finish_span(db_span, TraceStatus.SUCCESS)\n    \n    # Customer.IO span\n    cio_span = tracer.start_span(\n        \"send_welcome_event\",\n        service_name=\"customer_io\",\n        tags={\"event_type\": \"welcome\", \"channel\": \"email\"}\n    )\n    \n    tracer.add_span_log(\"info\", \"Sending welcome event to Customer.IO\")\n    time.sleep(0.1)  # Simulate API call\n    tracer.add_span_log(\"info\", \"Welcome event sent successfully\")\n    tracer.finish_span(cio_span, TraceStatus.SUCCESS)\n    \n    # Finish main trace\n    tracer.finish_span(main_trace, TraceStatus.SUCCESS)\n    \n    # Get trace tree\n    trace_tree = tracer.get_trace_tree(main_trace.trace_id)\n    print(f\"\\nTrace Analysis:\")\n    print(f\"  Total spans: {trace_tree['total_spans']}\")\n    print(f\"  Total duration: {trace_tree['total_duration_ms']:.2f}ms\")\n    print(f\"  Root spans: {len(trace_tree['root_spans'])}\")\n    \n    # Show span hierarchy\n    def print_span_tree(span_data, level=0):\n        indent = \"  \" * level\n        span = span_data[\"span\"]\n        print(f\"{indent}- {span['operation_name']} ({span['service_name']}): {span['duration_ms']:.1f}ms\")\n        for child in span_data[\"children\"]:\n            print_span_tree(child, level + 1)\n    \n    print(f\"\\nSpan Hierarchy:\")\n    for root_span in trace_tree['root_spans']:\n        print_span_tree(root_span)\n    \n    # Get tracing statistics\n    trace_stats = tracer.get_tracing_statistics()\n    print(f\"\\nTracing Statistics:\")\n    print(f\"  Total traces: {trace_stats['total_traces']}\")\n    print(f\"  Total spans: {trace_stats['total_spans']}\")\n    print(f\"  Active spans: {trace_stats['active_spans']}\")\n    print(f\"  Average trace duration: {trace_stats['average_trace_duration_ms']:.2f}ms\")\n    print(f\"  Average spans per trace: {trace_stats['average_spans_per_trace']}\")\n    \n    # Demonstrate trace function decorator\n    @trace_function(\"customer_lookup\", \"user_service\")\n    def lookup_customer(customer_id: str):\n        \"\"\"Example function with automatic tracing.\"\"\"\n        time.sleep(0.03)  # Simulate work\n        return {\"id\": customer_id, \"name\": \"John Doe\", \"plan\": \"premium\"}\n    \n    # The decorator creates its own tracer, so we'll call it\n    result = lookup_customer(\"customer_789\")\n    print(f\"\\nFunction tracing result: {result}\")\n    \n    print(\"SUCCESS: Using existing DistributedTracer with span hierarchy\")\n    \nexcept Exception as e:\n    print(f\"ERROR: Failed to use DistributedTracer: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Using Existing ObservabilityManager\n\nThe comprehensive ObservabilityManager combines all monitoring components into a unified interface.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Example: Using comprehensive ObservabilityManager\ntry:\n    # Initialize ObservabilityManager with Customer.IO client\n    observability = ObservabilityManager(client)\n    observability.setup_default_alerts()\n    \n    print(\"ObservabilityManager initialized with components:\")\n    print(f\"  - MetricsCollector: {type(observability.metrics_collector).__name__}\")\n    print(f\"  - AlertManager: {type(observability.alert_manager).__name__}\")\n    print(f\"  - HealthMonitor: {type(observability.health_monitor).__name__}\")\n    print(f\"  - DistributedTracer: {type(observability.tracer).__name__}\")\n    \n    # Record various operations\n    observability.record_request(\"api_call\", duration_ms=125.5, success=True, tags={\"endpoint\": \"/track\"})\n    observability.record_request(\"api_call\", duration_ms=89.2, success=True, tags={\"endpoint\": \"/identify\"})\n    observability.record_request(\"api_call\", duration_ms=2500.0, success=False, tags={\"endpoint\": \"/batch\"})\n    \n    # Record Customer.IO specific events\n    observability.record_customer_io_event(\"track\", \"user_123\", success=True, response_time_ms=95.3)\n    observability.record_customer_io_event(\"identify\", \"user_456\", success=True, response_time_ms=67.8)\n    observability.record_customer_io_event(\"batch\", \"batch_789\", success=False, response_time_ms=3000.0)\n    \n    # Record batch operations\n    observability.record_batch_operation(\n        batch_size=100,\n        processing_time_ms=1250.0,\n        success_count=95,\n        error_count=5\n    )\n    \n    # Use tracing\n    trace = observability.start_trace(\"complex_customer_operation\", service=\"customer_service\")\n    \n    span1 = observability.start_span(\"validate_input\", service=\"validation\")\n    time.sleep(0.02)\n    observability.finish_span(span1, success=True)\n    \n    span2 = observability.start_span(\"process_data\", service=\"processor\")\n    time.sleep(0.05)\n    observability.finish_span(span2, success=True)\n    \n    span3 = observability.start_span(\"send_to_customerio\", service=\"customer_io\")\n    time.sleep(0.08)\n    observability.finish_span(span3, success=True)\n    \n    observability.finish_span(trace, success=True)\n    \n    print(f\"\\nRecorded operations and completed trace: {trace.trace_id}\")\n    \n    # Get comprehensive dashboard data\n    dashboard = observability.get_dashboard_data()\n    print(f\"\\nDashboard Data:\")\n    print(f\"  System uptime: {dashboard['system']['uptime_seconds']:.1f}s\")\n    print(f\"  Total requests: {dashboard['requests']['total_requests']}\")\n    print(f\"  Error rate: {dashboard['requests']['error_rate_percent']:.1f}%\")\n    print(f\"  Requests/minute: {dashboard['requests']['requests_per_minute']:.1f}\")\n    print(f\"  Health status: {dashboard['system']['health_status']}\")\n    print(f\"  Active alerts: {dashboard['alerts']['active_alerts']}\")\n    print(f\"  Metrics buffer utilization: {dashboard['metrics']['buffer_utilization']:.1f}%\")\n    \n    # Generate health report\n    health_report = observability.generate_health_report()\n    print(f\"\\nHealth Report:\")\n    print(f\"  Overall health: {health_report['overall_health']}\")\n    print(f\"  Health checks: {health_report['summary']['passing_health_checks']}/{health_report['summary']['total_health_checks']} passing\")\n    print(f\"  Active alerts: {health_report['summary']['active_alert_count']}\")\n    \n    if health_report['recommendations']:\n        print(f\"  Recommendations:\")\n        for rec in health_report['recommendations'][:3]:  # Show first 3\n            print(f\"    - {rec}\")\n    \n    # Get manager metrics\n    manager_metrics = observability.get_metrics()\n    print(f\"\\nManager Metrics:\")\n    print(f\"  Manager uptime: {manager_metrics['manager']['uptime_seconds']:.1f}s\")\n    print(f\"  Total requests processed: {manager_metrics['manager']['total_requests']}\")\n    print(f\"  Total errors: {manager_metrics['manager']['total_errors']}\")\n    \n    components = manager_metrics['components']\n    print(f\"  Components status:\")\n    print(f\"    - MetricsCollector buffer: {components['metrics_collector']['buffer_utilization']:.1f}%\")\n    print(f\"    - AlertManager rules: {components['alert_manager']['total_rules']}\")\n    print(f\"    - HealthMonitor checks: {components['health_monitor']['registered_checks']}\")\n    print(f\"    - Tracer: {components['tracer']['total_traces']} traces, {components['tracer']['total_spans']} spans\")\n    \n    print(\"SUCCESS: Comprehensive observability system operational\")\n    \nexcept Exception as e:\n    print(f\"ERROR: Failed to use ObservabilityManager: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Practical Observability Examples\n\nComprehensive examples demonstrating observability features with the existing ObservabilityManager.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize ObservabilityManager with Customer.IO client\ntry:\n    client = CustomerIOClient(\n        site_id=\"test_site_123\",\n        api_key=\"test_key_456\",\n        region=\"us\"\n    )\n    \n    observability = ObservabilityManager(client)\n    observability.setup_default_alerts()\n    \n    print(\"SUCCESS: ObservabilityManager initialized with Customer.IO client\")\n    print(f\"Manager start time: {observability.start_time}\")\n    print(f\"Default alert rules configured: {len(observability.alert_manager.rules)}\")\n    \nexcept Exception as e:\n    print(f\"ERROR: Failed to initialize ObservabilityManager: {e}\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Example: Record various Customer.IO operations\ntry:\n    import random\n    \n    # Simulate various API operations with realistic performance patterns\n    operations = [\n        {\"name\": \"track_event\", \"endpoint\": \"/api/track\", \"base_time\": 100},\n        {\"name\": \"identify_user\", \"endpoint\": \"/api/identify\", \"base_time\": 80},\n        {\"name\": \"batch_operation\", \"endpoint\": \"/api/batch\", \"base_time\": 500},\n        {\"name\": \"suppress_user\", \"endpoint\": \"/api/suppress\", \"base_time\": 150},\n        {\"name\": \"delete_user\", \"endpoint\": \"/api/delete\", \"base_time\": 200}\n    ]\n    \n    print(\"Simulating Customer.IO operations...\")\n    \n    for i in range(15):\n        op = random.choice(operations)\n        \n        # Add realistic variance to response times\n        response_time = op[\"base_time\"] + random.uniform(-30, 100)\n        success = random.random() > 0.05  # 95% success rate\n        \n        # Record request metrics\n        observability.record_request(\n            operation=op[\"name\"],\n            duration_ms=response_time,\n            success=success,\n            tags={\"endpoint\": op[\"endpoint\"], \"version\": \"v1\"}\n        )\n        \n        # Record Customer.IO specific metrics\n        observability.record_customer_io_event(\n            event_type=op[\"name\"],\n            user_id=f\"user_{i % 5}\",  # 5 different users\n            success=success,\n            response_time_ms=response_time\n        )\n    \n    # Simulate batch operations\n    for batch_num in range(3):\n        batch_size = random.randint(50, 200)\n        processing_time = batch_size * random.uniform(8, 15)  # ~10ms per item\n        success_count = int(batch_size * random.uniform(0.9, 1.0))  # 90-100% success\n        error_count = batch_size - success_count\n        \n        observability.record_batch_operation(\n            batch_size=batch_size,\n            processing_time_ms=processing_time,\n            success_count=success_count,\n            error_count=error_count\n        )\n    \n    print(\"SUCCESS: Recorded 15 API operations and 3 batch operations\")\n    print(f\"Total requests tracked: {observability.request_count}\")\n    print(f\"Total errors tracked: {observability.error_count}\")\n    \nexcept Exception as e:\n    print(f\"ERROR: Failed to record operations: {e}\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Example: Complex distributed tracing scenario\ntry:\n    # Simulate a complex customer journey with multiple touchpoints\n    trace = observability.start_trace(\n        \"customer_journey_flow\",\n        service=\"customer_experience\",\n        user_id=\"user_premium_123\",\n        journey_type=\"onboarding\"\n    )\n    \n    print(f\"Started customer journey trace: {trace.trace_id}\")\n    \n    # Step 1: User registration validation\n    validation_span = observability.start_span(\n        \"validate_registration_data\",\n        service=\"validation_service\"\n    )\n    time.sleep(0.03)  # Simulate validation work\n    observability.tracer.add_span_log(\"info\", \"Email validation completed\")\n    observability.tracer.add_span_log(\"info\", \"Password strength verified\")\n    observability.finish_span(validation_span, success=True)\n    \n    # Step 2: Account creation\n    account_span = observability.start_span(\n        \"create_customer_account\",\n        service=\"account_service\"\n    )\n    time.sleep(0.05)  # Simulate database operations\n    observability.tracer.add_span_log(\"info\", \"Account created successfully\")\n    observability.tracer.add_span_log(\"info\", \"User preferences initialized\")\n    observability.finish_span(account_span, success=True)\n    \n    # Step 3: Customer.IO identification\n    cio_identify_span = observability.start_span(\n        \"customerio_identify_user\",\n        service=\"customer_io\"\n    )\n    time.sleep(0.08)  # Simulate API call\n    observability.tracer.add_span_log(\"info\", \"User identified in Customer.IO\")\n    observability.tracer.add_span_log(\"info\", \"Profile attributes synchronized\")\n    observability.finish_span(cio_identify_span, success=True)\n    \n    # Step 4: Send welcome sequence\n    welcome_span = observability.start_span(\n        \"trigger_welcome_sequence\",\n        service=\"customer_io\"\n    )\n    time.sleep(0.12)  # Simulate campaign trigger\n    observability.tracer.add_span_log(\"info\", \"Welcome email campaign triggered\")\n    observability.tracer.add_span_log(\"info\", \"Onboarding sequence initiated\")\n    observability.finish_span(welcome_span, success=True)\n    \n    # Step 5: Analytics tracking\n    analytics_span = observability.start_span(\n        \"track_registration_event\",\n        service=\"analytics_service\"\n    )\n    time.sleep(0.04)  # Simulate analytics processing\n    observability.tracer.add_span_log(\"info\", \"Registration event tracked\")\n    observability.tracer.add_span_log(\"info\", \"Conversion funnel updated\")\n    observability.finish_span(analytics_span, success=True)\n    \n    # Finish the main journey\n    observability.finish_span(trace, success=True)\n    \n    # Analyze the trace\n    trace_tree = observability.tracer.get_trace_tree(trace.trace_id)\n    print(f\"\\nCustomer Journey Analysis:\")\n    print(f\"  Total operation time: {trace_tree['total_duration_ms']:.2f}ms\")\n    print(f\"  Number of service calls: {trace_tree['total_spans']}\")\n    print(f\"  Journey success: All spans completed successfully\")\n    \n    # Show detailed breakdown\n    spans = observability.tracer.get_trace(trace.trace_id)\n    print(f\"\\nDetailed Timeline:\")\n    for span in sorted(spans, key=lambda s: s.start_time):\n        if span.duration_ms:\n            print(f\"  {span.operation_name} ({span.service_name}): {span.duration_ms:.1f}ms\")\n            if span.logs:\n                for log in span.logs[-1:]:  # Show last log entry\n                    print(f\"    └─ {log['message']}\")\n    \n    print(\"SUCCESS: Complex distributed tracing completed\")\n    \nexcept Exception as e:\n    print(f\"ERROR: Distributed tracing failed: {e}\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Example: Health monitoring and alerting\ntry:\n    # Run comprehensive health checks\n    health_results = observability.health_monitor.run_all_checks()\n    \n    print(\"Health Check Results:\")\n    for check_id, check in health_results.items():\n        status_icon = \"✅\" if check.is_healthy() else \"❌\"\n        print(f\"  {status_icon} {check.name}\")\n        print(f\"      Status: {check.status}\")\n        print(f\"      Response Time: {check.response_time_ms:.1f}ms\")\n        if check.details:\n            key_details = list(check.details.items())[:2]  # Show first 2 details\n            for key, value in key_details:\n                if isinstance(value, (int, float)):\n                    print(f\"      {key}: {value}\")\n                else:\n                    print(f\"      {key}: {str(value)[:50]}...\")\n        print()\n    \n    # Get overall system health\n    system_health = observability.health_monitor.get_system_health()\n    print(f\"Overall System Health: {system_health['status']}\")\n    print(f\"Health Score: {system_health['summary']['health_percentage']:.1f}%\")\n    print(f\"Healthy Checks: {system_health['summary']['healthy_checks']}/{system_health['summary']['total_checks']}\")\n    \n    # Check for any active alerts\n    active_alerts = observability.alert_manager.get_active_alerts()\n    print(f\"\\nAlert Status:\")\n    print(f\"  Active Alerts: {len(active_alerts)}\")\n    \n    if active_alerts:\n        for alert in active_alerts[:3]:  # Show first 3 alerts\n            print(f\"    - {alert.name} ({alert.severity})\")\n            print(f\"      {alert.message}\")\n            print(f\"      Triggered: {alert.triggered_at}\")\n    else:\n        print(\"  No active alerts - system operating normally\")\n    \n    # Show alert rule status\n    alert_stats = observability.alert_manager.get_alert_statistics()\n    print(f\"\\nAlert Configuration:\")\n    print(f\"  Total Rules: {alert_stats['total_rules']}\")\n    print(f\"  Enabled Rules: {alert_stats['enabled_rules']}\")\n    print(f\"  Alerts (24h): {alert_stats['alerts_last_24h']}\")\n    \n    print(\"SUCCESS: Health monitoring and alerting operational\")\n    \nexcept Exception as e:\n    print(f\"ERROR: Health monitoring failed: {e}\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Example: Dashboard data and comprehensive monitoring\ntry:\n    # Allow time for metrics to accumulate\n    time.sleep(1)\n    \n    # Get comprehensive dashboard data\n    dashboard = observability.get_dashboard_data()\n    \n    print(\"📊 OBSERVABILITY DASHBOARD\")\n    print(\"=\" * 50)\n    \n    # System overview\n    print(f\"🖥️  SYSTEM STATUS\")\n    print(f\"   Uptime: {dashboard['system']['uptime_seconds']:.1f} seconds\")\n    print(f\"   Health: {dashboard['system']['health_status']}\")\n    print(f\"   Started: {dashboard['system']['start_time'][:19]}Z\")\n    \n    # Request metrics\n    print(f\"\\n📈 REQUEST METRICS\")\n    requests = dashboard['requests']\n    print(f\"   Total Requests: {requests['total_requests']}\")\n    print(f\"   Total Errors: {requests['total_errors']}\")\n    print(f\"   Error Rate: {requests['error_rate_percent']:.1f}%\")\n    print(f\"   Req/Min: {requests['requests_per_minute']:.1f}\")\n    \n    # Performance metrics\n    if dashboard.get('performance'):\n        perf = dashboard['performance']\n        print(f\"\\n⚡ PERFORMANCE METRICS\")\n        print(f\"   Avg Response: {perf.get('mean', 0):.1f}ms\")\n        print(f\"   P95 Response: {perf.get('p95', 0):.1f}ms\")\n        print(f\"   Min/Max: {perf.get('min', 0):.1f}ms / {perf.get('max', 0):.1f}ms\")\n        print(f\"   Total Samples: {perf.get('count', 0)}\")\n    \n    # Alert status\n    alerts = dashboard['alerts']\n    print(f\"\\n🚨 ALERT STATUS\")\n    print(f\"   Active Alerts: {alerts['active_alerts']}\")\n    print(f\"   Critical Alerts: {alerts['critical_alerts']}\")\n    alert_stats = alerts['alert_statistics']\n    print(f\"   Alert Rules: {alert_stats['enabled_rules']}/{alert_stats['total_rules']} enabled\")\n    \n    # Tracing metrics\n    tracing = dashboard['tracing']\n    print(f\"\\n🔍 DISTRIBUTED TRACING\")\n    print(f\"   Total Traces: {tracing['total_traces']}\")\n    print(f\"   Total Spans: {tracing['total_spans']}\")\n    print(f\"   Active Spans: {tracing['active_spans']}\")\n    print(f\"   Avg Trace Duration: {tracing['average_trace_duration_ms']:.1f}ms\")\n    \n    # Metrics buffer status\n    metrics_info = dashboard['metrics']\n    print(f\"\\n📊 METRICS BUFFER\")\n    print(f\"   Buffer Utilization: {metrics_info['buffer_utilization']:.1f}%\")\n    print(f\"   Buffer Size: {metrics_info['buffer_size']}/{metrics_info['buffer_capacity']}\")\n    print(f\"   Metric Types: {metrics_info['aggregated_metric_types']}\")\n    print(f\"   Last Flush: {metrics_info['last_flush'][:19]}Z\")\n    \n    print(f\"\\n📅 Report Generated: {dashboard['timestamp'][:19]}Z\")\n    \n    # Generate detailed health report\n    print(\"\\n\" + \"=\" * 50)\n    health_report = observability.generate_health_report()\n    \n    print(f\"🏥 HEALTH REPORT\")\n    print(f\"   Overall Health: {health_report['overall_health']}\")\n    \n    summary = health_report['summary']\n    print(f\"   Health Checks: {summary['passing_health_checks']}/{summary['total_health_checks']} passing\")\n    print(f\"   Active Alerts: {summary['active_alert_count']}\")\n    print(f\"   Critical Alerts: {summary['critical_alert_count']}\")\n    \n    if health_report['recommendations']:\n        print(f\"\\n💡 RECOMMENDATIONS:\")\n        for i, rec in enumerate(health_report['recommendations'][:3], 1):\n            print(f\"   {i}. {rec}\")\n    else:\n        print(f\"\\n✅ No recommendations - system running optimally\")\n    \n    # Show manager metrics\n    manager_metrics = observability.get_metrics()\n    print(f\"\\n🔧 MANAGER STATUS\")\n    manager = manager_metrics['manager']\n    print(f\"   Manager Uptime: {manager['uptime_seconds']:.1f}s\")\n    print(f\"   Requests Processed: {manager['total_requests']}\")\n    print(f\"   Errors Tracked: {manager['total_errors']}\")\n    \n    features = manager_metrics['features']\n    enabled_features = [name for name, enabled in features.items() if enabled]\n    print(f\"   Active Features: {', '.join(enabled_features)}\")\n    \n    print(\"\\n✅ SUCCESS: Comprehensive observability system operational\")\n    print(\"   All monitoring components functioning correctly\")\n    print(\"   Ready for production workloads\")\n    \nexcept Exception as e:\n    print(f\"❌ ERROR: Dashboard generation failed: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis notebook demonstrates using the existing comprehensive **ObservabilityManager** from `utils/observability_manager.py` for Customer.IO data pipeline monitoring.\n\n**Key Components Used:**\n- **ObservabilityManager**: Unified interface for all monitoring capabilities\n- **MetricsCollector**: High-performance metrics collection with buffering and aggregation\n- **AlertManager**: Intelligent alerting with threshold management and rule evaluation\n- **HealthMonitor**: Comprehensive health checks for services and dependencies\n- **DistributedTracer**: Request correlation and performance analysis across services\n\n**Capabilities Demonstrated:**\n- Real-time metrics collection (counters, gauges, timers, histograms)\n- Configurable alerting rules with multiple severity levels and violation tracking\n- System resource monitoring and custom health check registration\n- Distributed tracing with span hierarchy and correlation across services\n- Performance analytics with statistical aggregation (P95, P99, mean, etc.)\n- Comprehensive dashboard data generation for monitoring systems\n- Automated health reporting with actionable recommendations\n\n**Integration Benefits:**\n- Customer.IO API monitoring and performance tracking\n- Batch operation monitoring with success/error rate tracking\n- Error tracking and intelligent alerting\n- System resource utilization monitoring\n- Custom health check registration for business-specific monitoring\n\n**Production Ready Features:**\n- Thread-safe operations with background processing\n- Configurable buffer sizes and flush intervals\n- Comprehensive error handling and recovery\n- Statistical aggregation and trend analysis\n- Multi-service distributed tracing support\n- Automated alert resolution and notification\n\nThe system integrates seamlessly with existing Customer.IO operations and provides enterprise-grade monitoring capabilities for production Databricks environments.",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}